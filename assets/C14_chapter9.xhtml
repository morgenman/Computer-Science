<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" dir="ltr" lang="en" xml:lang="en">
<head>
<meta charset="UTF-8"/>
<title>9 The Graphics Pipeline</title>
<link href="../styles/9781000426359.css" rel="stylesheet" type="text/css"/>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX","input/MathML","output/SVG"],
extensions: ["tex2jax.js","mml2jax.js","MathEvents.js"],
TeX: {
extensions: ["noErrors.js","noUndefined.js","autoload-all.js"]
},
MathMenu: {
showRenderer: false
},
menuSettings: {
zoom: "Click"
},
messageStyle: "none"
});
</script>
<script src="../mathjax/MathJax.js" type="text/javascript"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006665500" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter">
<section epub:type="chapter" role="doc-chapter">
<h1 class="chapz" id="c9"><span aria-label="177" epub:type="pagebreak" id="pg_177" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rc9" role="doc-backlink"><span class="green"><span class="big1">9</span><br/>The Graphics Pipeline</span></a></h1>
<p>The previous several chapters have established the mathematical scaffolding we need to look at the second major approach to rendering: drawing objects one by one onto the screen or <em>object-order rendering</em>. Unlike in ray tracing, where we consider each pixel in turn and find the objects that influence its color, we’ll now instead consider each geometric object in turn and find the pixels that it could have an effect on. The process of finding all the pixels in an image that are occupied by a geometric primitive is called <em>rasterization</em>, so object-order rendering can also be called rendering by rasterization. The sequence of operations that is required, starting with objects and ending by updating pixels in the image, is known as the <em>graphics pipeline</em>.</p>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent">Any graphics system has one or more types of “primitive object” that it can handle directly, and more complex objects are converted into these “primitives.” Triangles are the most often used primitive.</p>
</aside>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent">Rasterization-based systems are also called <em>scanline renderers</em>.</p>
</aside>
<p class="indent">Object-order rendering has enjoyed great success because of its efficiency. For large scenes, management of data access patterns is crucial to performance, and making a single pass over the scene visiting each bit of geometry once has significant advantages over repeatedly searching the scene to retrieve the objects required to shade each pixel.</p>
<p>The title of this chapter suggests that there is only one way to do <a id="index_term790"/><a id="index_term983"/>object-order rendering. Of course, this isn’t true—two quite different examples of <a id="index_term494"/>graphics pipelines with very different goals are the hardware pipelines used to support interactive rendering via APIs like OpenGL and Direct3D and the software pipelines used in film production, supporting APIs like RenderMan. Hardware pipelines must run fast enough to react in real time for games, visualizations, and user interfaces. Production pipelines must render the highest quality animation and visual effects possible and scale to enormous scenes, but may take much <a id="term-383"/><a id="term-397"/><a id="term-472"/><a id="term-620"/><a id="term-668"/><span aria-label="178" epub:type="pagebreak" id="pg_178" role="doc-pagebreak"/>more time to do so. Despite the different design decisions resulting from these divergent goals, a remarkable amount is shared among most, if not all, pipelines, and this chapter attempts to focus on these common fundamentals, erring on the side of following the hardware pipelines more closely.</p>
<p>The work that needs to be done in object-order rendering can be organized into the task of <a id="index_term939"/>rasterization itself, the operations that are done to geometry before rasterization, and the operations that are done to pixels after rasterization. The most common geometric operation is applying matrix transformations, as discussed in the previous two chapters, to map the points that define the geometry from object space to screen space, so that the input to the rasterizer is expressed in pixel coordinates, or <em><a id="index_term1025"/>screen space</em>. The most common pixelwise operation is <em>hidden <a id="index_term1143"/>surface removal</em> which arranges for surfaces closer to the viewer to appear in front of surfaces farther from the viewer. Many other operations also can be included at each stage, thereby achieving a wide range of different rendering effects using the same general process.</p>
<p>For the purposes of this chapter, we’ll discuss the graphics pipeline in terms of four stages (<a href="C14_chapter9.xhtml#f9_1">Figure 9.1</a>). Geometric objects are fed into the pipeline from an interactive application or from a scene description file, and they are always described by sets of vertices. The vertices are operated on in the <em>vertex-processing stage</em>, then the primitives using those vertices are sent to the <em>rasterization stage</em>. The rasterizer breaks each primitive into a number of <em>fragments</em>, one for each pixel covered by the primitive. The fragments are processed in the <em>fragment processing stage</em>, and then, the various fragments corresponding to each pixel are combined in the <em>fragment blending stage</em>.</p>
<figure id="f9_1" tabindex="0">
<img alt="" src="../images/fig9_1.jpg"/>
<figcaption><p><span class="blue">Figure 9.1.</span> The stages of a graphics pipeline.</p></figcaption>
</figure>
<p>We’ll begin by discussing rasterization and then illustrate the purpose of the geometric and pixel-wise stages by a series of examples.</p>
<section>
<h2 id="sec9_1"><a epub:type="backlink" href="C02a_toc.xhtml#rsec9_1" role="doc-backlink"><span class="green">9.1 Rasterization</span></a></h2>
<p>Rasterization is the central operation in object-order graphics, and the <em>rasterizer</em> is central to any graphics pipeline. For each primitive that comes in, the rasterizer has two jobs: it <em>enumerates</em> the pixels that are covered by the primitive and it <em>interpolates</em> values, called attributes, across the primitive—the purpose for these attributes will be clear with later examples. The output of the rasterizer is a set of <em>fragments</em>, one for each pixel covered by the primitive. Each fragment “lives” at a particular pixel and carries its own set of attribute values.</p>
<p>In this chapter, we will present rasterization with a view toward using it to render three-dimensional scenes. The same rasterization methods are used to draw <a id="term-621"/><span aria-label="179" epub:type="pagebreak" id="pg_179" role="doc-pagebreak"/>lines and shapes in 2D as well—although it is becoming more and more common to use the 3D graphics system “under the covers” to do all 2D drawing.</p>
<section>
<h3 id="sec9_1_1"><a id="index_term676"/><span class="green">9.1.1 Line Drawing</span></h3>
<p>Most graphics packages contain a line drawing command that takes two endpoints in screen coordinates (see <a href="C08_chapter3.xhtml#f3_10">Figure 3.10</a>) and draws a line between them. For example, the call for endpoints (1,1) and (3,2) would turn on <a id="index_term498"/>pixels (1,1) and (3,2) and fill in one pixel between them. For general screen coordinate endpoints (<em>x</em><sub>0</sub><em>,y</em><sub>0</sub>) and (<em>x</em><sub>1</sub><em>,y</em><sub>1</sub>), the routine should draw some “reasonable” set of pixels that approximates a line between them. Drawing such lines is based on line equations, and we have two types of equations to choose from: implicit and parametric. This section describes the approach using implicit lines.</p>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent">Even though we often use integer-valued endpoints for examples, it’s important to properly support arbitrary endpoints.</p>
</aside>
<section>
<h4 id="sec36"><span class="blue">Line Drawing Using Implicit Line Equations</span></h4>
<p>The most common way to draw lines using implicit equations is the <em>midpoint</em> algorithm ((Pitteway, 1967; van Aken &amp; Novak, 1985)). The midpoint algorithm ends up drawing the same lines as the <em><a id="index_term109"/>Bresenham algorithm</em> (Bresenham, 1965), but it is somewhat more straightforward.</p>
<p>The first thing to do is find the implicit equation for the line as discussed in <a href="C07_chapter2.xhtml#sec2_7_2">Section 2.7.2</a>:</p>
<div class="disp-formula" id="equ9_1">
<m:math alttext=""><m:mrow><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow><m:mo>≡</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mi>x</m:mi><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mi>y</m:mi><m:mo>+</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>=</m:mo><m:mn>0.</m:mn></m:mrow><m:mspace width="3em"/><m:mo>(9.1)</m:mo></m:math>
</div>
<p>We assume that <em>x</em><sub>0</sub> ≤ <em>x</em><sub>1</sub>. If that is not true, we swap the points so that it is true. The slope <em>m</em> of the line is given by</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi>m</m:mi><m:mo>=</m:mo><m:mfrac><m:mrow><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub></m:mrow><m:mrow><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub></m:mrow></m:mfrac><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>The following discussion assumes <em>m</em> ∈ (0, 1]. Analogous discussions can be derived for <em>m</em> ∈ (–∞, –1], <em>m</em> ∈ (–1, 0],and <em>m</em> ∈ (1, ∞). The four cases cover all possibilities.</p>
<p>For the case <em>m</em> ∈ (0, 1], there is more “run” than “rise” ; i.e., the line is moving faster in <em>x</em> than in <em>y</em>. If we have an API where the <em>y</em>-axis points downward, we might have a concern about whether this makes the process harder, but, in fact, we can ignore that detail. We can ignore the geometric notions of “up” and “down,” because the algebra is exactly the same for the two cases. Cautious readers can confirm that the resulting algorithm works for the <em>y</em>-axis downward case. The key assumption of the midpoint algorithm is that we draw the thinnest line possible <a id="term-384"/><a id="term-622"/><span aria-label="180" epub:type="pagebreak" id="pg_180" role="doc-pagebreak"/>that has no gaps. A diagonal connection between two pixels is not considered a gap.</p>
<p>As the line progresses from the left endpoint to the right, there are only two possibilities: draw a pixel at the same height as the pixel drawn to its left, or draw a pixel one higher. There will always be exactly one pixel in each column of pixels between the endpoints. Zero would imply a gap, and two would be too thick a line. There may be two pixels in the same row for the case we are considering; the line is more horizontal than vertical, so sometimes it will go right and sometimes up. This concept is shown in <a href="C14_chapter9.xhtml#f9_2">Figure 9.2</a>, where three “reasonable” lines are shown, each advancing more in the horizontal direction than in the vertical direction.</p>
<figure id="f9_2" tabindex="0">
<img alt="" src="../images/fig9_2.jpg"/>
<figcaption><p><span class="blue">Figure 9.2.</span> Three “reasonable” lines that go seven pixels horizontally and three pixels vertically.</p></figcaption>
</figure>
<p>The midpoint algorithm for <em>m</em> ∈ (0, 1] first establishes the leftmost pixel and the column number (x-value) of the rightmost pixel and then loops horizontally establishing the row (y-value) of each pixel. The basic form of the algorithm is</p>
<pre class="pre"><em>y</em> = <em>y</em><sub>0</sub><br/><strong>for</strong> <em>x</em> = <em>x</em><sub>0</sub> to <em>x</em><sub>1</sub> <strong>do</strong><br/>  draw(<em>x, y</em>)<br/>  <strong>if</strong> (some condition) <strong>then</strong><br/>     <em>y</em> = <em>y</em> +1</pre>
<p>Note that <em>x</em> and <em>y</em> are integers. In words this says, “keep drawing pixels from left to right and sometimes move upward in the <em>y</em>-direction while doing so.” The key is to establish efficient ways to make the decision in the <em>if</em> statement.</p>
<p>An effective way to make the choice is to look at the <em>midpoint</em> of the line between the two potential pixel centers. More specifically, the pixel just drawn is pixel (<em>x, y</em>) whose center in real screen coordinates is at (<em>x, y</em>). The candidate pixels to be drawn to the right are pixels (<em>x</em> + 1,<em>y</em>) and (<em>x</em> + 1,<em>y</em> + 1). The midpoint between the centers of the two candidate pixels is (<em>x</em> +1,<em>y</em> +0<em>.</em>5). If the line passes below this midpoint, we draw the bottom pixel, and otherwise, we draw the top pixel (<a href="C14_chapter9.xhtml#f9_3">Figure 9.3</a>).</p>
<figure id="f9_3" tabindex="0">
<img alt="" src="../images/fig9_3.jpg"/>
<figcaption><p><span class="blue">Figure 9.3.</span> Top: the line goes above the midpoint, so the top pixel is drawn. Bottom: the line goes below the midpoint, so the bottom pixel is drawn.</p></figcaption>
</figure>
<p>To decide whether the line passes above or below (<em>x</em>+1,<em>y</em> +0<em>.</em>5),weevaluate <em>f</em> (<em>x</em> +1,<em>y</em> +0<em>.</em>5) in Equation (9.1). Recall from <a href="C07_chapter2.xhtml#sec2_7_1">Section 2.7.1</a> that <em>f</em> (<em>x, y</em>) = 0 for points (<em>x, y</em>) on the line, <em>f</em> (<em>x, y</em>) &gt; 0 for points on one side of the line, and <em>f</em> (<em>x, y</em>) &lt; 0 for points on the other side of the line. Because –<em>f</em> (<em>x, y</em>) = 0 and <em>f</em> (<em>x, y</em>) = 0 are both perfectly good equations for the line, it is not immediately clear whether <em>f</em> (<em>x, y</em>) being positive indicates that (<em>x, y</em>) is above the line or whether it is below. However, we can figure it out; the key term in Equation (9.1) is the <em>y</em> term (<em>x</em><sub>1</sub> – <em>x</em><sub>0</sub>)<em>y</em> . Note that (<em>x</em><sub>1</sub> – <em>x</em><sub>0</sub>) is definitely positive because <em>x</em><sub>1</sub> &gt; <em>x</em><sub>0</sub>. This means that as <em>y</em> increases, the term (<em>x</em><sub>1</sub> – <em>x</em><sub>0</sub>)<em>y</em> gets larger (i.e., more positive or less negative). Thus, the case <em>f</em> (<em>x,</em> +∞) is definitely positive, and definitely above the line, implying points above the line are all positive.</p>
<p><a id="term-385"/><a id="term-623"/><span aria-label="181" epub:type="pagebreak" id="pg_181" role="doc-pagebreak"/>Another way to look at it is that the <em>y</em> component of the gradient vector is positive. So above the line, where <em>y</em> can increase arbitrarily, <em>f</em> (<em>x, y</em>) must be positive. This means we can make our code more specific by filling in the <em>if</em> statement:</p>
<pre class="pre"><strong>if</strong> <em>f</em> (<em>x</em> + 1,<em>y</em> + 0.5) &lt; 0 <strong>then</strong><br/>   <em>y</em> = <em>y</em> + 1</pre>
<p>The above code will work nicely for lines of the appropriate slope (i.e., between zero and one). The reader can work out the other three cases which differ only in small details.</p>
<p>If greater efficiency is desired, using an <em>incremental</em> method can help. An <a id="index_term616"/>incremental method tries to make a loop more efficient by reusing computation from the previous step. In the midpoint algorithm as presented, the main computation is the evaluation of <em>f</em> (<em>x</em> +1,<em>y</em> +0.5). Note that inside the loop, after the first iteration, either we already evaluated <em>f</em> (<em>x</em> – 1,<em>y</em> +0.5) or <em>f</em> (<em>x</em> – 1,<em>y</em> – 0.5) (<a href="C14_chapter9.xhtml#f9_4">Figure 9.4</a>). Note also this relationship:</p>
<figure id="f9_4" tabindex="0">
<img alt="" src="../images/fig9_4.jpg"/>
<figcaption><p><span class="blue">Figure 9.4.</span> When using the decision point shown between the two orange pixels, we just drew the blue pixel, so we evaluated <em>f</em> at one of the two left points shown.</p></figcaption>
</figure>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtable><m:mtr><m:mtd columnalign="right"><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>+</m:mo><m:mn>1</m:mn><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow></m:mtd><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd columnalign="left"><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd columnalign="right"><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>+</m:mo><m:mn>1</m:mn><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>+</m:mo><m:mn>1</m:mn><m:mo>)</m:mo></m:mrow></m:mtd><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd columnalign="left"><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mn>.</m:mn></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>This allows us to write an incremental version of the code:</p>
<pre class="pre"><em>y</em> = <em>y</em><sub>0</sub><br/><em>d</em> = <em>f</em> (<em>x</em><sub>0</sub> + 1,<em>y</em><sub>0</sub> + 0.5)<br/><strong>for</strong> <em>x</em> = <em>x</em><sub>0</sub> to <em>x</em><sub>1</sub> <strong>do</strong><br/>   draw(<em>x, y</em>)<br/>   <strong>if</strong> <em>d</em> &lt; 0 <strong>then</strong><br/>      <em>y</em> = <em>y</em> + 1<br/>      <em>d</em> = <em>d</em> + (<em>x</em><sub>1</sub> – <em>x</em><sub>0</sub>) + (<em>y</em><sub>0</sub> – <em>y</em><sub>1</sub>)<br/>   <strong>else</strong><br/>      <em>d</em> = <em>d</em> + (<em>y</em><sub>0</sub> – <em>y</em><sub>1</sub>)</pre>
<p class="noindent1">This code should run faster since it has little extra setup cost compared to the non-incremental version (that is not always true for incremental algorithms), but it may accumulate more numeric error because the evaluation of <em>f</em> (<em>x, y</em> + 0.5) may be composed of many adds for long lines. However, given that lines are rarely longer than a few thousand pixels, such an error is unlikely to be critical. Slightly longer setup cost, but faster loop execution, can be achieved by storing (<em>x</em><sub>1</sub> – <em>x</em><sub>0</sub>)+(<em>y</em><sub>0</sub> – <em>y</em><sub>1</sub>) and (<em>y</em><sub>0</sub> – <em>y</em><sub>1</sub>) as variables. We might hope a good compiler would do that for us, but if the code is critical, it would be wise to examine the results of compilation to make sure.</p>
</section>
</section>
<section>
<h3 id="sec9_1_2"><a id="index_term1278"/><span aria-label="182" epub:type="pagebreak" id="pg_182" role="doc-pagebreak"/><span class="green">9.1.2 Triangle Rasterization</span></h3>
<p>We often want to draw a 2D triangle with 2D points <strong>p</strong><sub>0</sub> = (<em>x</em><sub>0</sub>,<em>y</em><sub>0</sub>), <strong>p</strong><sub>1</sub> = (<em>x</em><sub>1</sub>,<em>y</em><sub>1</sub>), and <strong>p</strong><sub>2</sub> = (<em>x</em><sub>2</sub>,<em>y</em><sub>2</sub>) in screen coordinates. This is similar to the line drawing problem, but it has some of its own subtleties. As with line drawing, we may wish to interpolate color or other properties from values at the vertices. This is straightforward if we have the barycentric coordinates (<a href="C07_chapter2.xhtml#sec2_9">Section 2.9</a>). For example, if the vertices have colors <strong>c</strong><sub>0</sub>, <strong>c</strong><sub>1</sub>, and <strong>c</strong><sub>2</sub>, the color at a point in the triangle with barycentric coordinates (α, β, γ) is</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtext mathvariant="bold">c</m:mtext><m:mo>=</m:mo><m:mi>α</m:mi><m:msub><m:mrow><m:mtext mathvariant="bold">c</m:mtext></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>+</m:mo><m:mi>β</m:mi><m:msub><m:mrow><m:mtext mathvariant="bold">c</m:mtext></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>+</m:mo><m:mi>γ</m:mi><m:msub><m:mrow><m:mtext mathvariant="bold">c</m:mtext></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>This type of interpolation of color is known in graphics as <em>Gouraud</em> interpolation after its inventor (Gouraud, 1971).</p>
<p>Another subtlety of rasterizing triangles is that we are usually rasterizing triangles that share vertices and edges. This means we would like to rasterize adjacent triangles, so there are no holes. We could do this by using the midpoint algorithm to draw the outline of each triangle and then fill in the interior pixels. This would mean adjacent triangles both draw the same pixels along each edge. If the adjacent triangles have different colors, the image will depend on the order in which the two triangles are drawn. The most common way to rasterize triangles that avoids the order problem and eliminates holes is to use the convention that pixels are drawn if and only if their centers are inside the triangle; i.e., the barycentric coordinates of the pixel center are all in the interval (0, 1). Thisraises the issue of what to do if the center is exactly on the edge of the triangle. There are several ways to handle this as will be discussed later in this section. The key observation is that barycentric coordinates allow us to decide whether to draw a pixel and what color that pixel should be if we are interpolating colors from the vertices. So our problem of rasterizing the triangle boils down to efficiently finding the barycentric coordinates of pixel centers (Pineda, 1988). The brute force rasterization algorithm is</p>
<pre class="pre"><strong>for</strong> all <em>x</em> <strong>do</strong><br/>  <strong>for</strong> all <em>y</em> <strong>do</strong><br/>    compute (α, β, γ) for (<em>x, y</em>)<br/>    <strong>if</strong> (α ∈ [0, 1] and β ∈ [0, 1] and γ ∈ [0, 1]) <strong>then</strong><br/>       <strong>c</strong> = α<strong>c</strong><sub>0</sub> + β<strong>c</strong><sub>1</sub> + γ<strong>c</strong><sub>2</sub><br/>       drawpixel (<em>x, y</em>) with color <strong>c</strong></pre>
<p class="noindent1">The rest of the algorithm limits the outer loops to a smaller set of candidate pixels and makes the barycentric computation efficient.</p>
<p><a id="term-388"/><span aria-label="183" epub:type="pagebreak" id="pg_183" role="doc-pagebreak"/>We can add a simple efficiency by finding the bounding rectangle of the three vertices and only looping over this rectangle for candidate pixels to draw. We can compute barycentric coordinates using Equation (2.32). This yields the algorithm:</p>
<pre class="pre"><em>x</em><sub>min</sub> = floor(<em>x<sub>i</sub></em>)<br/><em>x</em><sub>max</sub> = ceiling(<em>x<sub>i</sub></em>)<br/><em>y</em><sub>min</sub> = floor(<em>y<sub>i</sub></em>)<br/><em>y</em><sub>max</sub> = ceiling(<em>y<sub>i</sub></em>)<br/><strong>for</strong> <em>y</em> = <em>y</em><sub>min</sub> to <em>y</em><sub>max</sub> <strong>do</strong><br/>  <strong>for</strong> <em>x</em> = <em>x</em><sub>min</sub> to <em>x</em><sub>max</sub> <strong>do</strong><br/>    α = <em>f</em><sub>12</sub>(<em>x, y</em>)<em>/f</em><sub>12</sub>(<em>x</em><sub>0</sub>,<em>y</em><sub>0</sub>)<br/>    β = <em>f</em><sub>20</sub>(<em>x, y</em>)<em>/f</em><sub>20</sub>(<em>x</em><sub>1</sub>,<em>y</em><sub>1</sub>)<br/>    γ = <em>f</em><sub>01</sub>(<em>x, y</em>)<em>/f</em><sub>01</sub>(<em>x</em><sub>2</sub>,<em>y</em><sub>2</sub>)<br/>    <strong>if</strong> (α &gt; 0 and β &gt; 0 and γ &gt; 0) <strong>then</strong><br/>       <strong>c</strong> = α<strong>c</strong><sub>0</sub> + β<strong>c</strong><sub>1</sub> + γ<strong>c</strong><sub>2</sub><br/>       drawpixel (<em>x, y</em>) with color <strong>c</strong></pre>
<p class="noindent1">Here, <em>f<sub>ij</sub></em> is the line given by Equation (9.1) with the appropriate vertices:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi>f</m:mi></m:mrow><m:mrow><m:mn>01</m:mn></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mi>x</m:mi><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mi>y</m:mi><m:mo>+</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>f</m:mi></m:mrow><m:mrow><m:mn>12</m:mn></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mi>x</m:mi><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mi>y</m:mi><m:mo>+</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>f</m:mi></m:mrow><m:mrow><m:mn>20</m:mn></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mi>x</m:mi><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:mo>)</m:mo></m:mrow><m:mi>y</m:mi><m:mo>+</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:mn>.</m:mn></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>Note that we have exchanged the test α ∈ (0, 1) with α &gt; 0 etc., because if all of α, β, γ are positive, then we know they are all less than one because α + β + γ = 1. We could also compute only two of the three barycentric variables and get the third from that relation, but it is not clear that this saves computation once the algorithm is made incremental, which is possible as in the line drawing algorithms; each of the computations of α, β, and γ does an evaluation of the form <em>f</em> (<em>x, y</em>) = <em>Ax</em> + <em>By</em> + <em>C</em>. In the inner loop, only <em>x</em> changes, and it changes by one. Note that <em>f</em> (<em>x</em> +1,<em>y</em>) = <em>f</em> (<em>x, y</em>)+ <em>A</em>. This is the basis of the incremental algorithm. In the outer loop, the evaluation changes for <em>f</em> (<em>x, y</em>) to <em>f</em> (<em>x, y</em> + 1), so a similar efficiency can be achieved. Because α, β, and γ change by constant increments in the loop, so does the color <strong>c</strong>. So this can be made incremental as well. For example, the red value for pixel (<em>x</em> + 1,<em>y</em>) differs from the red value for pixel (<em>x, y</em>) by a constant amount that can be precomputed. An example of a triangle with color interpolation is shown in <a href="C14_chapter9.xhtml#f9_5">Figure 9.5</a>.</p>
<figure id="f9_5" tabindex="0">
<img alt="" src="../images/fig9_5.jpg"/>
<figcaption><p><span class="blue">Figure 9.5.</span> A colored triangle with barycentric interpolation. Note that the changes in color components are linear in each row and column as well as along each edge. In fact, it is constant along every line, such as the diagonals, as well.</p></figcaption>
</figure>
<section>
<h4 id="sec37"><span class="blue">Dealing with Pixels on Triangle Edges</span></h4>
<p>We have still not discussed what to do for pixels whose centers are exactly on the edge of a triangle. If a pixel is exactly on the edge of a triangle, then it is <a id="term-386"/><a id="term-389"/><a id="term-624"/><span aria-label="184" epub:type="pagebreak" id="pg_184" role="doc-pagebreak"/>also on the edge of the adjacent triangle if there is one. There is no obvious way to award the pixel to one triangle or the other. The worst decision would be to not draw the pixel because a hole would result between the two triangles. Better, but still not good, would be to have both triangles draw the pixel. If the triangles are transparent, this will result in a double-coloring. We would really like to award the pixel to exactly one of the triangles, and we would like this process to be simple; which triangle is chosen does not matter as long as the choice is well defined.</p>
<p>One approach is to note that any off-screen point is definitely on exactly one side of the shared edge and that is the edge we will draw. For two non-overlapping triangles, the vertices not on the edge are on opposite sides of the edge from each other. Exactly one of these vertices will be on the same side of the edge as the off-screen point (<a href="C14_chapter9.xhtml#f9_6">Figure 9.6</a>). This is the basis of the test. The test if numbers <em>p</em> and <em>q</em> have the same sign can be implemented as the test <em>pq &gt;</em> 0, which is very efficient in most environments.</p>
<figure id="f9_6" tabindex="0">
<img alt="" src="../images/fig9_6.jpg"/>
<figcaption><p><span class="blue">Figure 9.6.</span> The off-screen point will be on one side of the triangle edge or the other. Exactly one of the non-shared vertices <strong>a</strong> and <strong>b</strong> will be on the same side.</p></figcaption>
</figure>
<p>Note that the test is not perfect because the line through the edge may also go through the off-screen point, but we have at least greatly reduced the number of problematic cases. Which off-screen point is used is arbitrary, and (<em>x, y</em>) = (–1, –1) is as good a choice as any. We will need to add a check for the case of a point exactly on an edge. We would like this check not to be reached for common cases, which are the completely inside or outside tests. This suggests</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>min</m:mi><m:mo>⁡</m:mo></m:mrow></m:msub><m:mo>=</m:mo><m:mtext>floor</m:mtext><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>i</m:mi></m:mrow></m:msub><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mtext>max</m:mtext></m:mrow></m:msub><m:mo>=</m:mo><m:mtext>ceiling</m:mtext><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>i</m:mi></m:mrow></m:msub><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mtext>min</m:mtext><m:mo>⁡</m:mo></m:mrow></m:msub><m:mo>=</m:mo><m:mtext>floor</m:mtext><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>i</m:mi></m:mrow></m:msub><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mtext>max</m:mtext><m:mo>⁡</m:mo></m:mrow></m:msub><m:mo>=</m:mo><m:mtext>ceiling</m:mtext><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>i</m:mi></m:mrow></m:msub><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<pre class="pre"><span aria-label="185" epub:type="pagebreak" id="pg_185" role="doc-pagebreak"/><em>f<sub>α</sub></em> = <em>f</em><sub>12</sub>(<em>x</em><sub>0</sub>,<em>y</em><sub>0</sub>)<br/><em>f<sub>β</sub></em> = <em>f</em><sub>20</sub>(<em>x</em><sub>1</sub>,<em>y</em><sub>1</sub>)<br/><em>f<sub>γ</sub></em> = <em>f</em><sub>01</sub>(<em>x</em><sub>2</sub>,<em>y</em><sub>2</sub>)<br/><strong>for</strong> <em>y</em> = <em>y</em><sub>min</sub> to <em>y</em><sub>max</sub> <strong>do</strong><br/>  <strong>for</strong> <em>x</em> = <em>x</em><sub>min</sub> to <em>x</em><sub>max</sub> <strong>do</strong><br/>    α = <em>f</em><sub>12</sub>(<em>x, y</em>)<em>/f<sub>α</sub></em><br/>    β = <em>f</em><sub>20</sub>(<em>x, y</em>)<em>/f<sub>β</sub></em><br/>    γ = <em>f</em><sub>01</sub>(<em>x, y</em>)<em>/f<sub>γ</sub></em><br/>    <strong>if</strong> (α ≥ 0 and β ≥ 0 and γ ≥ 0) <strong>then</strong><br/>       <strong>if</strong> (α &gt; 0 or <em>f<sub>α</sub>f</em><sub>12</sub>(–1, –1) &gt; 0) and<br/>          (β &gt; 0 or <em>f<sub>β</sub>f</em><sub>20</sub>(–1, –1) &gt; 0) and<br/>          (γ &gt; 0 or <em>f<sub>γ</sub>f</em><sub>01</sub>(–1, –1) &gt; 0) <strong>then</strong><br/>          <strong>c</strong> = α<strong>c</strong><sub>0</sub> + β<strong>c</strong><sub>1</sub> + γ<strong>c</strong><sub>2</sub><br/>          drawpixel (<em>x, y</em>) with color <strong>c</strong></pre>
<p class="noindent1">We might expect that the above code would work to eliminate holes and double-draws only if we use exactly the same line equation for both triangles. In fact, the line equation is the same only if the two shared vertices have the same order in the draw call for each triangle. Otherwise, the equation might flip in sign. This could be a problem depending on whether the compiler changes the order of operations. So if a robust implementation is needed, the details of the compiler and arithmetic unit may need to be examined. The first four lines in the pseudocode above must be coded carefully to handle cases where the edge exactly hits the pixel center.</p>
<p>In addition to being amenable to an incremental implementation, there are several potential early exit points. For example, if α is negative, there is no need to compute β or γ. While this may well result in a speed improvement, profiling is always a good idea; the extra branches could reduce pipelining or concurrency and might slow down the code. So as always, test any attractive-looking optimizations if the code is a critical section.</p>
<p>Another detail of the above code is that the divisions could be divisions by zero for degenerate triangles, i.e., if <em>f<sub>γ</sub></em> = 0. Either the floating point error conditions should be accounted for properly, or another test will be needed.</p>
</section>
</section>
<section>
<h3 id="sec9_1_3"><a id="index_term497"/><span class="green">9.1.3 Perspective Correct Interpolation</span></h3>
<p>There are some subtleties in achieving correct-looking perspective when interpolating quantities, such as texture coordinates or 3D positions, that need to vary linearly across the 3D triangles. We’ll use texture coordinates as an example of <a id="term-103"/><a id="term-381"/><span aria-label="186" epub:type="pagebreak" id="pg_186" role="doc-pagebreak"/>a quantity where perspective correction is important, but the same considerations apply to any attribute where linearity in 3D space is important.</p>
<p>The reason things are not straightforward is that just interpolating texture coordinates in screen space results in incorrect images, as shown for the grid texture in <a href="C14_chapter9.xhtml#f9_7">Figure 9.7</a>. Because things in perspective get smaller as the distance to the viewer increases, the lines that are evenly spaced in 3D should compress in 2D image space. More careful interpolation of texture coordinates is needed to accomplish this.</p>
<figure id="f9_7" tabindex="0">
<img alt="" src="../images/fig9_8.jpg"/>
<figcaption><p><span class="blue">Figure 9.7.</span> Left: correct perspective. Right: interpolation in screen space.</p></figcaption>
</figure>
<p>We can implement texture mapping on triangles by interpolating the (<em>u, v</em>) coordinates, modifying the rasterization method of <a href="C14_chapter9.xhtml#sec9_1_2">Section 9.1.2</a>, but this results in the problem shown at the right of <a href="C14_chapter9.xhtml#f9_7">Figure 9.7</a>. A similar problem occurs for triangles if screen space barycentric coordinates are used as in the following rasterization code:</p>
<pre class="pre"><strong>for</strong> all <em>x</em> <strong>do</strong><br/>  <strong>for</strong> all <em>y</em> <strong>do</strong><br/>    compute (α, β, γ) for (<em>x, y</em>)<br/>    <strong>if</strong> α ∈ (0, 1) and β ∈ (0, 1) and γ ∈ (0, 1) <strong>then</strong><br/>       <strong>t</strong> = α<strong>t</strong><sub>0</sub> + β<strong>t</strong><sub>1</sub> + γ<strong>t</strong><sub>2</sub><br/>       drawpixel (<em>x, y</em>) with color texture(<strong>t</strong>) for a solid texture<br/>       or with texture(β, γ) for a 2D texture.</pre>
<p class="noindent1">This code will generate images, but there is a problem. To unravel the basic problem, let’s consider the progression from world space <strong>q</strong> to homogeneous point <strong>r</strong> to homogenized point <strong>s</strong>:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>q</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>q</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>q</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:mn>1</m:mn></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:munder><m:mrow><m:mtext>transform</m:mtext></m:mrow><m:mrow><m:mo>→</m:mo></m:mrow></m:munder><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>h</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:munder><m:mrow><m:mtext>homogenize</m:mtext></m:mrow><m:mrow><m:mo>→</m:mo></m:mrow></m:munder><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>/</m:mo><m:msub><m:mrow><m:mi>h</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>/</m:mo><m:msub><m:mrow><m:mi>h</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>/</m:mo><m:msub><m:mrow><m:mi>h</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:mn>1</m:mn></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:mo>≡</m:mo><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>s</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>s</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>s</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:mn>1</m:mn></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>The simplest form of the texture coordinate interpolation problem is when we have texture coordinates (<em>u, v</em>) associated with two points, <strong>q</strong> and <strong>Q</strong>, and we need to generate texture coordinates in the image along the line between <strong>s</strong> and <strong>S</strong>. Ifthe world-space point <strong>q</strong>′ that is on the line between <strong>q</strong> and <strong>Q</strong> projects to the screen-space point <strong>s</strong>′ on the line between <strong>s</strong> and <strong>S</strong>, then the two points should have the same texture coordinates.</p>
<p>The naïve screen-space approach, embodied by the algorithm above, says that at the point <strong>s</strong>′ = <strong>s</strong> + α(<strong>S</strong> – <strong>s</strong>), we should use texture coordinates <em>u<sub>s</sub></em> + α(<em>u<sub>S</sub></em> – <em>u<sub>s</sub></em>) and <em>v<sub>s</sub></em> + α(<em>v<sub>S</sub></em> – <em>v<sub>s</sub></em>) . This doesn’t work correctly because the world-space point <strong>q</strong>′ that transforms to <strong>s</strong>′ is <em>not</em> <strong>q</strong> + α(<strong>Q</strong> – <strong>q</strong>).</p>
<p>However, we know from <a href="C13_chapter8.xhtml#sec8_4">Section 8.4</a> that the points on the line segment between <strong>q</strong> and <strong>Q</strong> do end up somewhere on the line segment between <strong>s</strong> and <strong>S</strong>; infact, <span aria-label="187" epub:type="pagebreak" id="pg_187" role="doc-pagebreak"/>in that section we showed that</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtext mathvariant="bold">q</m:mtext><m:mo>+</m:mo><m:mi>t</m:mi><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">Q</m:mtext><m:mo>−</m:mo><m:mtext mathvariant="bold">q</m:mtext><m:mo>)</m:mo></m:mrow><m:mo>↦</m:mo><m:mtext>s</m:mtext><m:mo>+</m:mo><m:mi>α</m:mi><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">S</m:mtext><m:mo>−</m:mo><m:mtext>s</m:mtext><m:mo>)</m:mo></m:mrow><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>The interpolation parameters <em>t</em> and α are not the same, but we can compute one from the other:<a epub:type="noteref" href="C14_chapter9.xhtml#fn9_1" id="rfn9_1" role="doc-noteref"><sup>1</sup></a></p>
<div class="disp-formula" id="equ9_2">
<m:math alttext=""><m:mrow><m:mtable><m:mtr><m:mtd><m:mi>t</m:mi><m:mrow><m:mo>(</m:mo><m:mi>α</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mi>α</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>R</m:mi></m:mrow></m:msub><m:mo>+</m:mo><m:mi>α</m:mi><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>R</m:mi></m:mrow></m:msub><m:mo>)</m:mo></m:mrow></m:mrow></m:mfrac></m:mtd><m:mtd><m:mtext>and</m:mtext></m:mtd><m:mtd><m:mi>α</m:mi><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>t</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>R</m:mi></m:mrow></m:msub><m:mi>t</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>+</m:mo><m:mi>t</m:mi><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>R</m:mi></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>)</m:mo></m:mrow></m:mrow></m:mfrac><m:mn>.</m:mn></m:mrow></m:mtd></m:mtr></m:mtable></m:mrow><m:mspace width="3em"/><m:mo>(9.2)</m:mo></m:math>
</div>
<p>These equations provide one possible fix to the screen-space interpolation idea. To get texture coordinates for the screen-space point <strong>s</strong>′ = <strong>s</strong> + α(<strong>S</strong> – <strong>s</strong>), compute <em>u</em>′<em><sub>s</sub></em> = <em>u<sub>s</sub></em> + <em>t</em>(α)(<em>u<sub>S</sub></em> – <em>u<sub>s</sub></em>) and <em>v</em>′<em><sub>s</sub></em> = <em>v<sub>s</sub></em> + <em>t</em>(α)(<em>v<sub>S</sub></em> – <em>v<sub>s</sub></em>). These are the coordinates of the point <strong>q</strong>′ that maps to <strong>s</strong>′, so this will work. However, it is slow to evaluate <em>t</em>(α) for each fragment, and there is a simpler way.</p>
<p>The key observation is that because, as we know, the perspective transform preserves lines and planes, it is safe to linearly interpolate any attributes we want across triangles, but only as long as they go through the perspective transformation along with the points (<a href="C14_chapter9.xhtml#f9_8">Figure 9.8</a>). To get a geometric intuition for this, reduce the dimension so that we have homogeneous points (<em>x<sub>r</sub></em>,<em>y<sub>r</sub></em>,<em>w<sub>r</sub></em>) and a single attribute <em>u</em> being interpolated. The attribute <em>u</em> is supposed to be a linear function of <em>x<sub>r</sub></em> and <em>y<sub>r</sub></em>,soifweplot <em>u</em> as a height field over (<em>x<sub>r</sub></em>,<em>y<sub>r</sub></em>), the result is a plane. Now, if we think of <em>u</em> as a third spatial coordinate (call it <em>u<sub>r</sub></em> to emphasize that it’s treated the same as the others) and send the whole 3D homogeneous point (<em>x<sub>r</sub></em>,<em>y<sub>r</sub></em>,<em>u<sub>r</sub></em>,<em>w<sub>r</sub></em>) through the perspective transformation, the result (<em>x<sub>s</sub></em>,<em>y<sub>s</sub></em>,<em>u<sub>s</sub></em>) still generates points that lie on a plane. There will be some warping within the plane, but the plane stays flat. This means that <em>u<sub>s</sub></em> is a linear function of (<em>x<sub>s</sub></em>,<em>y<sub>s</sub></em>)—which is to say, we can compute <em>u<sub>s</sub></em> anywhere by using linear interpolation based on the coordinates (<em>x<sub>s</sub></em>,<em>y<sub>s</sub></em>).</p>
<figure id="f9_8" tabindex="0">
<img alt="" src="../images/fig9_9.jpg"/>
<figcaption><p><span class="blue">Figure 9.8.</span> Geometric reasoning for screen-space interpolation. Top: <em>u<sub>r</sub></em> is to be interpolated as a linear function of (<em>x<sub>r</sub></em>,<em>y<sub>r</sub></em>). Bottom: after a perspective transformation from (<em>x<sub>r</sub></em>,<em>y<sub>r</sub></em>,<em>u<sub>r</sub></em>,<em>w<sub>r</sub></em>) to (<em>x<sub>s</sub></em>,<em>y<sub>s</sub></em>,<em>u<sub>s</sub></em>, 1), <em>u<sub>s</sub></em> is a linear function of (<em>x<sub>s</sub></em>,<em>y<sub>s</sub></em>).</p></figcaption>
</figure>
<p>Returning to the full problem, we need to interpolate texture coordinates (<em>u, v</em>) that are linear functions of the world space coordinates (<em>x<sub>q</sub></em>,<em>y<sub>q</sub></em>,<em>z<sub>q</sub></em>). After transforming the points to screen space, and adding the texture coordinates as if they were additional coordinates, we have</p>
<div class="disp-formula" id="equ9_3">
<m:math alttext=""><m:mrow><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:mi>u</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>v</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mn>1</m:mn></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo><m:munder><m:mrow><m:mtext>homogenize</m:mtext></m:mrow><m:mrow><m:mo>→</m:mo></m:mrow></m:munder><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:mi>u</m:mi><m:mo>/</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>v</m:mi><m:mo>/</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:mn>1</m:mn><m:mo>/</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>/</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>s</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>/</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:msub><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>s</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>/</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>s</m:mi></m:mrow></m:msub></m:mtd></m:mtr><m:mtr><m:mtd><m:mn>1</m:mn></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:mn>.</m:mn></m:mrow></m:mrow><m:mspace width="3em"/><m:mo>(9.3)</m:mo></m:math>
</div>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn9_1" id="fn9_1"><sup>1</sup></a> It is worthwhile to derive these functions yourself from Equation (7.6); in that chapter’s notation, α = <em>f</em>(<em>t</em>).</p></aside>
<p class="noindent1"><span aria-label="188" epub:type="pagebreak" id="pg_188" role="doc-pagebreak"/>The practical implication of the previous paragraph is that we <em>can</em> go ahead and interpolate all of these quantities based on the values of (<em>x<sub>s</sub></em>,<em>y<sub>s</sub></em>)—including the value <em>z<sub>s</sub></em>, used in the z-buffer. The problem with the naïve approach is simply that we are interpolating components selected inconsistently—as long as the quantities involved are from before or all from after the perspective divide, all will be well.</p>
<p>The one remaining problem is that (<em>u/w<sub>r</sub></em>,<em>v/w<sub>r</sub></em>) is not directly useful for looking up texture data; we need (<em>u, v</em>). This explains the purpose of the extra parameter we slipped into (9.3), whose value is always 1: once we have <em>u/w<sub>r</sub></em>, <em>v/w<sub>r</sub></em>,and 1<em>/w<sub>r</sub></em>, we can easily recover (<em>u, v</em>) by dividing.</p>
<p>To verify that this is all correct, let’s check that interpolating the quantity 1<em>/w<sub>r</sub></em> in screen space indeed produces the reciprocal of the interpolated <em>w<sub>r</sub></em> in world space. To see this is true, confirm (Exercise 2):</p>
<div class="disp-formula" id="equ9_4">
<m:math alttext=""><m:mrow><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mrow></m:mfrac><m:mo>+</m:mo><m:mi>α</m:mi><m:mrow><m:mo>(</m:mo><m:mi>t</m:mi><m:mo>)</m:mo></m:mrow><m:mrow><m:mo>(</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>R</m:mi></m:mrow></m:msub></m:mrow></m:mfrac><m:mo>−</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub></m:mrow></m:mfrac><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:msubsup><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow><m:mrow><m:mo>′</m:mo></m:mrow></m:msubsup></m:mrow></m:mfrac><m:mo>=</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>+</m:mo><m:mi>t</m:mi><m:mrow><m:mo>(</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>R</m:mi></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>)</m:mo></m:mrow></m:mrow></m:mfrac></m:mrow><m:mspace width="3em"/><m:mo>(9.4)</m:mo></m:math>
</div>
<p>remembering that α(<em>t</em>) and <em>t</em> are related by Equation 9.2.</p>
<p>This ability to interpolate 1<em>/w<sub>r</sub></em> linearly with no error in the transformed space allows us to correctly texture triangles. We can use these facts to modify our scan-conversion code for three points <strong>t</strong><em><sub>i</sub></em> = (<em>x<sub>i</sub></em>,<em>y<sub>i</sub></em>,<em>z<sub>i</sub></em>,<em>w<sub>i</sub></em>) that have been passed through the viewing matrices, but have not been homogenized, complete with texture coordinates <strong>t</strong><em><sub>i</sub></em> = (<em>u<sub>i</sub></em>,<em>v<sub>i</sub></em>):</p>
<pre class="pre"><strong>for</strong> all <em>x<sub>s</sub></em> <strong>do</strong><br/>  <strong>for</strong> all <em>y<sub>s</sub></em> <strong>do</strong><br/>   compute (α, β, γ) for (<em>x<sub>s</sub></em>,<em>y<sub>s</sub></em>)<br/>   <strong>if</strong> (α ∈ [0, 1] and β ∈ [0, 1] and γ ∈ [0, 1]) <strong>then</strong><br/>       <em>u<sub>s</sub></em> = α(<em>u</em><sub>0</sub><em>/w</em><sub>0</sub>) + β(<em>u</em><sub>1</sub><em>/w</em><sub>1</sub>) + γ(<em>u</em><sub>2</sub><em>/w</em><sub>2</sub>)<br/>       <em>v<sub>s</sub></em> = α(<em>v</em><sub>0</sub><em>/w</em><sub>0</sub>) + β(<em>v</em><sub>1</sub><em>/w</em><sub>1</sub>) + γ(<em>v</em><sub>2</sub><em>/w</em><sub>2</sub>)<br/>       1<em><sub>s</sub></em> = α(1<em>/w</em><sub>0</sub>) + β(1<em>/w</em><sub>1</sub>) + γ(1<em>/w</em><sub>2</sub>)<br/>       <em>u</em> = <em>u<sub>s</sub>/</em>1<em><sub>s</sub></em><br/>       <em>v</em> = <em>v<sub>s</sub>/</em>1<em><sub>s</sub></em><br/>       drawpixel (<em>x<sub>s</sub></em>,<em>y<sub>s</sub></em>) with color texture(<em>u, v</em>)</pre>
<p class="noindent1">Of course, many of the expressions appearing in this pseudocode would be precomputed outside the loop for speed.</p>
<p>In practice, modern systems interpolate all attributes in a perspective-correct way, unless some other method is specifically requested.</p>
</section>
<section>
<h3 id="sec9_1_4"><a id="index_term495"/><a id="index_term149"/><span aria-label="189" epub:type="pagebreak" id="pg_189" role="doc-pagebreak"/><span class="green">9.1.4 Clipping</span></h3>
<p>Simply transforming primitives into screen space and rasterizing them does not quite work by itself. This is because primitives that are outside the view volume—particularly, primitives that are behind the eye—can end up being rasterized, leading to incorrect results. For instance, consider the triangle shown in <a href="C14_chapter9.xhtml#f9_9">Figure 9.9</a>. Two vertices are in the view volume, but the third is behind the eye. The projection transformation maps this vertex to a nonsensical location behind the far plane, and if this is allowed to happen, the triangle will be rasterized incorrectly. For this reason, rasterization has to be preceded by a <em>clipping</em> operation that removes parts of primitives that could extend behind the eye.</p>
<figure id="f9_9" tabindex="0">
<img alt="" src="../images/fig9_10.jpg"/>
<figcaption><p><span class="blue">Figure 9.9.</span> The depth <em>z</em> is transformed to the depth <em>z <sup/></em> by the perspective transform. Note that when <em>z</em> moves from positive to negative, <em>z <sup/></em> switches from negative to positive. Thus vertices behind the eye are moved in front of the eye beyond <em>z <sup/></em> = <em>n</em> + <em>f</em>. This will lead to wrong results, which is why the triangle is first clipped to ensure all vertices are in front of the eye.</p></figcaption>
</figure>
<p>Clipping is a common operation in graphics, needed whenever one geometric entity “cuts” another. For example, if you clip a triangle against the plane <em>x</em> = 0, the plane cuts the triangle into two parts if the signs of the <em>x</em>-coordinates of the vertices are not all the same. In most applications of clipping, the portion of the triangle on the “wrong” side of the plane is discarded. This operation for a single plane is shown in <a href="C14_chapter9.xhtml#f9_10">Figure 9.10</a>.</p>
<figure id="f9_10" tabindex="0">
<img alt="" src="../images/fig9_7.jpg"/>
<figcaption><p><span class="blue">Figure 9.10.</span> A polygon is clipped against a clipping plane. The portion “inside” the plane is retained.</p></figcaption>
</figure>
<p>In clipping to prepare for rasterization, the “wrong” side is the side outside the view volume. It is always safe to clip away all geometry outside the view volume—that is, clipping against all six faces of the volume—but many systems manage to get away with only clipping against the near plane.</p>
<p>This section discusses the basic implementation of a clipping module. Those interested in implementing an industrial-speed clipper should see the book by Blinn mentioned in the notes at the end of this chapter.</p>
<p>The two most common approaches for implementing clipping are</p>
<ol class="list-order">
<li>
<p class="list">In world coordinates using the six planes that bound the truncated viewing pyramid,</p>
</li>
<li>
<p class="list">In the 4D transformed space before the homogeneous divide.</p>
</li>
</ol>
<p>Either possibility can be effectively implemented (J. Blinn, 1996) using the following approach for each triangle:</p>
<pre class="pre"><strong>for</strong> each of six planes <strong>do</strong><br/>  <strong>if</strong> (triangle entirely outside of plane) <strong>then</strong><br/>     break (triangle is not visible)<br/> <strong>else if</strong> triangle spans plane <strong>then</strong><br/>     clip triangle<br/> <strong>if</strong> (quadrilateral is left) <strong>then</strong><br/>    break into two triangles</pre>
<section>
<h4 id="sec38"><span aria-label="190" epub:type="pagebreak" id="pg_190" role="doc-pagebreak"/><span class="blue">Clipping Before the Transform (Option 1)</span></h4>
<p>Option 1 has a straightforward implementation. The only question is, “What are the six plane equations?” Because these equations are the same for all triangles <a id="term-221"/><a id="term-288"/><span aria-label="191" epub:type="pagebreak" id="pg_191" role="doc-pagebreak"/>rendered in the single image, we do not need to compute them very efficiently. For this reason, we can just invert the transform shown in <a href="C12_chapter7.xhtml#f7_12">Figure 7.12</a> and apply it to the eight vertices of the transformed view volume:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtable columnalign="right"><m:mtr><m:mtd><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo>)</m:mo><m:mo>=</m:mo><m:mrow><m:mo>(</m:mo><m:mi>l</m:mi><m:mo>,</m:mo><m:mi>b</m:mi><m:mo>,</m:mo><m:mi>n</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:mrow><m:mo>(</m:mo><m:mi>r</m:mi><m:mo>,</m:mo><m:mi>b</m:mi><m:mo>,</m:mo><m:mi>n</m:mi><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:mrow><m:mo>(</m:mo><m:mi>l</m:mi><m:mo>,</m:mo><m:mi>t</m:mi><m:mo>,</m:mo><m:mi>n</m:mi><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:mrow><m:mo>(</m:mo><m:mi>r</m:mi><m:mo>,</m:mo><m:mi>t</m:mi><m:mo>,</m:mo><m:mi>n</m:mi><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:mrow><m:mo>(</m:mo><m:mi>l</m:mi><m:mo>,</m:mo><m:mi>b</m:mi><m:mo>,</m:mo><m:mi>f</m:mi><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:mrow><m:mo>(</m:mo><m:mi>r</m:mi><m:mo>,</m:mo><m:mi>b</m:mi><m:mo>,</m:mo><m:mi>f</m:mi><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:mrow><m:mo>(</m:mo><m:mi>l</m:mi><m:mo>,</m:mo><m:mi>t</m:mi><m:mo>,</m:mo><m:mi>f</m:mi><m:mo>)</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:mrow><m:mo>(</m:mo><m:mi>r</m:mi><m:mo>,</m:mo><m:mi>t</m:mi><m:mo>,</m:mo><m:mi>f</m:mi><m:mo>)</m:mo><m:mn>.</m:mn></m:mrow></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>The plane equations can be inferred from here. Alternatively, we can use vector geometry to get the planes directly from the viewing parameters.</p>
</section>
<section>
<h4 id="sec39"><span class="blue">Clipping in Homogeneous Coordinates (Option 2)</span></h4>
<p>Surprisingly, the option usually implemented is that of clipping in homogeneous coordinates before the divide. Here, the view volume is 4D, and it is bounded by 3D volumes (hyperplanes). These are</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtable columnalign="right"><m:mtr><m:mtd><m:mo>−</m:mo><m:mi>x</m:mi><m:mo>+</m:mo><m:mi>l</m:mi><m:mi>w</m:mi><m:mo>=</m:mo><m:mn>0</m:mn><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>x</m:mi><m:mo>−</m:mo><m:mi>r</m:mi><m:mi>w</m:mi><m:mo>=</m:mo><m:mn>0</m:mn><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:mo>−</m:mo><m:mi>y</m:mi><m:mo>+</m:mo><m:mi>b</m:mi><m:mi>w</m:mi><m:mo>=</m:mo><m:mn>0</m:mn><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>y</m:mi><m:mo>−</m:mo><m:mi>t</m:mi><m:mi>w</m:mi><m:mo>=</m:mo><m:mn>0</m:mn><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:mo>−</m:mo><m:mi>z</m:mi><m:mo>+</m:mo><m:mi>n</m:mi><m:mi>w</m:mi><m:mo>=</m:mo><m:mn>0</m:mn><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>z</m:mi><m:mo>−</m:mo><m:mi>f</m:mi><m:mi>w</m:mi><m:mo>=</m:mo><m:mn>0.</m:mn></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>These planes are quite simple, so the efficiency is better than for Option 1. They still can be improved by transforming the view volume [<em>l, r</em>] × [<em>b, t</em>] × [<em>f, n</em>] to [0, 1]<sup>3</sup>. It turns out that the clipping of the triangles is not much more complicated than in 3D.</p>
</section>
<section>
<h4 id="sec40"><span class="blue">Clipping against a Plane</span></h4>
<p>No matter which option we choose, we must clip against a plane. Recall from <a href="C07_chapter2.xhtml#sec2_7_5">Section 2.7.5</a> that the implicit equation for a plane through point <strong>q</strong> with normal <strong>n</strong> is</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">p</m:mtext><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mtext mathvariant="bold">n</m:mtext><m:mo>⋅</m:mo><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">p</m:mtext><m:mo>−</m:mo><m:mtext mathvariant="bold">q</m:mtext><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mn>0.</m:mn></m:mrow></m:math>
</div>
<p><a id="term-387"/><a id="term-669"/><span aria-label="192" epub:type="pagebreak" id="pg_192" role="doc-pagebreak"/>This is often written</p>
<div class="disp-formula" id="equ9_5">
<m:math alttext=""><m:mrow><m:mi>f</m:mi><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">p</m:mtext><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mtext mathvariant="bold">n</m:mtext><m:mo>⋅</m:mo><m:mrow><m:mtext mathvariant="bold">p</m:mtext></m:mrow><m:mo>+</m:mo><m:mi>D</m:mi><m:mo>=</m:mo><m:mn>0.</m:mn></m:mrow><m:mspace width="3em"/><m:mo>(9.5)</m:mo></m:math>
</div>
<p>Interestingly, this equation not only describes a 3D plane, but also describes a line in 2D and the volume analog of a plane in 4D. All of these entities are usually called planes in their appropriate dimension.</p>
<p>If we have a line segment between points <strong>a</strong> and <strong>b</strong>, we can “clip” it against a plane using the techniques for cutting the edges of 3D triangles in BSP tree programs described in <a href="C17_chapter12.xhtml#sec12_4_3">Section 12.4.3</a>. Here, the points <strong>a</strong> and <strong>b</strong> are tested to determine whether they are on opposite sides of the plane <em>f</em> (<strong>p</strong>) = 0 by checking whether <em>f</em> (<strong>a</strong>) and <em>f</em> (<strong>b</strong>) have different signs. Typically, <em>f</em> (<strong>p</strong>) &lt; 0 is defined to be “inside” the plane, and <em>f</em> (<strong>p</strong>) &gt; 0 is “outside” the plane. If the plane does split the line, then we can solve for the intersection point by substituting the equation for the parametric line,</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mrow><m:mtext mathvariant="bold">p</m:mtext><m:mo>=</m:mo><m:mtext mathvariant="bold">a</m:mtext><m:mo>+</m:mo><m:mi>t</m:mi><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">b</m:mtext><m:mo>−</m:mo><m:mtext mathvariant="bold">a</m:mtext><m:mo>)</m:mo></m:mrow><m:mo>,</m:mo></m:mrow></m:mrow></m:math>
</div>
<p>into the <em>f</em> (<strong>p</strong>) = 0 plane of Equation (9.5). This yields</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mrow><m:mtext mathvariant="bold">n</m:mtext><m:mo mathvariant="bold">⋅</m:mo><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">a</m:mtext><m:mo>+</m:mo><m:mi>t</m:mi><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">b</m:mtext><m:mo>−</m:mo><m:mtext mathvariant="bold">a</m:mtext><m:mo>)</m:mo></m:mrow><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mi>D</m:mi><m:mo>=</m:mo><m:mn>0.</m:mn></m:mrow></m:mrow></m:math>
</div>
<p>Solving for <em>t</em> gives</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi>t</m:mi><m:mo>=</m:mo><m:mfrac><m:mrow><m:mtext mathvariant="bold">n</m:mtext><m:mo>⋅</m:mo><m:mtext mathvariant="bold">a</m:mtext><m:mo>+</m:mo><m:mi>D</m:mi></m:mrow><m:mrow><m:mtext mathvariant="bold">n</m:mtext><m:mo>⋅</m:mo><m:mrow><m:mo>(</m:mo><m:mrow><m:mtext mathvariant="bold">a</m:mtext><m:mo>−</m:mo><m:mtext mathvariant="bold">b</m:mtext></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow></m:mfrac><m:mn>.</m:mn><m:mtext> </m:mtext></m:mrow></m:math>
</div>
<p>We can then find the intersection point and “shorten” the line.</p>
<p>To clip a triangle, we again can follow <a href="C17_chapter12.xhtml#sec12_4_3">Section 12.4.3</a> to produce one or two triangles.</p>
</section>
</section>
</section>
<section>
<h2 id="sec9_2"><a epub:type="backlink" href="C02a_toc.xhtml#rsec9_2" role="doc-backlink"><span class="green">9.2 Operations Before and After Rasterization</span></a></h2>
<p>Before a primitive can be rasterized, the vertices that define it must be in screen coordinates, and the colors or other attributes that are supposed to be interpolated across the primitive must be known. Preparing this data is the job of the <em>vertex-processing</em> stage of the pipeline. In this stage, incoming vertices are transformed by the modeling, viewing, and projection transformations, mapping them from their original coordinates into screen space (where, recall, position is measured in terms of pixels). At the same time, other information, such as colors, surface normals, or texture coordinates, is transformed as needed; we’ll discuss these additional attributes in the examples below.</p>
<p>After rasterization, further processing is done to compute a color and depth for each fragment. This processing can be as simple as just passing through an interpolated color and using the depth computed by the rasterizer; or it can involve <a id="term-564"/><a id="term-737"/><a id="term-885"/><span aria-label="193" epub:type="pagebreak" id="pg_193" role="doc-pagebreak"/>complex shading operations. Finally, the blending phase combines the fragments generated by the (possibly several) primitives that overlapped each pixel to compute the final color. The most common blending approach is to choose the color of the fragment with the smallest depth (closest to the eye).</p>
<p>The purposes of the different stages are best illustrated by examples.</p>
<section>
<h3 id="sec9_2_1"><span class="green">9.2.1 Simple 2D Drawing</span></h3>
<p>The simplest possible pipeline does nothing in the vertex or fragment stages, and in the blending stage, the color of each fragment simply overwrites the value of the previous one. The application supplies primitives directly in pixel coordinates, and the rasterizer does all the work. This basic arrangement is the essence of many simple, older APIs for drawing user interfaces, plots, graphs, and other 2D content. Solid color shapes can be drawn by specifying the same color for all vertices of each primitive, and our model pipeline also supports smoothly varying color using interpolation.</p>
</section>
<section>
<h3 id="sec9_2_2"><span class="green">9.2.2 A Minimal 3D Pipeline</span></h3>
<p>To draw objects in 3D, the only change needed to the <a id="index_term1289"/>2D drawing pipeline is a single matrix transformation: the vertex-processing stage multiplies the incoming vertex positions by the product of the modeling, camera, projection, and viewport matrices, resulting in screen-space triangles that are then drawn in the same way as if they’d been specified directly in 2D.</p>
<p>One problem with the minimal 3D pipeline is that in order to get occlusion relationships correct—to get nearer objects in front of farther away objects—primitives must be drawn in back-to-front order. This is known as the <em>painter’s algorithm</em> for hidden <a id="index_term1144"/>surface removal, by analogy to painting the background of a painting first, and then painting the foreground over it. The <a id="index_term816"/>painter’s algorithm is a perfectly valid way to remove hidden surfaces, but it has several drawbacks. It cannot handle triangles that intersect one another, because there is no correct order in which to draw them. Similarly, several triangles, even if they don’t intersect, can still be arranged in an <em>occlusion cycle</em>, as shown in <a href="C14_chapter9.xhtml#f9_11">Figure 9.11</a>, another case in which the back-to-front order does not exist. And most importantly, sorting the primitives by depth is slow, especially for large scenes, and disturbs the efficient flow of data that makes object-order rendering so fast. <a href="C14_chapter9.xhtml#f9_12">Figure 9.12</a> shows the result of this process when the objects are not sorted by depth.</p>
<figure id="f9_11" tabindex="0">
<img alt="" src="../images/fig9_11.jpg"/>
<figcaption><p><span class="blue">Figure 9.11.</span> Two <a id="index_term797"/>occlusion cycles, which cannot be drawn in back-to-front order.</p></figcaption>
</figure>
<figure id="f9_12" tabindex="0">
<img alt="" src="../images/fig9_12.jpg"/>
<figcaption><p><span class="blue">Figure 9.12.</span> The result of drawing two spheres of identical size using the minimal pipeline. The sphere that appears smaller is farther away but is drawn last, so it incorrectly overwrites the nearer one.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec9_2_3"><a id="index_term1468"/><a id="term-262"/><a id="term-1144"/><span aria-label="194" epub:type="pagebreak" id="pg_194" role="doc-pagebreak"/><span class="green">9.2.3 Using a z-Buffer for Hidden Surfaces</span></h3>
<p>In practice, the painter’s algorithm is rarely used; instead, a simple and effective <a id="index_term523"/>hidden surface removal algorithm known as the <em>z-buffer</em> algorithm is used. The method is very simple: at each pixel, we keep track of the distance to the closest surface that has been drawn so far, and we throw away fragments that are farther away than that distance. The closest distance is stored by allocating an extra value for each pixel, in addition to the red, green, and blue color values, which is known as the depth, or z-value. The <em><a id="index_term318"/>depth buffer</em>, or z-buffer, is the name for the grid of depth values.</p>
<p>The z-buffer algorithm is implemented in the fragment blending phase, by comparing the depth of each fragment with the current value stored in the z-buffer. If the fragment’s depth is closer, both its color and its depth value overwrite the values currently in the color and depth buffers. If the fragment’s depth is farther away, it is discarded. To ensure that the first fragment will pass the depth test, the <em>z</em> buffer is initialized to the maximum depth (the depth of the far plane). Irrespective of the order in which surfaces are drawn, the same fragment will win the depth test, and the image will be the same.</p>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent">Of course there can be ties in the depth test, in which case the order may well matter.</p>
</aside>
<p class="indent">The z-buffer algorithm requires each fragment to carry a depth. This is done simply by interpolating the <em>z</em>-coordinate as a vertex attribute, in the same way that color or other attributes are interpolated.</p>
<p>The z-buffer is such a simple and practical way to deal with hidden surfaces in object-order rendering that it is by far the dominant approach. It is much simpler than geometric methods that cut surfaces into pieces that can be sorted by depth, because it avoids solving any problems that don’t need to be solved. The depth order only needs to be determined at the locations of the pixels, and that is all that the z-buffer does. It is universally supported by hardware graphics pipelines and is also the most commonly used method for software pipelines. <a href="C14_chapter9.xhtml#f9_13">Figures 9.13</a> and <a href="C14_chapter9.xhtml#f9_14">9.14</a> show example results.</p>
<figure id="f9_13" tabindex="0">
<img alt="" src="../images/fig9_13.jpg"/>
<figcaption><p><span class="blue">Figure 9.13.</span> The result of drawing the same two spheres using the z-buffer.</p></figcaption>
</figure>
<figure id="f9_14" tabindex="0">
<img alt="" src="../images/fig9_14.jpg"/>
<figcaption><p><span class="blue">Figure 9.14.</span> A z-buffer rasterizing two triangles in each of two possible orders. The first triangle is fully rasterized. The second triangle has every pixel computed, but for three of the pixels, the depth-contest is lost, and those pixels are not drawn. The final image is the same regardless.</p></figcaption>
</figure>
<section>
<h4 id="sec41"><span class="blue">Precision Issues</span></h4>
<p>In practice, the <em>z</em>-values stored in the buffer are nonnegative integers. This is preferable to true floats because the fast memory needed for the z-buffer is somewhat expensive and is worth keeping to a minimum.</p>
<p>The use of integers can cause some precision problems. If we use an integer range having <em>B</em> values <em>{</em>0, 1,<em>...,B</em> – 1<em>}</em>, we can map 0 to the near clipping plane <em>z</em> = <em>n</em> and <em>B</em> –1 to the far clipping plane <em>z</em> = <em>f</em> . Note, that for this discussion, we assume <em>z</em>, <em>n</em>,and <em>f</em> are positive. This will result in the same results as the negative case, but the details of the argument are easier to follow. We send each <em>z</em>-value to <a id="term-263"/><a id="term-1145"/><span aria-label="195" epub:type="pagebreak" id="pg_195" role="doc-pagebreak"/>a “bucket” with depth Δ<em>z</em> = (<em>f</em> – <em>n</em>)<em>/B</em>. We would not use the integer z-buffer if memory were not a premium, so it is useful to make <em>B</em> as small as possible.</p>
<p>If we allocate <em>b</em> bits to store the <em>z</em>-value, then <em>B</em> = 2<em><sup>b</sup></em>. We need enough bits to make sure any triangle in front of another triangle will have its depth mapped to distinct depth bins.</p>
<p>For example, if you are rendering a scene where triangles have a separation of at least one meter, then Δ<em>z &lt;</em> 1 should yield images without artifacts. There are <a id="term-769"/><span aria-label="196" epub:type="pagebreak" id="pg_196" role="doc-pagebreak"/>two ways to make Δ<em>z</em> smaller: move <em>n</em> and <em>f</em> closer together or increase <em>b</em>. If <em>b</em> is fixed, as it may be in APIs or on particular hardware platforms, adjusting <em>n</em> and <em>f</em> is the only option.</p>
<p>The precision of z-buffers must be handled with great care when perspective images are created. The value Δ<em>z</em> above is used <em>after</em> the perspective divide. Recall from <a href="C13_chapter8.xhtml#sec8_3">Section 8.3</a> that the result of the perspective divide is</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi>z</m:mi><m:mo>=</m:mo><m:mi>n</m:mi><m:mo>+</m:mo><m:mi>f</m:mi><m:mo>−</m:mo><m:mfrac><m:mrow><m:msub><m:mrow><m:mi>f</m:mi></m:mrow><m:mrow><m:mi>n</m:mi></m:mrow></m:msub></m:mrow><m:mrow><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>w</m:mi></m:mrow></m:msub></m:mrow></m:mfrac><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>The actual bin depth is related to <em>z<sub>w</sub></em>, the world depth, rather than <em>z</em>, the post-perspective divide depth. We can approximate the bin size by differentiating both sides:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi mathvariant="normal">Δ</m:mi><m:mi>z</m:mi><m:mo>≈</m:mo><m:mfrac><m:mrow><m:msub><m:mrow><m:mi>f</m:mi></m:mrow><m:mrow><m:mi>n</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">Δ</m:mi><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>w</m:mi></m:mrow></m:msub></m:mrow><m:mrow><m:msubsup><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msubsup></m:mrow></m:mfrac><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>Bin sizes vary in depth. The bin size in world space is</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi mathvariant="normal">Δ</m:mi><m:msub><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>w</m:mi></m:mrow></m:msub><m:mo>≈</m:mo><m:mfrac><m:mrow><m:msubsup><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msubsup><m:mi mathvariant="normal">Δ</m:mi><m:mi>z</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi>f</m:mi></m:mrow><m:mrow><m:mi>n</m:mi></m:mrow></m:msub></m:mrow></m:mfrac><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>Note that the quantity Δ<em>z</em> is as previously discussed. The biggest bin will be for <em>z</em> = <em>f</em> ,where</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi mathvariant="normal">Δ</m:mi><m:msubsup><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>w</m:mi></m:mrow><m:mrow><m:mtext>max</m:mtext></m:mrow></m:msubsup><m:mo>≈</m:mo><m:mfrac><m:mrow><m:mi>f</m:mi><m:mi mathvariant="normal">Δ</m:mi><m:mi>z</m:mi></m:mrow><m:mrow><m:mi>n</m:mi></m:mrow></m:mfrac><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>Note that choosing <em>n</em> = 0, a natural choice if we don’t want to lose objects right in front of the eye, will result in an infinitely large bin—a very bad condition. To make Δ<em>z</em><sup>max</sup><em><sub>w</sub></em> as small as possible, we want to minimize <em>f</em> and maximize <em>n</em>. Thus, it is always important to choose <em>n</em> and <em>f</em> carefully.</p>
</section>
</section>
<section>
<h3 id="sec9_2_4"><a id="index_term1053"/><span class="green">9.2.4 Per-vertex Shading</span></h3>
<p>So far the application sending triangles into the pipeline is responsible for setting the color; the rasterizer just interpolates the colors and they are written directly into the output image. For some applications, this is sufficient, but in many cases, we want 3D objects to be drawn with shading, using the same illumination equations that we used for image-order rendering in <a href="C09_chapter4.xhtml#c4">Chapter 4</a>. Recall that these equations require a light direction, an eye direction, and a surface normal to compute the color of a surface.</p>
<p>One way to handle shading computations is to perform them in the vertex stage. The application provides normal vectors at the vertices, and the positions and colors of the lights are provided separately (they don’t vary across the surface, <a id="term-589"/><a id="term-592"/><a id="term-761"/><a id="term-764"/><a id="term-765"/><a id="term-770"/><span aria-label="197" epub:type="pagebreak" id="pg_197" role="doc-pagebreak"/>so they don’t need to be specified for each vertex). For each vertex, the direction to the viewer and the direction to each light are computed based on the positions of the camera, the lights, and the vertex. The desired shading equation is evaluated to compute a color, which is then passed to the rasterizer as the vertex color. Per-vertex shading is sometimes called <em><a id="index_term1047"/>Gouraud shading</em>.</p>
<p>One decision to be made is the coordinate system in which shading computations are done. World space or eye space are good choices. It is important to choose a coordinate system that is orthonormal when viewed in world space, because shading equations depend on angles between vectors, which are not preserved by operations like nonuniform scale that are often used in the modeling transformation, or perspective projection, often used in the projection to the canonical view volume. Shading in eye space has the advantage that we don’t need to keep track of the camera position, because the camera is always at the origin in eye space, in perspective projection, or the view direction is always +<em>z</em> in orthographic projection.</p>
<p>Per-vertex shading has the disadvantage that it cannot produce any details in the shading that are smaller than the primitives used to draw the surface, because it only computes shading once for each vertex and never in between vertices. For instance, in a room with a floor that is drawn using two large triangles and illuminated by a light source in the middle of the room, shading will be evaluated only at the corners of the room, and the interpolated value will likely be much too dark in the center. Also, curved surfaces that are shaded with specular highlights must be drawn using primitives small enough that the highlights can be resolved.</p>
<p><a href="C14_chapter9.xhtml#f9_15">Figure 9.15</a> shows our two spheres drawn with per-vertex shading.</p>
<figure id="f9_15" tabindex="0">
<img alt="" src="../images/fig9_15.jpg"/>
<figcaption><p><span class="blue">Figure 9.15.</span> Two spheres drawn using per-vertex (Gouraud) shading. Because the triangles are large, interpolation artifacts are visible.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec9_2_5"><span class="green">9.2.5 Per-fragment Shading</span></h3>
<p>To avoid the interpolation artifacts associated with per-vertex shading, we can avoid interpolating colors by performing the shading computations <em>after</em> the interpolation, in the fragment stage. In per-fragment shading, the same shading equations are evaluated, but they are evaluated for each fragment using interpolated vectors, rather than for each vertex using the vectors from the application.</p>
<p>In per-fragment shading, the geometric information needed for shading is passed through the rasterizer as attributes, so the vertex stage must coordinate with the fragment stage to prepare the data appropriately. One approach is to interpolate the eye-space surface normal and the eye-space vertex position, which then can be used just as they would in per-vertex shading.</p>
<p><a href="C14_chapter9.xhtml#f9_16">Figure 9.16</a> shows our two spheres drawn with per-fragment shading.</p>
<figure id="f9_16" tabindex="0">
<img alt="" src="../images/fig9_16.jpg"/>
<figcaption><p><span class="blue">Figure 9.16.</span> Two spheres drawn using per-fragment shading. Because the triangles are large, interpolation artifacts are visible.</p></figcaption>
</figure>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent"><a id="index_term1051"/>Per-fragment shading is sometimes called Phong shading, which is confusing because the same name is attached to the Phong illumination model.</p>
</aside>
</section>
<section>
<h3 id="sec9_2_6"><a id="term-871"/><span aria-label="198" epub:type="pagebreak" id="pg_198" role="doc-pagebreak"/><span class="green">9.2.6 Texture Mapping</span></h3>
<p><em>Textures</em> (discussed in <a href="C16_chapter11.xhtml#c11">Chapter 11</a>) are images that are used to add extra detail to the shading of surfaces that would otherwise look too homogeneous and artificial. The idea is simple: each time shading is computed, we read one of the values used in the shading computation—the diffuse color, for instance—from a texture instead of using the attribute values that are attached to the geometry being rendered. This operation is known as a <em>texture lookup</em>: the shading code specifies a <em>texture coordinate</em>, a point in the domain of the texture, and the texture-mapping system finds the value at that point in the texture image and returns it. The texture value is then used in the shading computation.</p>
<p>The most common way to define texture coordinates is simply to make the texture coordinate another vertex attribute. Each primitive then knows where it lives in the texture.</p>
</section>
<section>
<h3 id="sec9_2_7"><span class="green">9.2.7 Shading Frequency</span></h3>
<p>The decision about where to place shading computations depends on how fast the color changes—the <em>scale</em> of the details being computed. Shading with large-scale features, such as diffuse shading on curved surfaces, can be evaluated fairly infrequently and then interpolated: it can be computed with a low <em>shading frequency</em>. Shading that produces small-scale features, such as sharp highlights or detailed textures, needs to be evaluated at a high shading frequency. For details that need to look sharp and crisp in the image, the shading frequency needs to be at least one shading sample per pixel.</p>
<p>So large-scale effects can safely be computed in the vertex stage, even when the vertices defining the primitives are many pixels apart. Effects that require a high shading frequency can also be computed at the vertex stage, as long as the vertices are close together in the image; alternatively, they can be computed at the fragment stage when primitives are larger than a pixel.</p>
<p>For example, a hardware pipeline as used in a computer game, generally using primitives that cover several pixels to ensure high efficiency, normally does most shading computations per fragment. On the other hand, the PhotoRealistic RenderMan system does all shading computations per vertex, after first subdividing, or <em>dicing</em>, all surfaces into small quadrilaterals called <em><a id="index_term739"/>micropolygons</em> that are about the size of pixels. Since the primitives are small, per-vertex shading in this system achieves a high <a id="index_term1063"/>shading frequency that is suitable for detailed shading.</p>
</section>
</section>
<section>
<h2 id="sec9_3"><a id="index_term51"/><a id="index_term10"/><a id="term-24"/><span aria-label="199" epub:type="pagebreak" id="pg_199" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec9_3" role="doc-backlink"><span class="green">9.3 Simple Antialiasing</span></a></h2>
<p>Just as with ray tracing, rasterization will produce jagged lines and triangle edges if we make an all-or-nothing determination of whether each pixel is inside the primitive or not. In fact, the set of fragments generated by the simple triangle rasterization algorithms described in this chapter, sometimes called standard or <em>aliased</em> rasterization, is exactly the same as the set of pixels that would be mapped to that triangle by a ray tracer that sends one ray through the center of each pixel. Also as in ray tracing, the solution is to allow pixels to be partly covered by a primitive (Crow, 1978). In practice, this form of blurring helps visual quality, especially in animations. This is shown as the top line of <a href="C14_chapter9.xhtml#f9_17">Figure 9.17</a>.</p>
<figure id="f9_17" tabindex="0">
<img alt="" src="../images/fig9_17.jpg"/>
<figcaption><p><span class="blue">Figure 9.17.</span> An antialiased and a jaggy line viewed at close range so individual pixels are visible.</p></figcaption>
</figure>
<p>There are a number of different approaches to antialiasing in rasterization applications. Just as with a ray tracer, we can produce an antialiased image by setting each pixel value to the average color of the image over the square area belonging to the pixel, an approach known as <em>box filtering</em>. This means we have to think of all drawable entities as having well-defined areas. For example, the line in <a href="C14_chapter9.xhtml#f9_17">Figure 9.17</a> can be thought of as approximating a one-pixel-wide rectangle.</p>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent">There are better filters than the box, but a <a id="index_term102"/>box filter will suffice for all but the most demanding applications.</p>
</aside>
<p class="indent">The easiest way to implement box-filter antialiasing is by <em><a id="index_term1137"/>supersampling</em>: create images at very high resolutions and then downsample. For example, if our goal is a 256 × 256 pixel image of a line with width 1.2 pixels, we could rasterize a rectangle version of the line with width 4.8 pixels on a 1024 × 1024 screen, and then average 4 × 4 groups of pixels to get the colors for each of the 256 × 256 pixels in the “shrunken” image. This is an approximation of the actual boxfiltered image, but works well when objects are not extremely small relative to the distance between pixels.</p>
<p><span aria-label="200" epub:type="pagebreak" id="pg_200" role="doc-pagebreak"/>Supersampling is quite expensive, however. Because the very sharp edges that cause aliasing are normally caused by the edges of <a id="index_term894"/>primitives, rather than sudden variations in shading within a primitive, a widely used optimization is to sample visibility at a higher rate than shading. If information about coverage and depth is stored for several points within each pixel, very good antialiasing can be achieved even if only one color is computed. In systems like RenderMan that use per-vertex shading, this is achieved by rasterizing at high resolution: it is inexpensive to do so because shading is simply interpolated to produce colors for the many fragments, or visibility samples. In systems with per-fragment shading, such as hardware pipelines, <em>multisample antialiasing</em> is achieved by storing for each fragment a single color plus a coverage mask and a set of depth values.</p>
</section>
<section>
<h2 id="sec9_4"><a id="index_term285"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec9_4" role="doc-backlink"><span class="green">9.4 Culling Primitives for Efficiency</span></a></h2>
<p>The strength of object-order rendering, that it requires a single pass over all the geometry in the scene, is also a weakness for complex scenes. For instance, in a model of an entire city, only a few buildings are likely to be visible at any given time. A correct image can be obtained by drawing all the primitives in the scene, but a great deal of effort will be wasted processing geometry that is behind the visible buildings, or behind the viewer, and therefore doesn’t contribute to the final image.</p>
<p>Identifying and throwing away invisible geometry to save the time that would be spent processing it is known as <em>culling</em>. Three commonly implemented culling strategies (often used in tandem) are</p>
<ul class="list-bullet">
<li>
<p class="list"><span style="color:green">view volume culling</span>—the removal of geometry that is outside the view volume;</p>
</li>
<li>
<p class="list"><span style="color:green">occlusion culling</span>—the removal of geometry that may be within the view volume but is obscured, or occluded, by other geometry closer to the camera;</p>
</li>
<li>
<p class="list"><span style="color:green">backface culling</span>—the removal of primitives facing away from the camera.</p>
</li>
</ul>
<p>We will briefly discuss view volume culling and <a id="index_term68"/>backface culling, but culling in high performance systems is a complex topic; see <a id="index_term5"/>Akenine-Möller, <a id="index_term510"/>Haines, and <a id="index_term526"/>Hoffman (2008) for a complete discussion and for information about <a id="index_term796"/>occlusion culling.</p>
<section>
<h3 id="sec9_4_1"><span aria-label="201" epub:type="pagebreak" id="pg_201" role="doc-pagebreak"/><span class="green">9.4.1 View Volume Culling</span></h3>
<p>When an entire primitive lies outside the view volume, it can be culled, since it will produce no fragments when rasterized. If we can cull many primitives with a quick test, we may be able to speed up drawing significantly. On the other hand, testing primitives individually to decide exactly which ones need to be drawn may cost more than just letting the rasterizer eliminate them.</p>
<p>View <a id="index_term1457"/>volume culling, also known as <em><a id="index_term1314"/>view frustum culling</em>, is especially helpful when many triangles are grouped into an object with an associated bounding volume. If the bounding volume lies outside the view volume, then so do all the triangles that make up the object. For example, if we have 1000 triangles bounded by a single sphere with center <strong>c</strong> and radius <em>r</em>, we can check whether the sphere lies outside the clipping plane,</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">p</m:mtext><m:mo>−</m:mo><m:mtext mathvariant="bold">a</m:mtext><m:mo>)</m:mo><m:mo>⋅</m:mo><m:mtext mathvariant="bold">n</m:mtext><m:mo>=</m:mo><m:mn>0</m:mn><m:mo>,</m:mo></m:mrow></m:mrow></m:math>
</div>
<p>where <strong>a</strong> is a point on the plane, and <strong>p</strong> is a variable. This is equivalent to checking whether the signed distance from the center of the sphere <strong>c</strong> to the plane is greater than +<em>r</em>. This amounts to the check that</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mrow><m:mfrac><m:mrow><m:mrow><m:mo>(</m:mo><m:mtext mathvariant="bold">c</m:mtext><m:mo>−</m:mo><m:mtext mathvariant="bold">a</m:mtext><m:mo>)</m:mo><m:mo>⋅</m:mo><m:mtext mathvariant="bold">n</m:mtext></m:mrow></m:mrow><m:mrow><m:mrow><m:mo>‖</m:mo><m:mtext mathvariant="bold">n</m:mtext><m:mo>‖</m:mo></m:mrow></m:mrow></m:mfrac></m:mrow><m:mo>&gt;</m:mo><m:mi>r</m:mi><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>Note that the sphere may overlap the plane even in a case where all the triangles do lie outside the plane. Thus, this is a conservative test. How conservative the test is depends on how well the sphere bounds the object.</p>
<p>The same idea can be applied hierarchically if the scene is organized in one of the spatial data structures described in <a href="C17_chapter12.xhtml#c12">Chapter 12</a>.</p>
</section>
<section>
<h3 id="sec9_4_2"><a id="index_term69"/><span class="green">9.4.2 Backface Culling</span></h3>
<p>When polygonal models are closed, i.e., they bound a closed space with no holes, they are often assumed to have outward facing normal vectors as discussed in <a href="C10_chapter5.xhtml#c5">Chapter 5</a>. For such models, the polygons that face away from the eye are certain to be overdrawn by polygons that face the eye. Thus, those polygons can be culled before the pipeline even starts.</p>
</section>
</section>
</section>
<section>
<h2 id="sec42"><span aria-label="202" epub:type="pagebreak" id="pg_202" role="doc-pagebreak"/><span class="green">Frequently Asked Questions</span></h2>
<ul class="list-bullet">
<li>
<p class="list"><span class="green">I’ve often seen clipping discussed at length, and it is a much more involved process than that described in this chapter. What is going on here?</span></p>
<p class="noindent1b">The clipping described in this chapter works, but lacks optimizations that an industrial-strength clipper would have. These optimizations are discussed in detail in Blinn’s definitive work listed in the chapter notes.</p>
</li>
<li>
<p class="list"><span class="green">How are polygons that are not triangles rasterized?</span></p>
<p class="noindent1b">These can either be done directly scan-line by scan-line, or they can be broken down into triangles. The latter appears to be the more popular technique.</p></li>
<li>
<p class="list"><span class="green">Is it always better to antialias?</span></p>
<p class="noindent1b">No. Some images look crisper without antialiasing. Many programs use unantialiased “screen fonts” because they are easier to read.</p></li>
<li>
<p class="list"><span class="green">The documentation for my API talks about “scene graphs” and “matrix stacks.” Are these part of the graphics pipeline?</span></p>
<p class="noindent1b">The graphics pipeline is certainly designed with these in mind, and whether we define them as part of the pipeline is a matter of taste. This book delays their discussion until <a href="C17_chapter12.xhtml#c12">Chapter 12</a>.</p>
</li>
<li>
<p class="list"><span class="green">Is a uniform distance z-buffer better than the standard one that includes perspective matrix nonlinearities?</span></p>
<p class="noindent1b">It depends. One “feature” of the nonlinearities is that the z-buffer has more resolution near the eye and less in the distance. If a level-of-detail system is used, then geometry in the distance is coarser and the “unfairness” of the z-buffer can be a good thing.</p>
</li>
<li>
<p class="list"><span class="green">Is a software z-buffer ever useful?</span></p>
<p class="noindent1b">Yes. Most of the movies that use 3D computer graphics have used a variant of the software z-buffer developed by Pixar (Cook, Carpenter, &amp; Catmull, 1987).</p>
</li>
</ul>
<section>
<h2 id="sec43"><span aria-label="203" epub:type="pagebreak" id="pg_203" role="doc-pagebreak"/><span class="green">Notes</span></h2>
<p>A wonderful book about designing a graphics pipeline is <em>Jim Blinn’s Corner: A Trip Down the Graphics Pipeline</em> (J. Blinn, 1996). Many nice details of the pipeline and culling are in <em>3D Game Engine Design</em> (Eberly, 2000) and <em>Real-Time Rendering</em> (Akenine-Möller et al., 2008).</p>
</section>
<section>
<h2 id="sec44"><span class="green">Exercises</span></h2>
<p class="qpara"><span class="green">1.</span> Suppose that in the perspective transform, we have <em>n</em> = 1 and <em>f</em> = 2. Under what circumstances will we have a “reversal” where a vertex before and after the perspective transform flips from in front of to behind the eye or vice versa?</p>
<p class="qpara"><span class="green">2.</span> Is there any reason not to clip in <em>x</em> and <em>y</em> after the perspective divide (see <a href="C16_chapter11.xhtml#f11_2">Figure 11.2</a>, stage 3)?</p>
<p class="qpara"><span class="green">3.</span> Derive the incremental form of the midpoint line-drawing algorithm with colors at endpoints for 0 &lt; <em>m</em> ≤ 1.</p>
<p class="qpara"><span class="green">4.</span> Modify the triangle-drawing algorithm so that it will draw exactly one pixel for points on a triangle edge which goes through (<em>x, y</em>) = (–1, –1).</p>
<p class="qpara"><span class="green">5.</span> Suppose you are designing an integer z-buffer for flight simulation where all of the objects are at least one meter thick, are never closer to the viewer than 4 m, and may be as far away as 100 km. How many bits are needed in the z-buffer to ensure there are no visibility errors? Suppose that visibility errors only matter near the viewer, i.e., for distances less than 100 m. How many bits are needed in that case?</p>
</section>
</section>
</body>
</html>