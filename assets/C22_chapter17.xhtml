<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" dir="ltr" lang="en" xml:lang="en">
<head>
<meta charset="UTF-8"/>
<title>17 Using Graphics Hardware</title>
<link href="../styles/9781000426359.css" rel="stylesheet" type="text/css"/>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX","input/MathML","output/SVG"],
extensions: ["tex2jax.js","mml2jax.js","MathEvents.js"],
TeX: {
extensions: ["noErrors.js","noUndefined.js","autoload-all.js"]
},
MathMenu: {
showRenderer: false
},
menuSettings: {
zoom: "Click"
},
messageStyle: "none"
});
</script>
<script src="../mathjax/MathJax.js" type="text/javascript"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006665500" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter">
<section epub:type="chapter" role="doc-chapter">
<header>
<p class="chap-auz"><span class="green">Peter Willemsen</span></p>
<h1 class="chapz1" id="c17"><a id="term-359"/><span aria-label="461" epub:type="pagebreak" id="pg_461" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rc17" role="doc-backlink"><span class="green"><span class="big1">17</span><br/>Using Graphics Hardware</span></a></h1>
</header>
<section>
<h2 id="sec17_1"><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_1" role="doc-backlink"><span class="green">17.1 Hardware Overview</span></a></h2>
<p>Throughout most of this book, the focus is on the fundamentals that underly computer graphics rather than on any specifics relating to the APIs or hardware on which the algorithms may be implemented. This chapter takes a slightly different route and blends the details of using graphics hardware with some of the practical issues associated with programming that hardware. This chapter is designed to be an introductory guide to graphics hardware and could be used as the basis for a set of weekly labs that investigate graphics hardware.</p>
</section>
<section>
<h2 id="sec17_2"><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_2" role="doc-backlink"><span class="green">17.2 What Is Graphics Hardware</span></a></h2>
<p><em>Graphics hardware</em> describes the hardware components necessary to quickly render 3D objects as pixels on your computer’s screen using specialized rasterization-based (and in some cases, ray-tracer–based) hardware architectures. The use of the term <em>graphics hardware</em> is meant to elicit a sense of the physical components necessary for performing a range of graphics computations. In other words, the hardware is the set of chipsets, transistors, buses, processors, and computing cores found on current video cards. As you will learn in this chapter, and eventually experience yourself, current graphics hardware is very good at processing <span aria-label="462" epub:type="pagebreak" id="pg_462" role="doc-pagebreak"/>descriptions of 3D objects and transforming those representations into the colored pixels that fill your monitor.</p>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent"><span class="green">Real-Time Graphics:</span> By <a id="index_term963"/>real-time graphics, we generally mean that the graphics-related computations are being carried out fast enough that the results can be viewed immediately. Being able to conduct operations at 60Hz or higher is considered real time. Once the time to refresh the display (<em>frame rate</em>) drops below 15Hz, the speed is considered more interactive than it is real-time, but this distinction is not critical. Because the computations need to be fast, the equations used to render the graphics are often approximations to what could be done if more time were available.</p>
</aside>
<p class="indent">Graphics hardware has certainly changed very <em>rapidly</em> over the last decade. Newer graphics hardware provides more parallel processing capabilities, as well as better support for specialized rendering. One <a id="index_term464"/>explanation for the fast pace is the video game industry and its economic momentum. Essentially what this means is that each new graphics card provides better performance and processing capabilities. As a result, video games appear more visually realistic. The processors on graphics hardware, often called GPUs, or Graphics Processing Units, are highly parallel and afford thousands of concurrent threads of execution. The hardware is designed for throughput which allows larger numbers of pixels and vertices to be processed in shorter amounts of time. All of this parallelism is good for graphics algorithms, but other work has benefited from the parallel hardware. In addition to video games, GPUs are used to accelerate physics computations, develop real-time ray tracing codes, solve Navier-Stokes related equations for fluid flow simulations, and develop faster codes for understanding the climate (Purcell, Buck, Mark, &amp; Hanrahan, 2002; S. G. Parker et al., 2010; Harris, 2004). Several APIs and SDKs have been developed that afford more direct general purpose computation, such as OpenCL and NVIDIA’s CUDA. Hardware accelerated ray tracing APIs also exist to accelerate ray-object intersection (S. G. Parker et al., 2010). Similarly, the standard APIs that are used to program the graphics components of video games, such as OpenGL and DirectX, also allow mechanisms to leverage the graphics hardware’s parallel capabilities. Many of these APIs change as new hardware is developed to support more sophisticated computations.</p>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent"><span class="green">Fragment:</span> <a id="index_term375"/><em>Fragment</em> is a term that describes the information associated with a pixel prior to being processedinthe final <a id="index_term486"/>stages of the graphics pipeline. This definition includes much of the data that might be used to calculate the color of the pixel, such as the pixel’s scene depth, texture coordinates, or stencil information.</p>
</aside>
<p class="indent">Graphics hardware is programmable. As a developer, you have control over much of the computations associated with processing geometry, vertices, and the fragments that eventually become pixels. Recent hardware changes as well as ongoing updates to the APIs, such as OpenGL or DirectX, support a completely programmable pipeline. These changes afford developers creative license to exploit the computation available on GPUs. Prior to this, fixed-function <a id="index_term942"/>rasterization pipelines forced the computation to a specific style of vertex transformations, lighting, and fragment processing. The fixed functionality of the pipeline ensured that basic coloring, lighting, and texturing could occur very quickly. Whether it is a programmable interface, or fixed-function computation, the basic computations of the rasterization pipeline are similar, and follow the illustration in <a href="C22_chapter17.xhtml#f17_1">Figure 17.1</a>. In the rasterization pipeline, vertices are transformed from local space to global space, and eventually into screen coordinates, after being transformed by the viewing and projection transformation matrices. The set of screen coordinates associated with a geometry’s vertices are rasterized into fragments. The final stages of the pipeline process the fragments into pixels and can apply <a id="term-296"/><a id="term-379"/><a id="term-549"/><span aria-label="463" epub:type="pagebreak" id="pg_463" role="doc-pagebreak"/>per-fragment texture lookups, lighting, and any necessary blending. In general, the pipeline lends itself to parallel execution and the GPU cores can be used to process both vertices and fragments concurrently. Additional details about the rasterization pipeline can be found in <a href="C13_chapter8.xhtml#c8">Chapter 8</a>.</p>
<figure id="f17_1" tabindex="0">
<img alt="" src="../images/fig17_1.jpg"/>
<figcaption><p><span class="blue">Figure 17.1.</span> The basic graphics hardware pipeline consists of stages that transform 3D data into 2D screen objects ready for rasterizing and coloring by the pixel processing stages.</p></figcaption>
</figure>
</section>
<section>
<h2 id="sec17_3"><a id="index_term521"/><a id="index_term770"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_3" role="doc-backlink"><span class="green">17.3 Heterogeneous Multiprocessing</span></a></h2>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent"><span class="green">Host:</span> In a graphics hardware program, the host refers to the CPU components of the application.</p>
</aside>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent"><span class="green">Device:</span> The GPU side of the graphics application, including the data and computation that are stored and executed on the GPU.</p>
</aside>
<p class="indent">When using graphics hardware, it is convenient to distinquish between the CPU and the GPU as separate computational entities. In this context, the term <em>host</em> is used to refer to the CPU including the threads and memory available to it. The term <em>device</em> is used to refer to the GPU, or the graphics processing units, and the threads and memory associated with it. This makes some sense because most graphics hardware is comprised of external hardware that is connected to the machine via the PCI bus. The hardware may also be soldered to the machine as a separate chipset. In this sense, the graphics hardware represents a specialized co-processor since both the CPU (and its cores) can be programmed, as can the GPU and its cores. All programs that utilize graphics hardware must first establish a mapping between the CPU and the GPU memory. This is a rather low-level detail that is necessary so that the graphics hardware driver residing within the operating system can interface between the hardware and the operating system and windowing system software. Recall that because the host (CPU) and the device <a id="term-567"/><span aria-label="464" epub:type="pagebreak" id="pg_464" role="doc-pagebreak"/>(GPU) are separate, data must be communicated between the two systems. More formally, this mapping between the operating system, the hardware driver, the hardware, and the windowing system is known as the graphics <em>context</em>.The context is usually established through API calls to the windowing system. Details about establishing a context is outside the scope of this chapter, but many windowing system development libraries have ways to query the graphics hardware for various capabilities and establish the graphics context based on those requirements. Because setting up the context is windowing system dependent, it also means that such code is not likely to be cross-platform code. However, in practice, or at least when starting out, it is very unlikely that such low-level context setup code will be required since many higher level APIs exist to help people develop portable interactive applications.</p>
<p>Many of the frameworks for developing interactive applications support querying input devices such as the keyboard or mouse. Some frameworks provide access to the network, audio system, and other higher level system resources. In this regard, many of these APIs are the preferred way to develop graphics, and even game applications.</p>
<p>Cross-platform hardware acceleration is often achieved with the OpenGL API. OpenGL is an open industry standard graphics API that supports hardware acceleration on many types of graphics hardware. OpenGL represents one of the most common APIs for <a id="index_term477"/>programming graphics hardware along with APIs such as DirectX. While OpenGL is available on many operating systems and hardware architectures, DirectX is specific to Microsoft-based systems. For the purposes of this chapter, hardware programming concepts and examples will be presented with OpenGL.</p>
<section>
<h3 id="sec17_3_1"><span class="green">17.3.1 Programming with OpenGL</span></h3>
<p>When you program with the OpenGL API, you are writing code for at least two processors: the CPU(s) and the GPU(s). OpenGL is implemented in a C-style API and all functions are prefixed with “gl” to indicate their inclusion with OpenGL. OpenGL function calls change the state of the graphics hardware and can be used to declare and define geometry, load vertex and fragment shaders, and determine how computation will occur as data passes through the hardware.</p>
<p>The variant of OpenGL that this chapter presents is the OpenGL 3.3 Core Profile version. While not the most recent version of OpenGL, the 3.3 version of OpenGL is in line with the future direction of <a id="index_term801"/>OpenGL programming. These versions are focused on improving efficiency while also fully placing the programming of the pipeline within the hands of the developer. Many of the function <a id="term-380"/><a id="term-742"/><a id="term-773"/><a id="term-828"/><span aria-label="465" epub:type="pagebreak" id="pg_465" role="doc-pagebreak"/>calls present in earlier versions of OpenGL are not present in these newer APIs. For instance, <em>immediate mode</em> rendering is deprecated. Immediate mode rendering was used to send data from the CPU memory to the graphics card memory as needed each frame and was often very inefficient, especially for larger models and complex scenes. The current API focuses on storing data on the graphics card before it is needed and instancing it at render time. As another example, OpenGL’s matrix stacks have been deprecated as well, leaving the developer to use third-party matrix libraries (such as GLM) or their own classes to create the necessary matrices for viewing, projection, and transformation, as presented in <a href="C12_chapter7.xhtml#c7">Chapter 7</a>. As a result, OpenGL’s shader language (GLSL) has taken on larger roles as well, performing the necessary matrix tranformations along with lighting and shading within the shaders. Because the fixed-function pipeline which performed per-vertex transformation and lighting is no longer present, programmers must develop all shaders themselves. The shading examples presented in this chapter will utilize the GLSL 3.3 Core Profile version shader specification. Future readers of this chapter will want to explore the current OpenGL and OpenGL Shading Language specifications for additional details on what these APIs and languages can support.</p>
</section>
</section>
<section>
<h2 id="sec17_4"><a id="index_term461"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_4" role="doc-backlink"><span class="green">17.4 Graphics Hardware Programming: Buffers, State, and Shaders</span></a></h2>
<p>Three concepts will help to understand contemporary graphics hardware programming. The first is the notion of a data <em>buffer</em>, which is quite simply, a linear allocation of memory on the device that can store various data on which the GPUs will operate. The second is the idea that the graphics card maintains a computational <em>state</em> that determines how computations associated with scene data and shaders will occur on the graphics hardware. Moreover, state can be communicated from the host to the device and even within the device between shaders. <em>Shaders</em> represent the mechanism by which computation occurs on the GPU related to per-vertex or per-fragment processing. This chapter will focus on vertex and fragment shaders, but specialized geometry and compute shaders also exist in the current versions of OpenGL. <a id="index_term1055"/>Shaders play a very important role in how modern graphics hardware functions.</p>
<section>
<h3 id="sec17_4_1"><span class="green">17.4.1 Buffers</span></h3>
<p>Buffers are the primary structure to store data on graphics hardware. They represent the graphics hardware’s internal memory associated with everything from <a id="term-372"/><a id="term-697"/><span aria-label="466" epub:type="pagebreak" id="pg_466" role="doc-pagebreak"/>geometry, textures, and image plane data. With regard to the rasterization pipeline described in <a href="C13_chapter8.xhtml#c8">Chapter 8</a>, the computations associated with hardware-accelerated rasterization read and write the various buffers on the GPU. From a programming standpoint, an application must initialize the buffers on the GPU that are needed for the application. This amounts to a host to device copy operation. At the end of various stages of execution, device to host copies can be performed as well to pull data from the GPU to the CPU memory. Additionally, mechanisms do exist in OpenGL’s API that allow device memory to be mapped into host memory so that an application program can write directly to the buffers on the graphics card.</p>
</section>
<section>
<h3 id="sec17_4_2"><span class="green">17.4.2 Display Buffer</span></h3>
<p>In the graphics pipeline, the final set of pixel colors can be linked to the display, or they may be written to disk as a PNG image. The data associated with these pixels is generally a 2D array of color values. The data is inherently 2D, but it is efficiently represented on the GPU as a 1D linear array of memory. This array implements the <em>display buffer</em>, which eventually gets mapped to the window. Rendering images involves communicating the changes to the display buffer on the graphics hardware through the graphics API. At the end of the rasterization pipeline, the fragment processing and blending stages write data to the output display buffer memory. Meanwhile, the windowing system reads the contents of the display buffer to produce the raster images on the monitor’s window.</p>
</section>
<section>
<h3 id="sec17_4_3"><span class="green">17.4.3 Cycle of Refresh</span></h3>
<p>Most applications prefer a double-buffered display state. What this means is that there are two buffers associated with a graphics window: the front buffer and the back buffer. The purpose of the double-buffered system is that the application can communicate changes to the back buffer (and thus, write changes to that buffer) while the front-buffer memory is used to drive the pixel colors on the window.</p>
<p>At the end of the rendering loop, the buffers are swapped through a pointer exchange. The front-buffer pointer points to the back buffer and the back-buffer pointer is then assigned to the previous front buffer. In this way, the windowing system will refresh the content of the window with the most up-to-date buffer. If the buffer pointer swap is synchronized with the windowing system’s refresh of the entire display, the rendering will appear seamless. Otherwise, users may observe a tearing of the geometry on the actual display as changes to the scene’s geometry and fragments are processed (and thus written to the <a id="index_term339"/>display buffer) faster than the screen is refreshed.</p>
<p><a id="term-829"/><a id="index_term1129"/><span aria-label="467" epub:type="pagebreak" id="pg_467" role="doc-pagebreak"/>When the display is considered a memory buffer, one of the simplest operations on the display is essentially a memory setting (or copying) operation that zeros-out, or clears the memory to a default state. For a graphics program, this likely means clearing the background of the window to a specific color. To clear the background color (to black) in an OpenGL application, the following code can be used:</p>
<pre class="pre">glClearColor( 0.0f, 0.0f, 0.0f, 1.0f );<br/>glClear( GL_COLOR_BUFFER_BIT );</pre>
<p>The first three arguments for the <span class="monospace">glClearColor</span> function represent the <em>red</em>, <em>green</em>,and <em>blue</em> color components, specified within the range [0<em>,</em> 1]. The fourth argument represents opacity, or <em>alpha</em> value, ranging from 0<em>.</em>0 being completely transparent to 1<em>.</em>0 being completely opaque. The <em>alpha</em> value is used to determine transparency through various fragment blending operations in the final stages of the pipeline.</p>
<p>This operation only clears the color buffer. In addition to the color buffer, specified by <span class="monospace">GL_COLOR_BUFFER_BIT</span>, being cleared to black in this case, graphics hardware also uses a depth buffer to represent the distance that fragments are relative to the camera (you may recall the discussion of the z-buffer algorithm in <a href="C13_chapter8.xhtml#c8">Chapter 8</a>). Clearing the depth buffer is necessary to ensure operation of the z-buffer algorithm and allow correct hidden surface removal to occur. Clearing the depth buffer can be achieved by <em>or</em>’ing two bit field values together, as follows:</p>
<pre class="pre">glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</pre>
<p>Within a basic interactive graphics application, this step of clearing is normally the first operation performed before any geometry or fragments are processed.</p>
</section>
</section>
<section>
<h2 id="sec17_5"><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_5" role="doc-backlink"><span class="green">17.5 State Machine</span></a></h2>
<p>By illustrating the buffer-clearing operation for the display’s color and depth buffers, the idea of graphics hardware <em>state</em> is also introduced. The <span class="monospace">glClearColor</span> function sets the default color values that are written to all the pixels within the color buffer when <span class="monospace">glClear</span> is called. The clear call initializes the color component of the display buffer and can also reset the values of the depth buffer. If the clear color does not change within an application, the clear color need only be set once, and often this is done in the initialization of an OpenGL program. Each time that <span class="monospace">glClear</span> is called it uses the previously set state of the clear color.</p>
<p><a id="term-367"/><a id="term-370"/><a id="term-568"/><span aria-label="468" epub:type="pagebreak" id="pg_468" role="doc-pagebreak"/>Note also that the z-buffer algorithm state can be enabled and disabled as needed. The z-buffer algorithm is also known in OpenGL as the depth test. By enabling it, a fragment’s depth value will be compared to the depth value currently stored in the depth buffer prior to writing any fragment colors to the color buffer. Sometimes, the depth test is not necessary and could potentially slow down an application. Disabling the depth test will prevent the z-buffer computation and change the behavior of the executable. Enabling the z-buffer test with OpenGL is done as follows:</p>
<pre class="pre">glEnable(GL_DEPTH_TEST);<br/>glDepthFunc(GL_LESS);</pre>
<p>The <span class="monospace">glEnable</span> call turns on the depth test while the <span class="monospace">glDepthFunc</span> call sets the mechanism for how the depth comparison is performed. In this case, the depth function is set to its default value of <span class="monospace">GL_LESS</span> to show that other state variables exist and can be modified. The converse of the <span class="monospace">glEnable</span> calls are <span class="monospace">glDisable</span> calls.</p>
<p>The idea of state in OpenGL mimics the use of static variables in object-oriented classes. As needed, programmers enable, disable, and/or set the state of OpenGL variables that reside on the graphics card. These state then affect any succeeding computations on the hardware. In general, efficient OpenGL programs attempt to minimize state changes, enabling states that are needed, while disabling states that are not required for rendering.</p>
</section>
<section>
<h2 id="sec17_6"><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_6" role="doc-backlink"><span class="green">17.6 Basic OpenGL Application Layout</span></a></h2>
<p>A simple and basic OpenGL application has, at its heart, a display loop that is called either as fast as possible, or at a rate that coincides with the refresh rate of the monitor or display device. The example loop below uses the GLfW library, which supports OpenGL coding across multiple platforms.</p>
<pre class="pre"><span class="voilet">while</span> (!glfwWindowShouldClose(window)) {<br/>{<br/>  <span class="red">// OpenGL code is called here,</span><br/>  <span class="red">//  each time this loop is executed.</span><br/>  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);<br/><br/>  <span class="red">// Swap front and back buffers</span><br/>  glfwSwapBuffers(window);<br/><br/>  <span class="red">// Poll for events</span><br/>  glfwPollEvents();<br/><a id="term-338"/><span aria-label="469" epub:type="pagebreak" id="pg_469" role="doc-pagebreak"/>  <span class="voilet">if</span> (glfwGetKey( window, GLFW_KEY_ESCAPE ) == GLFW_PRESS)<br/>     glfwSetWindowShouldClose(window, 1);<br/>}</pre>
<p>The loop is tightly constrained to operate only while the window is open. This example loop resets the color buffer values and also resets the z-buffer depth values in the graphics hardware memory based on previously set (or default) values. Input devices, such as keyboards, mouse, network, or some other interaction mechanism are processed at the end of the loop to change the state of data structures associated with the program. The call to <span class="monospace">glfwSwapBuffers</span> synchronizes the graphics context with the display refresh, performing the pointer swap between the front and back buffers so that the updated graphics state is displayed on the user’s screen. The call to swap the buffers occurs after all graphics calls have been issued.</p>
<p>While conceptually separate, the depth and color buffers are often collectively called the <em>framebuffer</em>. By clearing the contents of the framebuffer, the application can proceed with additional OpenGL calls to push geometry and fragments through the graphics pipeline. The framebuffer is directly related to the size of the window that has been opened to contain the graphics context. The window, or viewport, dimensions are needed by OpenGL to construct the <em>M<sub>vp</sub></em> matrix (from <a href="C12_chapter7.xhtml#c7">Chapter 7</a>) within the hardware. This is accomplished through the following code, demonstrated again with the GLfW toolkit, which provides functions for querying the requested window (or framebuffer) dimensions:</p>
<pre class="pre">int nx, ny;<br/>glfwGetFramebufferSize(window, &amp;nx, &amp;ny);<br/>glViewport(0, 0, nx, ny);</pre>
<p class="noindent">In this example, <span class="monospace">glViewport</span> sets the OpenGL state for the window dimension using <span class="monospace">nx</span> and <span class="monospace">ny</span> for the width and height of the window and the viewport being specified to start at the origin.</p>
<p>Technically, OpenGL writes to the <a id="index_term380"/>framebuffer memory as a result of operations that rasterize geometry, and process fragments. These writes happen before the pixels are displayed on the user’s monitor.</p>
</section>
<section>
<h2 id="sec17_7"><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_7" role="doc-backlink"><span class="green">17.7 Geometry</span></a></h2>
<p>Similar to the idea of a display buffer, geometry is also specified using arrays to store vertex data and other vertex attributes, such as vertex colors, normals, or texture coordinates needed for shading. The concept of buffers will be used to <a id="term-369"/><a id="term-703"/><span aria-label="470" epub:type="pagebreak" id="pg_470" role="doc-pagebreak"/>allocate storage on the graphics hardware, transferring data from the host to the device.</p>
<section>
<h3 id="sec17_7_1"><a id="index_term466"/><span class="green">17.7.1 Describing Geometry for the Hardware</span></h3>
<p>One of the challenges with graphics hardware programming is the management of the 3D data and its transfer to and from the memory of the graphics hardware. Most graphics hardware work with specific sets of geometric primitives. The different primitive types leverage primitive complexity for processing speed on the graphics hardware. Simpler primitives can sometimes be processed very fast. The caveat is that the primitive types need to be general purpose so as to model a wide range of geometry from very simple to very complex. On typical graphics hardware, the primitive types are limited to one or more of the following:</p>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent"><span class="green">Primitives:</span> The three primitives (points, lines, triangles, and quads) are really the only primitives available! Even when creating spline-based surfaces, such as NURBS, the surfaces are tessellated into triangle primitives by the graphics hardware.</p>
</aside>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent"><a id="index_term984"/><span class="green">Point Rendering:</span> Point and line primitives may initially appear to be limited in use, but researchers have used points to render very complex geometry (Rusinkiewicz &amp; Levoy, 2000; Dachsbacher, Vogelgsang, &amp; Stamminger, 2003).</p>
</aside>
<ul class="list-bullet">
<li>
<p class="list"><span class="green">points</span>—single vertices used to represent points or particle systems;</p>
</li>
<li>
<p class="list"><span class="green">lines</span>—pairs of vertices used to represent lines, silhouettes, or edge-highlighting;</p>
</li>
<li>
<p class="list"><span class="green">triangles</span>—triangles, triangle strips, indexed triangles, indexed triangle strips, quadrilaterals, or triangle meshes approximating geometric surfaces.</p>
</li>
</ul>
<p class="indent">These three <a id="index_term895"/>primitive types form the basic building blocks for most geometry that can be defined. An example of a triangle mesh rendered with OpenGL is shown in <a href="C22_chapter17.xhtml#f17_2">Figure 17.2</a>.</p>
<figure id="f17_2" tabindex="0">
<img alt="" src="../images/fig17_2.jpg"/>
<figcaption><p><span class="blue">Figure 17.2.</span> How your geometry is organized will affect the performance of your application. This wireframe depiction of the Little Cottonwood Canyon terrain dataset shows tens of thousands of triangles organized as a triangle mesh running at real-time rates. <em>The image is rendered using the VTerrain Project terrain system courtesy of Ben Discoe.</em></p></figcaption>
</figure>
</section>
</section>
<section>
<h2 id="sec17_8"><a id="index_term1056"/><a id="term-339"/><a id="term-743"/><a id="term-774"/><span aria-label="471" epub:type="pagebreak" id="pg_471" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_8" role="doc-backlink"><span class="green">17.8 A First Look at Shaders</span></a></h2>
<p>Modern versions of OpenGL require that shaders be used to process vertices and fragments. As such, no primitives can be rendered without at least one vertex shader to process the incoming primitive vertices and another shader to process the rasterized fragments. Advanced shader types exist within OpenGL and the OpenGL Shading Language: <em>geometry shaders</em> and <em>compute shaders</em>. Geometry shaders are designed to process primitives, potentially creating additional primitives, and can support geometric instancing operations. Compute shaders are designed for performing general computation on the GPU, and can be linked into the set of shaders necessary for a specific application. For more information on geometry and compute shaders, the reader is referred the OpenGL specification documents and other resources.</p>
<section>
<h3 id="sec17_8_1"><span class="green">17.8.1 Vertex Shader Example</span></h3>
<p>Vertex shaders provide control over how vertices are transformed and often help prepare data for use in fragment shaders. In addition to standard transformations and potential per-vertex lighting operations, vertex shaders could be used to perform general computation on the GPU. For instance, if the vertices represent particles and the particle motion can be (simply) modeled within the vertex shader computations, the CPU can mostly be removed from performing those computations. The ability to perform computations on the vertices already stored in the graphics hardware memory is a potential performance gain. While this approach is useful in some situations, advanced general computation may be more appropriately coded with compute shaders.</p>
<p>In <a href="C12_chapter7.xhtml#c7">Chapter 7</a>, the viewport matrix <em>M<sub>vp</sub></em> was introduced. It transforms the canonical view volume coordinates to screen coordinates. Within the canonical view volume, coordinates exist in the range of [–1, 1]. Anything outside of this range is clipped. If we make an initial assumption that the geometry exists within this range and the z-value is ignored, we can create a very simple vertex shader. This vertex shader passes the vertex positions through to the rasterization stage, where the final viewport transformation will occur. Note that because of this simplification, there are no projection, viewing, or model transforms that will be applied to the incoming vertices. This is initially cumbersome for creating anything except very simple scenes, but will help introduce the concepts of shaders and allow you to render an initial triangle to the screen. The <em>passthrough</em> vertex shader follows:</p>
<pre class="pre"><a id="term-122"/><span aria-label="472" epub:type="pagebreak" id="pg_472" role="doc-pagebreak"/><br/><span class="bluez">#version</span> 330 <span class="bluez">core</span><br/><br/><span class="voilet">layout</span>(location=0) <span class="voilet">in vec3</span> in_Position;<br/><span class="voilet">void</span> main(<span class="voilet">void</span>)<br/>{<br/>  <span class="bluez">gl_Position</span> = <span class="voilet">vec4</span>(in_Position, 1.0);<br/>}</pre>
<p>This vertex shader does only one thing. It passes the incoming vertex position out as the <span class="monospace">gl_Position</span> that OpenGL uses to rasterize fragments. Note that <span class="monospace">gl_Position</span> is a built-in, reserved variable that signifies one of the key outputs required from a vertex shader. Also note the <span class="monospace">version</span> string in the first line. In this case, the string instructs the GLSL compiler that version 3.3 of the GLSL Core profile is to be used to compile the shading language.</p>
<p>Vertex and fragment shaders are SIMD operations that respectively operate on all the vertices or fragments being processed in the pipeline. Additional data can be communicated from the host to the shaders executing on the device by using input, output, or uniform variables. Data that is passed into a shader is prefixed with the keyword <span class="monospace">in</span>. The location of that data as it relates to specificvertex attributes or fragment output indices is also specified directly in the shader. Thus,</p>
<pre class="pre">layout(location=0) in vec3 in_Position;</pre>
<p class="noindent">specifies that <span class="monospace">in_Position</span> is an input variable that is of type <span class="monospace">vec3</span>. The source of that data is the attribute index 0 that is associated with the geometry. The name of this variable is determined by the programmer, and the link between the incoming geometry and the shader occurs while setting up the vertex data on the device. The GLSL contains a nice variety of types useful to graphics programs, including <span class="monospace">vec2</span>, <span class="monospace">vec3</span>, <span class="monospace">vec4</span>, <span class="monospace">mat2</span>, <span class="monospace">mat3</span>,and <span class="monospace">mat4</span> to name a few. Standard types such as <span class="monospace">int</span> or <span class="monospace">float</span> also exist. In shader programming, vectors, such as <span class="monospace">vec4</span> hold 4-components corresponding to the <em>x</em>, <em>y</em>, <em>z</em>,and <em>w</em> components of a <a id="index_term531"/>homogeneous coordinate, or the <em>r</em>, <em>g</em>, <em>b</em>,and <em>a</em> components of a RGBA tuple. The labels for the types can be interchanged as needed (and even repeated) in what is called <em>swizzling</em> (e.g., in_Position.zyxa). Moreover, the component-wise labels are overloaded and can be used appropriately to provide context.</p>
<p>All shaders must have a main function that performs the primary computation across all inputs. In this example, the main function simply copies the input vertex position (<span class="monospace">in_Position</span>), which is of type <span class="monospace">vec3</span> into the built-in vertex shader output variable, which is of type <span class="monospace">vec4</span>. Note that many of the built-in types have constructors that are useful for conversions such as the one presented here to convert the incoming vertex position’s <span class="monospace">vec3</span> type into <span class="monospace">gl_Position</span>’s <span class="monospace">vec4</span> <span aria-label="473" epub:type="pagebreak" id="pg_473" role="doc-pagebreak"/>type. Homogeneous coordinates are used with OpenGL, so 1<em>.</em>0 is specified as the fourth coordinate to indicate that the vector is a position.</p>
</section>
<section>
<h3 id="sec17_8_2"><span class="green">17.8.2 Fragment Shader Example</span></h3>
<p>If the simplest vertex shader simply passes clip coordinates through, the simplest fragment shader sets the color of the fragment to a constant value.</p>
<pre class="pre"><span class="bluez">#version</span> 330 <span class="bluez">core</span><br/><span class="voilet">layout</span>(location=0) <span class="voilet">out vec4</span> out_FragmentColor;<br/><span class="voilet">void</span> main(<span class="voilet">void</span>)<br/>{<br/>  out_FragmentColor = <span class="voilet">vec4</span>(0.49, 0.87, 0.59, 1.0);<br/>}</pre>
<p class="noindent">In this example, all fragments will be set to a light shade of green. One key difference is the use of the <span class="monospace">out</span> keyword. In general, the keywords <span class="monospace">in</span> and <span class="monospace">out </span>in shader programs indicate the flow of data into, and out of, shaders. While the vertex shader received incoming vertices and output them to a built-in variable, the fragment shader declares its outgoing value which is written out to the color buffer:</p>
<pre class="pre">layout(location=0) out vec4 out_FragmentColor; </pre>
<p class="noindent">The output variable <span class="monospace">out_FragmentColor</span> is again user defined. The location of the output is color buffer index 0. Fragment shaders can output to multiple buffers, but this is an advanced topic left to the reader that will be needed if OpenGL’s framebuffer objects are investigated. The use of the <span class="monospace">layout</span> and <span class="monospace">location</span> keywords makes an explicit connection between the application’s geometric data in the vertex shader and the output color buffers in the fragment shader.</p>
</section>
<section>
<h3 id="sec17_8_3"><span class="green">17.8.3 Loading, Compiling, and Using Shaders</span></h3>
<p>Shader programs are transferred onto the graphics hardware in the form of character strings. They must then be compiled and linked. Furthermore, shaders are coupled together into shader programs so that vertex and fragment processing occur in a consistent manner. A developer can activate a shader that has been successfully compiled and linked into a shader program as needed, while also deactivating shaders when not required. While the detailed process of creating, <a id="term-996"/><span aria-label="474" epub:type="pagebreak" id="pg_474" role="doc-pagebreak"/>loading, compiling, and linking shader programs is not provided in this chapter, the following OpenGL functions will be helpful in creating shaders:</p>
<ul class="list-bullet">
<li>
<p class="list"><span style="color:green">glCreateShader</span> creates a handle to a shader on the hardware.</p>
</li>
<li>
<p class="list"><span style="color:green">glShaderSource</span> loads the character strings into the graphics hardware memory.</p>
</li>
<li>
<p class="list"><span style="color:green">glCompileShader</span> performs the actual compilation of the shader within the hardware.</p>
</li>
</ul>
<p>The functions above need to be called for each shader. So, for the simple pass-through shaders, each of those functions would be called for both the vertex shader code and the fragment shader code provided. At the end of the compilation phase, compilation status and any errors can be queried using additional OpenGL commands.</p>
<p>After both shader codes are loaded and compiled, they can be linked into a shader program. The shader program is what is used to affect rendering of geometry.</p>
<ul class="list-bullet">
<li>
<p class="list"><span style="color:green">glCreateProgram</span> creates a program object that will contain the previously compiled shaders.</p>
</li>
<li>
<p class="list"><span style="color:green">glAttachShader</span> attaches a shader to the shader program object. In the simple example, this function will be called for both the compiled vertex shader and the compiled fragment shader objects.</p>
</li>
<li>
<p class="list"><span style="color:green">glLinkProgram</span> links the shaders internally after all shaders have been attached to the program object.</p>
</li>
<li>
<p class="list"><span style="color:green">glUseProgram</span> binds the shader program for use on the graphics hardware. As shaders are needed, the program handles are bound using this function. When no shaders are needed, they can be unbound by using the shader program handle 0 as an argument to this function.</p>
</li>
</ul>
</section>
</section>
<section>
<h2 id="sec17_9"><a id="index_term1312"/><a id="index_term462"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_9" role="doc-backlink"><span class="green">17.9 Vertex Buffer Objects</span></a></h2>
<p>Vertices are stored on the graphics hardware using buffers, known as <em>vertex buffer objects</em>. In addition to vertices, any additional <em>vertex attributes</em>, such as colors, normal vectors, or texture coordinates, will also be specified using vertex buffer objects.</p>
<p><a id="term-368"/><a id="term-371"/><span aria-label="475" epub:type="pagebreak" id="pg_475" role="doc-pagebreak"/>First, let’s focus on specifying the geometric primitive themselves. This starts by allocating the vertices associated with the primitive within the host memory of the application. The most general way to do this is to define an array on the host to contain the vertices needed for the primitive. For instance, a single triangle, fully contained within the canonical volume, could be defined statically on the host as follows:</p>
<pre class="pre">GLfloat vertices[] = {-0.5f, -0.5f, 0.0f, 0.5f, -0.5f, 0.0f, 0.0f, 0.5f, 0.0f}; </pre>
<p class="noindent">If the simple passthrough shaders are used for this triangle, then all vertices will be rendered. Although the triangle is placed on the <em>z</em> = 0 plane, the <em>z</em> coordinates for this example do not really matter since they are essentially dropped in the final transformation into screen coordinates. Another thing to note is the use of the type <span class="monospace">GLfloat</span> in these examples. Just as the GLSL language has specialized types, OpenGL has related type which generally can intermix well with the standard types (like float). For preciseness, the OpenGL types will be used when necessary.</p>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent"><span class="green">OpenGL Coordinate System:</span> The coordinate system used by OpenGL is identical to that presented in this book. It is a right-handed coordinate system with +<em>x</em> to the right, +<em>y</em> up, and +<em>z</em> away from the screen (or window). Thus, –<em>z</em> points into the monitor.</p>
</aside>
<p class="indent">Before the vertices can be processed, a vertex buffer is first created on the device to store the vertices. The vertices on the host are then transferred to the device. After this, the vertex buffer can be referenced as needed to draw the array of vertices stored in the buffer. Moreover, after the initial transfer of vertex data, no additional copying of data across the host to device bus need occur, especially if the geometry remains static across rendering loop updates. Any host memory can also be deleted if it was dynamically allocated.</p>
<p>Vertex buffer objects, often called VBOs, represent the primary mechanism with modern OpenGL to store vertex and vertex attributes in the graphics memory. For efficiency purposes, the initial setup of a VBO and the transfer of vertex-related data mostly happens prior to entering the display loop. As an example, to create a VBO for this triangle, the following code could be used:</p>
<pre class="pre">GLuint triangleVBO[1];<br/>glGenBuffers(1, triangleVBO);<br/>glBindBuffer(GL_ARRAY_BUFFER, triangleVBO[0]);<br/>glBufferData(GL_ARRAY_BUFFER, 9 * sizeof(GLfloat), vertices, GL_STATIC_DRAW);<br/>glBindBuffer(GL_ARRAY_BUFFER, 0); </pre>
<p class="noindent">Three OpenGL calls are required to create and allocate the vertex buffer object. The first, <span class="monospace">glGenBuffers</span> creates a handle that can be used to refer to the VBO once it is stored on the device. Multiple handles to VBOs (stored in arrays) can be created in a single <span class="monospace">glGenBuffers</span> call, as illustrated but not utilized here. Note that when a buffer object is generated, the actual allocation of space on the device is not yet performed.</p>
<p>With OpenGL, <em>objects</em>, such as vertex buffer objects, are primary targets for computation and processing. Objects must be bound to a known OpenGL state <a id="term-995"/><span aria-label="476" epub:type="pagebreak" id="pg_476" role="doc-pagebreak"/>when used and unbound when not in use. Examples of OpenGL’s use of objects include the vertex buffer objects, framebuffer objects, texture objects, and shader programs, to name a few. In the current example, the <span class="monospace">GL_ARRAY_BUFFER</span> state of OpenGL is bound to the triangle VBO handle that was generated previously. This essentially makes the triangle VBO the active vertex buffer object. Any operations that affect vertex buffers that follow the <span class="monospace">glBindBuffer(GL_ARRAY_BUFFER, triangleVBO[0])</span> command will use the triangle data in the VBO either by reading the data or writing to it.</p>
<p>Vertex data is copied from the host (the <span class="monospace">vertices</span> array) to the device (currently bound <span class="monospace">GL_ARRAY_BUffER</span>)using the</p>
<pre class="pre">glBufferData(GL_ARRAY_BUFFER, 9 * sizeof(GLfloat), vertices, GL_STATIC_DRAW); </pre>
<p class="noindent">call. The arguments represent the type of target, the size in bytes of the buffer to be copied, the pointer to the host buffer, and an enumerated type that indicates how the buffer will be used. In the current example, the target is <span class="monospace">GL_ARRAY_BUFFER</span>, the size of the data is 9* <span class="monospace">sizeof(GLfloat)</span>, and the last argument is <span class="monospace">GL_STATIC_DRAW</span> indicating to OpenGL that the vertices will not change over the course of the rendering. Finally, when the VBO no longer needs to be an active target for reading or writing, it is unbound with the <span class="monospace">glBindBuffer(GL_ARRAY_BUFFER, 0)</span> call. In general, binding any of OpenGL’s objects or buffers to handle <span class="monospace">0</span>, unbinds, or disables that buffer from affecting subsequent functionality.</p>
</section>
<section>
<h2 id="sec17_10"><a id="index_term1310"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_10" role="doc-backlink"><span class="green">17.10 Vertex Array Objects</span></a></h2>
<p>While vertex buffer objects are the storage containers for vertices (and vertex attributes), <em>vertex array objects</em> represent OpenGL’s mechanism to bundle vertex buffers together into a consistent vertex state that can be communicated and linked with shaders in the graphics hardware. Recall that the fixed function pipeline of the past no longer exists and therefore, per-vertex state, such as normals or even vertex colors, must be stored in hardware buffers and then referenced in shaders, using input variables (e.g., <span class="monospace">in</span>).</p>
<p>As with vertex buffer objects, vertex array objects, or VAOs, must be created and allocated with any necessary state being set while the vertex array object is bound. For instance, the following code shows how to create a VAO to contain the triangle VBO previously defined:</p>
<pre class="pre">GLuint VAO;<br/>glGenVertexArrays(1, &amp;VAO);<br/>glBindVertexArray(VAO);<br/><span aria-label="477" epub:type="pagebreak" id="pg_477" role="doc-pagebreak"/>glEnableVertexAttribArray(0);<br/>glBindBuffer(GL_ARRAY_BUFFER, triangleVBO[0]);<br/>glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(GLfloat), 0);<br/><br/>glBindVertexArray(0); </pre>
<p class="noindent">When defining a vertex array object, specific vertex buffer objects can be bound to specific vertex attributes (or inputs) in shader code. Recall the use of</p>
<pre class="pre">layout(location=0) in vec3 in_Position </pre>
<p class="noindent">in the passthrough vertex shader. This syntax indicate that the shader variable will receive its data from attribute index 0 in the bound vertex array object. In host code, the mapping is created using the</p>
<pre class="pre">glEnableVertexAttribArray(0);<br/>glBindBuffer(GL_ARRAY_BUFFER, triangleVBO[0]);<br/>glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(GLfloat), 0); </pre>
<p class="noindent">calls. The first call enables the vertex attribute index (in this case, 0). The next two calls connect the previously defined vertex buffer object that holds the vertices to the vertex attribute itself. Because <span class="monospace">glVertexAttribPointer</span> utilizes the currently bound VBO, it is important that the <span class="monospace">glBindBuffer</span> is issued before assigning the vertex attribute pointer. These function calls create a mapping that binds the vertices in our vertex buffer to the <span class="monospace">in_Position</span> variable within the vertex shader. The <span class="monospace">glVertexAttribPointer</span> calls seems complicated but it basically sets attribute index 0 to hold three components (e.g., <em>x, y, z</em>)of <span class="monospace">GLfloats</span> (the 2nd and 3rd arguments) that are not normalized (the fourth argument). The fifth argument instructs OpenGL that three float values separate the starts of each vertex set. In other words, the vertices are tightly packed in the memory, one after the other. The final argument is a pointer to the data, but because a vertex buffer has been bound prior to this call, the data will be associated with the vertex buffer.</p>
<p>The previous steps that initialize and construct the vertex array object, the vertex buffer objects, and the shaders should all be executed prior to entering the display loop. All memory from the vertex buffer will have been transferred to the GPU and the vertex array objects will make the connection between the data and shader input variable indexes. In the display loop, the following calls will trigger the processing of the vertex array object:</p>
<pre class="pre">glBindVertexArray(VAO);<br/>glDrawArrays(GL_TRIANGLES, 0, 3);<br/>glBindVertexArray(0);</pre>
<p class="noindent"><a id="term-373"/><a id="term-375"/><a id="term-744"/><a id="term-775"/><span aria-label="478" epub:type="pagebreak" id="pg_478" role="doc-pagebreak"/>Note again, that a bind call makes the vertex array object active. The call to <span class="monospace">glDrawArrays</span> initiates the pipeline for this geometry, describing that the geometry should be interpreted as a series of triangle primitives starting at offset 0 and only rendering three of the indices. In this example, there are only three elements in the array and the primitive is a triangle, so a single triangle will be rendered.</p>
<p>Combining all of these steps, the assembled code for the triangle would resemble the following, assuming that shader and vertex data loading are contained in external functions:</p>
<pre class="pre"><span class="red">// Set the viewport once</span><br/><span class="voilet">int</span> nx, ny;<br/>glfwGetFramebufferSize(window, &amp;nx, &amp;ny);<br/>glViewport(0, 0, nx, ny);<br/><br/><span class="red">// Set clear color state</span><br/>glClearColor( 0.0f, 0.0f, 0.0f, 1.0f );<br/><br/><span class="red">// Create the Shader programs, VBO, and VAO</span><br/>GLuint shaderID = loadPassthroughShader();<br/>GLuint VAO = loadVertexData();<br/><br/><span class="voilet">while</span> (!glfwWindowShouldClose(window)) {<br/>{<br/>   glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);<br/><br/>   glUseProgram( shaderID );<br/><br/>   glBindVertexArray(VAO);<br/>   glDrawArrays(GL_TRIANGLES, 0, 3);<br/>   glBindVertexArray(0);<br/><br/>   glUseProgram( 0 );<br/><br/>   <span class="red">// Swap front and back buffers</span><br/>   glfwSwapBuffers(window);<br/><br/>   <span class="red">// Poll for events</span><br/>   glfwPollEvents();<br/>   <span class="voilet">if</span> (glfwGetKey( window, GLFW_KEY_ESCAPE ) == GLFW_PRESS)<br/>      glfwSetWindowShouldClose(window, 1);<br/>} </pre>
<p><a href="C22_chapter17.xhtml#f17_3">Figure 17.3</a> shows the result of using the shaders and vertex state to render the canonical view volume triangle.</p>
<figure id="f17_3" tabindex="0">
<img alt="" src="../images/fig17_3.jpg"/>
<figcaption><p><span class="blue">Figure 17.3.</span> The canonical triangle rendered using the simple vertex and fragment shaders.</p></figcaption>
</figure>
</section>
<section>
<h2 id="sec17_11"><a id="index_term490"/><a id="term-342"/><a id="term-934"/><span aria-label="479" epub:type="pagebreak" id="pg_479" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_11" role="doc-backlink"><span class="green">17.11 Transformation Matrices</span></a></h2>
<p>Current versions of OpenGL have removed the matrix stacks that were once used to reference the projection and modelview matrices from the hardware. Because these matrix stacks no longer exist, the programmer must write matrix code that can be transferred to vertex shaders where the transformations will occur. That initially may seem challenging. However, several libraries and toolkits have been developed to assist with cross-platform development of OpenGL code. One of these libraries, GLM, or OpenGL Mathematics, has been developed to track the OpenGL and GLSL specifications closely so that interoperation between GLM and the hardware will work seamlessly.</p>
<section>
<h3 id="sec17_11_1"><span class="green">17.11.1 GLM</span></h3>
<p>GLM provides several basic math types useful to computer graphics. For our purposes, we will focus on just a few types and a handful of functions that make use of matrix transforms within the shaders easy. A few types that will be used include the following:</p>
<ul class="list-bullet">
<li>
<p class="list"><span style="color:green">glm::vec3</span>—a compact array of 3 floats that can be accessed using the same component-wise access found in the shaders;</p>
</li>
<li>
<p class="list"><span style="color:green">glm::vec4</span>—a compact array of 4 floats that can be accessed using the same component-wise access found in the shaders;</p>
</li>
<li>
<p class="list"><span style="color:green">glm::mat4</span>—a 4 × 4 matrix storage represented as 16 floats. The matrix is stored in column-major format.</p>
</li>
</ul>
<p>Similarly, GLM provides functions for creating the projection matrices, <strong>M</strong><sub>orth</sub> and <strong>M</strong><sub>p</sub>, as well as functions for generating the view matrix, <strong>M</strong><sub>cam</sub>:</p>
<ul class="list-bullet">
<li>
<p class="list"><span style="color:green">glm::ortho</span> creates a 4 × 4 orthographic projection matrix.</p>
</li>
<li>
<p class="list"><span style="color:green">glm::perspective</span> creates the 4 × 4 perspective matrix.</p>
</li>
<li>
<p class="list"><span style="color:green">glm::lookAt</span> creates the 4 × 4 homogeneous transform that translates and orients the camera.</p>
</li>
</ul>
</section>
<section>
<h3 id="sec17_11_2"><a id="index_term475"/><a id="term-574"/><span aria-label="480" epub:type="pagebreak" id="pg_480" role="doc-pagebreak"/><span class="green">17.11.2 Using an Orthographic Projection</span></h3>
<p>A simple extension to the previous example would be to place the triangle vertices into a more flexible coordinate system and render the scene using an orthographic projection. The vertices in the previous example could become:</p>
<pre class="pre">GLfloat vertices[] = {-3.0f, -3.0f, 0.0f, 3.0f, -3.0f, 0.0f, 0.0f, 3.0f, 0.0f}; </pre>
<p class="noindent">Using GLM, an orthographic projection can be created easily on the host. For instance,</p>
<pre class="pre">glm::mat4 projMatrix = glm::ortho(-5.0f, 5.0f, -5.0, 5.0, -10.0f, 10.0f); </pre>
<p class="noindent">The projection matrix can then be applied to each vertex transforming it into clip coordinates. The vertex shader will be modified to perform this operation:</p>
<p class="center"><strong>v</strong><sub>canon</sub> = <strong>M</strong><sub>orth</sub><strong>v</strong>.</p>
<p class="noindent">This computation will occur in a modified vertex shader that uses <em>uniform</em> variables to communicate data from the host to the device. Uniform variables represent static data that is invariant across the execution of a shader program. The data is the same for all elements and remains static. However, uniform variables can be modified by an application between executions of a shader. This is the primary mechanism that data within the host application can communicate changes to shader computations. Uniform data often represent the graphics state associated with an application. For instance, the projection, view, or model matrices can be set and accessed through uniform variables. Information about light sources within a scene may also be obtained through uniform variables.</p>
<p>Modifying the vertex shader requires adding a uniform variable to hold the projection matrix. We can use GLSL’s <span class="monospace">mat4</span> type to store this data. The projection matrix can then be used naturally to tranform the incoming vertices into the canonical coordinate system:</p>
<pre class="pre"><span class="bluez">#version</span> 330 core<br/><br/><span class="voilet">layout</span>(location=0) <span class="voilet">in vec3</span> in_Position;<br/><span class="voilet">uniform mat4</span> projMatrix;<br/><br/><span class="voilet">void</span> main(<span class="voilet">void</span>)<br/>{<br/><span class="bluez">gl_Position</span> = projMatrix * <span class="voilet">vec4</span>(in_Position, 1.0);<br/>}</pre>
<p class="noindent"><a id="term-593"/><a id="term-771"/><a id="term-781"/><span aria-label="481" epub:type="pagebreak" id="pg_481" role="doc-pagebreak"/>The application code need only transfer the uniform variable from the host memory (a GLM mat4) into the device’s shader program (a GLSL mat4). This is easy enough, but requires that the host side of the application acquire a handle to the uniform variable after the shader program has been linked. For instance, to obtain a handle to the <span class="monospace">projMatrix</span> variable, the following call would be issued once, after shader program linking is complete:</p>
<pre class="pre">GLint pMatID = glGetUniformLocation(shaderProgram, "projMatrix"); </pre>
<p class="noindent">The first argument is the shader program object handle and the second argument is the character string of the variable name in the shader. The id can then be used with a variety of OpenGL <span class="monospace">glUniform</span> function call to transfer the memory on the host into the device. However, shader programs must first be bound prior to setting the value related to a uniform variable. Also, because GLM is used to store the projection matrix on the host, a GLM helper function will be used to obtain a pointer to the underlying matrix, and allow the copy to proceed.</p>
<pre class="pre">glUseProgram( shaderID );<br/><br/>glUniformMatrix4fv(pMatID, 1, GL_FALSE, glm::value_ptr(projMatrix));<br/><br/>glBindVertexArray(VAO);<br/>glDrawArrays(GL_TRIANGLES, 0, 3);<br/>glBindVertexArray(0);<br/><br/>glUseProgram( 0 ); </pre>
<p class="noindent">Notice the form that <span class="monospace">glUniform</span> takes. The function name ends with characters that help define how it is used. In this case, a single 4 × 4 matrix of floats is being tranferred into the uniform variable. The <span class="monospace">v</span> indicates that an array contains the data, rather than passing by value. The third argument lets OpenGL know whether the matrix should be transposed (a potentially handy feature), and the last argument is a pointer to the memory where the matrix resides.</p>
<p>By this section of the chapter, you should have a sense for the role that shaders and <a id="index_term1061"/>vertex data play in rendering objects with OpenGL. Shaders, in particular, form a very important role in modern OpenGL. The remaining sections will further explore the role of shaders in rendering scenes, atempting to build upon the role that shaders play in other rendering styles presented in this book.</p>
</section>
</section>
<section>
<h2 id="sec17_12"><a id="index_term485"/><a id="index_term1311"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_12" role="doc-backlink"><span class="green">17.12 Shading with Per-Vertex Attributes</span></a></h2>
<p>The previous examples specified a single triangle with no additional data. Vertex attributes, such as normal vectors, texture coordinates, or even colors, can be <span aria-label="482" epub:type="pagebreak" id="pg_482" role="doc-pagebreak"/>interleaved with the vertex data in a vertex buffer. The memory layout is straightforward. Below, the color of each vertex is set after each vertex in the array. Three components are used to represent the red, green, and blue channels. Allocating the vertex buffer is identical with the exception being that the size of the array is now 18 <span class="monospace">GLfloats</span> instead of 9.</p>
<pre class="pre">GLfloat vertexData[] = {0.0f, 3.0f, 0.0f, 1.0f, 1.0f, 0.0f, -3.0f, -3.0f, 0.0f, 0.0f, 1.0f, 1.0f, 3.0f, -3.0f, 0.0f, 1.0f, 0.0f, 1.0f}; </pre>
<p class="noindent">The vertex array object specification is different. Because the color data is interleaved between vertices, the vertex attribute pointers must stride across the data appropriately. The second vertex attribute index must also be enabled. Building off the previous examples, we construct the new VAO as follows:</p>
<pre class="pre">glBindBuffer(GL_ARRAY_BUFFER, m_triangleVBO[0]);<br/><br/>glEnableVertexAttribArray(0);<br/>glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 6 * sizeof(GLfloat),<br/>                      0);<br/><br/>glEnableVertexAttribArray(1);<br/>glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 6 * sizeof(GLfloat),<br/>                      (const GLvoid *)12);</pre>
<p class="noindent">A single VBO is used and bound prior to setting the attributes since both vertex and color data are contained within the VBO. The first vertex attribute is enabled at index 0, which will represent the vertices in the shader. Note that the stride (the 5th argument) is different as the vertices are separated by six floats (e.g., the <em>x, y, z</em> of the vertex followed by the <em>r, g, b</em> of the color). The second vertex attribute index is enabled and will represent the vertex color attributes in the shader at location 1. It has the same stride, but the last argument now represents the pointer offset forthestartofthe first color value. While 12 is used in the above example, this is identical to stating <span class="monospace">3 * sizeof(GLfloat)</span>. In other words, we need to jump across the three floats representing the vertex <em>x, y, z</em> values to locate the first color attribute in the array.</p>
<p>The shaders for this example are only slightly modified. The primary differences in the vertex shader (shown below) are (1) the second attribute, color, is at location 1 and (2) <span class="monospace">vColor</span> is an output variable that is set in the main body of the vertex shader.</p>
<pre class="pre"><span class="bluez">#version</span> 330 <span class="bluez">core</span><br/><br/><span class="voilet">layout</span>(location=0) <span class="voilet">in vec3</span> in_Position;<br/><span class="voilet">layout</span>(location=1) <span class="voilet">in vec3</span> in_Color;<br/><span aria-label="483" epub:type="pagebreak" id="pg_483" role="doc-pagebreak"/><span class="voilet">out vec3</span> vColor;<br/><br/><span class="voilet">uniform mat4</span> projMatrix;<br/><br/><span class="voilet">void</span> main(<span class="voilet">void</span>)<br/>{<br/>  vColor = in_Color;<br/>  <span class="bluez">gl_Position</span> = projMatrix * <span class="voilet">vec4</span>(in_Position, 1.0);<br/>}</pre>
<p class="noindent">Recall that the keywords <span class="monospace">in</span> and <span class="monospace">out</span> refer to the flow of data between shaders. Data that flows out of the vertex shader becomes input data in the connected fragment shader, provided that the variable names match up. Moreover, <span class="monospace">out </span>variables that are passed to fragment shaders are interpolated across the fragments using barycentric interpolation. Some modification of the interpolation can be achieved with additional keywords, but this detail will be left to the reader. In this example, three vertices are specified, each with a specific color value. Within the fragment shader, the colors will be interpolated across the face of the triangle.</p>
<p>The fragment shader changes are simple. The <span class="monospace">vColor</span> variable that was set and passed <em>out</em> of the vertex shader now becomes an <em>in</em> variable. As fragments are processed, the <span class="monospace">vColor vec3</span> will contain the correctly interpolated values based on the location of the fragment within the triangle.</p>
<pre class="pre"><span class="bluez">#version</span> 330 <span class="bluez">core</span><br/><br/><span class="voilet">layout</span>(location=0) <span class="voilet">out vec4</span> fragmentColor;<br/><br/><span class="voilet">in vec3</span> vColor;<br/><br/><span class="voilet">void</span> main(<span class="voilet">void</span>)<br/>{<br/>  fragmentColor = <span class="voilet">vec4</span>(vColor, 1.0);<br/>}</pre>
<p class="noindent">The image that results from running this shader with the triangle data is shown in <a href="C22_chapter17.xhtml#f17_4">Figure 17.4</a>.</p>
<figure id="f17_4" tabindex="0">
<img alt="" src="../images/fig17_4.jpg"/>
<figcaption><p><span class="blue">Figure 17.4.</span> Setting the colors of each vertex in the vertex shader and passing the data to the fragment shader results in barycentric interpolation of the colors.</p></figcaption>
</figure>
<section>
<h3 id="sec17_12_1"><span class="green">17.12.1 Structs of Vertex Data</span></h3>
<p>The previous example illustrates the interleaving of data in an array. Vertex buffers can be used in a variety of ways, including separate vertex buffers for different model attributes. Interleaving data has advantages as the attributes associated with a vertex are near the vertex in memory and can likely take advantage <a id="term-376"/><a id="term-594"/><a id="term-772"/><span aria-label="484" epub:type="pagebreak" id="pg_484" role="doc-pagebreak"/>of memory locality when operating in the shaders. While the use of these interleaved arrays is straightforward, it can become cumbersome to manage large models in this way, especially as data structures are used for building robust (and sustainable) software infrastructure for graphics (see <a href="C17_chapter12.xhtml#c12">Chapter 12</a>). It is rather simple to store vertex data as vectors of structs that contain the vertex and any related attributes. When done this way, the structure need only be mapped into the vertex buffer. For instance, the following structure contains the vertex position and vertex color, using GLM’s <span class="monospace">vec3</span> type:</p>
<pre class="pre">struct vertexData<br/>{<br/>    glm::vec3 pos;<br/>    glm::vec3 color;<br/>};<br/>std::vector&lt; vertexData &gt; modelData;</pre>
<p class="noindent">The STL vector will hold all vertices related to all the triangles in the model. We will continue to use the same layout for triangles as in previous examples, which is a basic triangle strip. Every three vertices represents a triangle in the list. There are other data organizations that can be used with OpenGL, and <a href="C17_chapter12.xhtml#c12">Chapter 12</a> presents other options for organizing data more efficiently.</p>
<p>Once the data is loaded into the vector, the same calls used before load the data into the vertex buffer object:</p>
<pre class="pre">int numBytes = modelData.size() * sizeof(vertexData);<br/><a id="term-361"/><a id="term-377"/><a id="term-590"/><a id="term-766"/><a id="term-767"/><span aria-label="485" epub:type="pagebreak" id="pg_485" role="doc-pagebreak"/>glBufferData(GL_ARRAY_BUFFER, numBytes, modelData.data(), GL_STATIC_DRAW);<br/>glBindBuffer(GL_ARRAY_BUFFER, 0);</pre>
<p class="noindent">STL vectors store data contiguously. The <span class="monospace">vertexData</span> struct used above is represented by a flat memory layout (it does not contain pointers to other data elements) and is contiguous. However, the STL vector is an abstraction and the pointer that references the underlying memory must be queried using the <span class="monospace">data() </span>member. That pointer is provided to the call to <span class="monospace">glBufferData</span>. Attribute assignment in the vertex array object is identical as the locality of the vertex attributes remains the same.</p>
</section>
</section>
<section>
<h2 id="sec17_13"><a id="index_term1044"/><a id="index_term465"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_13" role="doc-backlink"><span class="green">17.13 Shading in the Fragment Processor</span></a></h2>
<p>The graphics pipeline chapter (<a href="C13_chapter8.xhtml#c8">Chapter 8</a>) and the surface shading chapter (<a href="C15_chapter10.xhtml#c10">Chapter 10</a>) do a nice job of describing and illustrating the effects of per-vertex and <a id="index_term1052"/>per-fragment shading as they relate to rasterization and shading in general. With modern <a id="index_term1048"/>graphics hardware, applying shading algorithms in the fragment processor produces better visual results and more accurately approximates lighting. Shading that is computed on a per-vertex basis is often subject to visual artifacts related to the underlying geometry tessellation. In particular, per-vertex based shading often fails to approximate the appropriate intensities across the face of the triangle since the lighting is only being calculated at each vertex. For example, when the distance to the light source is small, as compared with the size of the face being shaded, the illumination on the face will be incorrect. <a href="C22_chapter17.xhtml#f17_5">Figure 17.5</a> illustrates this situation. The center of the triangle will not be illuminated brightly, despite being very close to the light source, since the lighting on the vertices, which are far from the light source, are used to interpolate the shading across the face. Of course, increasing the tessellation of the geometry can improve the visuals. However, this solution is of limited use in <a id="index_term964"/>real-time graphics as the added geometry required for more accurate illumination can result in slower rendering.</p>
<figure id="f17_5" tabindex="0">
<img alt="" src="../images/fig17_5.jpg"/>
<figcaption><p><span class="blue">Figure 17.5.</span> The distance to the light source is small relative to the size of the triangle.</p></figcaption>
</figure>
<p>Fragment shaders operate on the fragments that emerge from rasterization after vertices have been transformed and clipped. Generally speaking, fragment shaders must output a value that is written to a framebuffer. Often times, this is the color of the pixel. If the depth test is enabled, the fragment’s depth value will be used to control whether the color and its depth are written to the framebuffer memory. The data that fragment shaders use for computation comes from various sources:</p>
<ul class="list-bullet">
<li>
<p class="list"><span style="color:green">Built-in OpenGL variables.</span> These variables are provided by the system. Examples of fragment shader variables include <span class="monospace">gl_FragCoord</span> or <span aria-label="486" epub:type="pagebreak" id="pg_486" role="doc-pagebreak"/><span class="monospace">gl_FrontFacing</span>. These variables can change based on revisions to OpenGL and GLSL, so it is advised that you check the specification for the version of OpenGL and GLSL that you are targeting.</p>
</li>
<li>
<p class="list"><span style="color:green">Uniform variables.</span> Uniform variables are transferred from the host to the device and can change as needed based on user input or changing simulation state in the application. These variables are declared and defined by the programmer for use within both vertex and fragment shaders. The projection matrix in the previous <a id="index_term1062"/>vertex shader examples was communicated to the shader via a uniform variable. If needed, the same uniform variable names can be used within both vertex and fragment shaders.</p>
</li>
<li>
<p class="list"><span style="color:green">Input variables.</span> <em>Input</em> variables are specified in the fragment shader with the prefixed keyword <span class="monospace">in</span>. Recall that data can flow into and out of shaders. Vertex shaders can output data to the next shader stage using the <span class="monospace">out</span> keyword (e.g., <span class="monospace">out vec3 vColor</span>, in a previous example). The outputs are linked to inputs when the next stage uses an <span class="monospace">in</span> keyword followed by the same type and name qualifiers (e.g., <span class="monospace">in vec3 vColor</span> in the previous example’s corresponding fragment shader).</p>
</li>
</ul>
<p>Any data that is passed to a fragment shader through the <em>in</em>-<em>out</em> linking mechanism will vary on a per-fragment basis using barycentric interpolation. The interpolation is computed outside of the shader by the graphics hardware. Within this infrastructure, fragment shaders can be used to perform per-fragment shading algorithms that evaluate specific equations across the face of the triangle. Vertex shaders provide support computations, transforming vertices and staging intermediate per-vertex values that will be interpolated for the fragment code.</p>
<p>The following shader program code implements per-fragment, Blinn-Phong shading. It brings together much of what has been presented in this chapter thus far and binds it to the shader descriptions from <a href="C09_chapter4.xhtml#c4">Chapter 4</a>. An interleaved vertex buffer is used to contain the vertex position and normal vectors. These values manifest in the vertex shader as vertex array attributes for index 0 and index 1. The shading computations that occur in the fragment shader code are performed in camera coordinates (sometimes referred to as eye-space).</p>
<section>
<h3 id="sec17_13_1"><span class="green">17.13.1 Blinn-Phong Shader Program: Vertex Shader</span></h3>
<p>The vertex shader stage of our program is used to transform the incoming vertices using the <strong>M</strong><sub>model</sub> and <strong>M</strong><sub>cam</sub> matrices into camera coordinates. It also uses the <span aria-label="487" epub:type="pagebreak" id="pg_487" role="doc-pagebreak"/>normal matrix, (<strong>M</strong><sup>–1</sup>)<sup><em>T</em></sup>, to appropriately transform the incoming normal vector attribute. The vertex shader outputs three variables to the fragment stage:</p>
<ul class="list-bullet">
<li>
<p class="list"><span style="color:green">•</span> <span class="monospace"><span style="color:green">normal</span></span>. The vertex’s normal vector as transformed into the camera coordinate system.</p>
</li>
<li>
<p class="list"><span style="color:green">•</span> <span class="monospace"><span style="color:green">h</span></span>. The half-vector needed for Blinn-Phong shading.</p>
</li>
<li>
<p class="list"><span style="color:green">•</span> <span class="monospace"><span style="color:green">l</span></span>. The light direction transformed into the camera coordinate system.</p>
</li>
</ul>
<p>Each of these variables will then be available for fragment computation, after applying barycentric interpolation across the three vertices in the triangle.</p>
<p>A single point light is used with this shader program. The light position and intensity is communicated to both the vertex and fragment shaders using a uniform variable. The light data is declared using GLSL’s struct qualifer, which allows variables to be grouped together in meaningful ways. Although not presented here, GLSL supports arrays and for-loop control structures, so additional lights could easily be added to this example.</p>
<p>All matrices are also provided to the vertex shader using uniform variables. For now, we will imagine that the model (or local transform) matrix will be set to the indentity matrix. In the following section, more detail will be provided to expand on how the model matrix can be specified on the host using GLM.</p>
<pre class="pre"><span class="bluez">#version</span> 330 <span class="bluez">core</span><br/><br/><span class="red">//</span><br/><span class="red">// Blinn-Phong Vertex Shader</span><br/><span class="red">//</span><br/><br/><span class="voilet">layout</span>(location=0) <span class="voilet">in vec3</span> in_Position;<br/><span class="voilet">layout</span>(location=1) <span class="voilet">in vec3</span> in_Normal;<br/><br/><span class="voilet">out vec4</span> normal;<br/><span class="voilet">out vec3</span> half;<br/><span class="voilet">out vec3</span> lightdir;<br/><br/><span class="voilet">struct</span> LightData {<br/>  <span class="voilet">vec3</span> position;<br/>  <span class="voilet">vec3</span> intensity;<br/>};<br/><span class="voilet">uniform</span> LightData light;<br/><br/><span class="voilet">uniform mat4</span> projMatrix;<br/><span class="voilet">uniform mat4</span> viewMatrix;<br/><span class="voilet">uniform mat4</span> modelMatrix;<br/><span class="voilet">uniform mat4</span> normalMatrix;<br/><a id="term-747"/><span aria-label="488" epub:type="pagebreak" id="pg_488" role="doc-pagebreak"/><span class="voilet">void</span> main(<span class="voilet">void</span>)<br/>{<br/>  <span class="red">// Calculate lighting in eye space: transform the local</span><br/>  <span class="red">// position to world and then camera coordinates.</span><br/>  <span class="voilet">vec4</span> pos = viewMatrix * modelMatrix * <span class="voilet">vec4</span>(in_Position, 1.0);<br/>  <span class="voilet">vec4</span> lightPos = viewMatrix * <span class="voilet">vec4</span>(light.position, 1.0);<br/><br/>  normal = normalMatrix * <span class="voilet">vec4</span>(in_Normal, 0.0);<br/><br/>  <span class="voilet">vec3</span> v = <span class="voilet">normalize</span>( -pos.xyz );<br/>  lightdir = <span class="voilet">normalize</span>( lightPos.xyz - pos.xyz );<br/>  half = <span class="voilet">normalize</span>( v + lightdir );<br/><br/>  <span class="bluez">gl_Position</span> = projMatrix * pos;<br/>}</pre>
<p>The vertex shader’s main function first transforms the position and light position into camera coordinates using <span class="monospace">vec4</span> types to correspond with the 4 × 4 matrices of GLSL’s <span class="monospace">mat4</span>. We then transform the normal vector and store it in the <span class="monospace">out vec4 normal</span> variable. The view (or eye) vector and light direction vector are then calculated, which leads to the computation of the half vector needed for <a id="index_term1036"/>Blinn-Phong shading. The final computation completes the calculation of</p>
<p class="center"><strong>v</strong><sub>canon</sub> = <strong>M</strong><sub>proj</sub><strong>M</strong><sub>cam</sub><strong>M</strong><sub>model</sub><strong>v</strong></p>
<p class="noindent">by applying the projection matrix. It then sets the canonical coordinates of the vertex to the built-in GLSL vertex shader output variable <span class="monospace">gl_Position</span>.After this, the vertex is in clip-coordinates and is ready for rasterization.</p>
</section>
<section>
<h3 id="sec17_13_2"><a id="index_term1045"/><span class="green">17.13.2 Blinn-Phong Shader Program: Fragment Shader</span></h3>
<p>The fragment shader computes the Blinn-Phong shading model. It receives barycentric interpolated values for the vertex normal, half vector, and light direction. Note that these variables are specified using the <span class="monospace">in</span> keyword as they come <em>in</em> from the vertex processing stage. The light data is also shared with the fragment shader using the same uniform specification that was used in the vertex shader. The matrices are not required so no uniform matrix variables are declared. The material properties for the geometric model are communicated through uniform variables to specify <em>k<sub>a</sub>,k<sub>d</sub>,k<sub>s</sub>,I<sub>a</sub></em>,and <em>p</em>. Together, the data allow the fragment shader to compute Equation 4.3:</p>
<p class="center"><em>L</em> = <em>k<sub>a</sub> I<sub>a</sub></em> + <em>k<sub>d</sub> Imax</em>(0<em>,</em> <strong>n</strong> · <strong>l</strong>)+ <em>k<sub>s</sub> Imax</em>(0<em>,</em> <strong>n</strong> · <strong>h</strong>)<em><sup>p</sup></em></p>
<p class="noindent">at each fragment.</p>
<pre class="pre"><span aria-label="489" epub:type="pagebreak" id="pg_489" role="doc-pagebreak"/><br/><span class="bluez">#version</span> 330 <span class="bluez">core</span><br/><br/><span class="red">//</span><br/><span class="red">// Blinn-Phong Fragment Shader</span><br/><span class="red">//</span><br/><br/><span class="voilet">in vec4</span> normal;<br/><span class="voilet">in vec3</span> half;<br/><span class="voilet">in vec3</span> lightdir;<br/><br/><span class="voilet">layout</span>(location=0) <span class="voilet">out vec4</span> fragmentColor;<br/><br/><span class="voilet">struct</span> LightData {<br/>  <span class="voilet">vec3</span> position;<br/>  <span class="voilet">vec3</span> intensity;<br/>};<br/><span class="voilet">uniform</span> LightData light;<br/><br/><span class="voilet">uniform vec3</span> Ia;<br/><span class="voilet">uniform vec3</span> ka, kd, ks;<br/><span class="voilet">uniform float</span> phongExp;<br/><br/><span class="voilet">void</span> main(<span class="voilet">void</span>)<br/>{<br/>  <span class="voilet">vec3</span> n = <span class="voilet">normalize</span>(normal.xyz);<br/>  <span class="voilet">vec3</span> h = <span class="voilet">normalize</span>(half);<br/>  <span class="voilet">vec3</span> l = <span class="voilet">normalize</span>(lightdir);<br/><br/>  <span class="voilet">vec3</span> intensity = ka * Ia<br/>                   + kd * light.intensity * <span class="voilet">max</span>( 0.0, <span class="voilet">dot</span>(n, l) )<br/>                   + ks * light.intensity<br/>                        * <span class="voilet">pow</span>( <span class="voilet">max</span>( 0.0, <span class="voilet">dot</span>(n, h) ), phongExp );<br/>  fragmentColor = <span class="voilet">vec4</span>( intensity, 1.0 );<br/>} </pre>
<p class="noindent">The fragment shader writes the computed intensity to the fragment color output buffer. <a href="C22_chapter17.xhtml#f17_6">Figure 17.6</a> illustrates several examples that show the effect of per-<a id="index_term374"/>fragment shading across varying degrees of tessellation on a geometric model. This fragment shader introduces the use of structures for holding uniform variables. It should be noted that they are user-defined structures, and in this example, the <span class="monospace">LightData</span> type holds only the light position and its intensity. In host code, the uniform variables in structures are referenced using the fully qualified variable name when requesting the handle to the uniform variable, as in:</p>
<pre class="pre">lightPosID = shader.createUniform( "light.position" );<br/>lightIntensityID = shader.createUniform( "light.intensity" );</pre>
<figure id="f17_6" tabindex="0">
<img alt="" src="../images/fig17_6.jpg"/>
<figcaption><p><span class="blue">Figure 17.6.</span> Per-fragment shading applied across increasing tessellation of a subdivision sphere. The specular highlight is apparent with lower tessellations.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec17_13_3"><span aria-label="490" epub:type="pagebreak" id="pg_490" role="doc-pagebreak"/><span class="green">17.13.3 A Normal Shader</span></h3>
<p>Once you have a working shader program, such as the Blinn-Phong one presented here, it is easy to expand your ideas and develop new shaders. It may also be helpful to develop a set of very specific shaders for debugging. One such shader is the normal shader program. Normal shading is often helpful to understand whether the incoming geometry is organized correctly or whether the computations are correct. In this example, the vertex shader remains the same. Only the fragment shader changes:</p>
<pre class="pre"><span class="bluez">#version</span> 330 <span class="bluez">core</span><br/><br/><span class="voilet">in vec4</span> normal;<br/><br/><span class="voilet">layout</span>(location=0) <span class="voilet">out vec4</span> fragmentColor;<br/><br/><span class="voilet">void</span> main(<span class="voilet">void</span>)<br/>{<br/>  <span class="red">// Notice the use of swizzling here to access</span><br/>  <span class="red">// only the xyz values to convert the normal vec4</span><br/>  <span class="red">// into a vec3 type!</span><br/>  <span class="voilet">vec3</span> intensity = <span class="voilet">normalize</span>(normal.xyz) * 0.5 + 0.5;<br/>  fragmentColor = <span class="voilet">vec4</span>( intensity, 1.0 );<br/>}</pre>
<p class="noindent">Whichever shaders you start building, be sure to comment them! The GLSL specification allows comments to be included in shader code, so leave yourself some details that can guide you later.</p>
</section>
</section>
<section>
<h2 id="sec17_14"><a id="index_term1050"/><a id="index_term1049"/><a id="term-362"/><a id="term-365"/><a id="term-466"/><a id="term-467"/><a id="term-526"/><a id="term-527"/><a id="term-762"/><a id="term-763"/><span aria-label="491" epub:type="pagebreak" id="pg_491" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_14" role="doc-backlink"><span class="green">17.14 Meshes and Instancing</span></a></h2>
<p>Once basic shaders are working, it’s interesting to start creating more complex scenes. Some 3D model files are simple to load and others require more effort. One simple 3D object file representation is the OBJ format. OBJ is a widely used format and several codes are available to load these types of files. The array of structs mechanism presented earlier works well for containing the OBJ data on the host. It can then easily be transferred into a VBO and vertex array objects.</p>
<p>Many 3D models are defined in their own local coordinate systems and need various transformations to align them with the OpenGL coordinate system. For instance, when the Stanford Dragon’s OBJ file is loaded into the OpenGL coordinate system, it appears lying on its side at the origin. Using GLM, we can create the model transformations to place objects within our scenes. For the dragon model, this means rotating –90 degrees about <span class="inline-formula"><m:math alttext=""><m:mrow><m:mover><m:mrow><m:mi>X</m:mi></m:mrow><m:mrow><m:mo>→</m:mo></m:mrow></m:mover></m:mrow></m:math></span>, and then translating up in <span class="inline-formula"><m:math alttext=""><m:mrow><m:mover><m:mrow><m:mi>Y</m:mi></m:mrow><m:mrow><m:mo>→</m:mo></m:mrow></m:mover></m:mrow></m:math></span>. The effective model transform becomes</p>
<p class="center"><strong>M</strong><sub>model</sub> = <strong>M</strong><sub>translate</sub> <strong>M</strong><sub>rotX</sub>,</p>
<p class="noindent">and the dragon is presented upright and above the ground plane, as shown in <a href="C22_chapter17.xhtml#f17_7">Figure 17.7</a>. To do this we utilize several functions from GLM for generating local model transforms:</p>
<ul class="list-bullet">
<li>
<p class="list"><span style="color:green">glm::translate</span> creates a translation matrix.</p>
</li>
<li>
<p class="list"><span style="color:green">glm::rotate</span> creates a rotation matrix, specified in either degrees or radians about a specificaxis.</p>
</li>
<li>
<p class="list"><span style="color:green">glm::scale</span> creates a scale matrix.</p>
</li>
</ul>
<figure id="f17_7" tabindex="0">
<img alt="" src="../images/fig17_7.jpg"/>
<figcaption><p><span class="blue">Figure 17.7.</span> Images are described from left to right. The default local orientation of the dragon, lying on its side. After a –90 degree rotation about <span class="inline-formula"><m:math alttext=""><m:mrow><m:mover><m:mrow><m:mi>X</m:mi></m:mrow><m:mrow><m:mo>→</m:mo></m:mrow></m:mover></m:mrow></m:math></span>, the dragon is upright but still centered about the origin. Finally, after applying a translation of 1.0 in <span class="inline-formula"><m:math alttext=""><m:mrow><m:mover><m:mrow><m:mi>Y</m:mi></m:mrow><m:mrow><m:mo>→</m:mo></m:mrow></m:mover></m:mrow></m:math></span>, the dragon is ready for instancing.</p></figcaption>
</figure>
<p class="noindent"><span aria-label="492" epub:type="pagebreak" id="pg_492" role="doc-pagebreak"/>We can apply these functions to create the model transforms and pass the model matrix to the shader using uniform variables. The Blinn-Phong vertex shader contains instructions that apply the local transform to the incoming vertex. The following code shows how the dragon model is rendered:</p>
<pre class="pre">glUseProgram( BlinnPhongShaderID );<br/><br/><span class="red">// Describe the Local Transform Matrix</span><br/>glm::mat4 modelMatrix = glm::mat4(1.0); <span class="red">// Identity Matrix</span><br/>modelMatrix = glm::translate(modelMatrix, glm::vec3(0.0f, 1.0f, ↩ 0.0f));<br/><span class="voilet">float</span> rot = (-90.0f / 180.0f) * M_PI;<br/>modelMatrix = glm::rotate(modelMatrix, rot, glm::vec3(1, 0, 0));<br/><br/><span class="red">// Set the Normal Matrix</span><br/>glm::mat4 normalMatrix = glm::transpose( glm::inverse( viewMatrix↩ * modelMatrix ) );<br/><br/><span class="red">// Pass the matrices to the GPU memory</span><br/>glUniformMatrix4fv(nMatID, 1, GL_FALSE, glm::value_ptr(↩normalMatrix));<br/>glUniformMatrix4fv(pMatID, 1, GL_FALSE, glm::value_ptr(projMatrix↩));<br/>glUniformMatrix4fv(vMatID, 1, GL_FALSE, glm::value_ptr(viewMatrix↩));<br/>glUniformMatrix4fv(mMatID, 1, GL_FALSE, glm::value_ptr(↩modelMatrix));<br/><br/><span class="red">// Set material for this object</span><br/>glm::vec3 kd( 0.2, 0.2, 1.0 );<br/>glm::vec3 ka = kd * 0.15f;<br/>glm::vec3 ks( 1.0, 1.0, 1.0 );<br/><span class="voilet">float</span> phongExp = 32.0;<br/><br/>glUniform3fv(kaID, 1, glm::value_ptr(ka));<br/>glUniform3fv(kdID, 1, glm::value_ptr(kd));<br/>glUniform3fv(ksID, 1, glm::value_ptr(ks));<br/>glUniform1f(phongExpID, phongExp);<br/><br/><span class="red">// Process the object and note that modelData.size() holds</span><br/><span class="red">// the number of vertices, not the number of triangles!</span><br/>glBindVertexArray(VAO);<br/>glDrawArrays(GL_TRIANGLES, 0, modelData.size());<br/>glBindVertexArray(0);<br/><br/>glUseProgram( 0 );</pre>
<section>
<h3 id="sec17_14_1"><a id="index_term748"/><a id="term-55"/><a id="term-295"/><a id="term-378"/><a id="term-591"/><a id="term-748"/><a id="term-758"/><a id="term-768"/><a id="term-877"/><span aria-label="493" epub:type="pagebreak" id="pg_493" role="doc-pagebreak"/><span class="green">17.14.1 Instancing Models</span></h3>
<p>Instancing with OpenGL is implemented differently than instancing with the ray tracer. With the ray tracer, rays are inversely transformed into the local space of the object using the model transform matrix. With OpenGL, instancing is performed by loading a single copy of the object as a vertex array object (with associated vertex buffer objects), and then reusing the geometry as needed. Like the ray tracer, only a single object is loaded into memory, but many may be rendered.</p>
<p>Modern OpenGL nicely supports this style of instancing because vertex shaders can (and must) compute the necessary transformations to transform vertices into clip coordinates. By writing generalized shaders that embed these transformations, such as presented with the Blinn-Phong vertex shader, models can be rerendered with the same underlying local geometry. Different material types and transforms can be queried from higher-level class structures to populate the uniform variables passed from host to device each frame. Animations and interactive control are also easily created as the model transforms can change over time across the the display loop iteration. <a href="C22_chapter17.xhtml#f17_8">Figures 17.8</a> and <a href="C22_chapter17.xhtml#f17_9">17.9</a> use the memory footprint of one dragon, yet render three different dragon models to the screen.</p>
<figure id="f17_8" tabindex="0">
<img alt="" src="../images/fig17_8.jpg"/>
<figcaption><p><span class="blue">Figure 17.8.</span> The results of running the Blinn-Phong shader program on the three dragons using uniform variables to specify material properties and transformations.</p></figcaption>
</figure>
<figure id="f17_9" tabindex="0">
<img alt="" src="../images/fig17_9.jpg"/>
<figcaption><p><span class="blue">Figure 17.9.</span> Setting the uniform variable <em>k<sub>s</sub></em> = (0, 0, 0) in the Blinn-Phong shader program produces Lambertian shading.</p></figcaption>
</figure>
</section>
</section>
<section>
<h2 id="sec17_15"><a id="index_term1185"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_15" role="doc-backlink"><span class="green">17.15 Texture Objects</span></a></h2>
<p><a id="index_term1059"/>Textures are an effective means to manipulate visual effects with OpenGL shaders. They are used extensively with many hardware-based graphics algorithms and OpenGL supports them natively with <em>Texture objects</em>. Like the previous OpenGL concepts, texture objects must be allocated and initialized by copying data on the host to the GPU memory and setting OpenGL state. Texture coordinates are often integrated into the vertex buffer objects and passed as vertex attributes to shader programs. Fragment shaders typically perform the texture lookup function using interpolated texture coordinate passed from the vertex shaders.</p>
<p>Textures are rather simple to add to your code if you already have working shader and vertex array objects. The standard OpenGL techniques for creating objects on the hardware are used with textures. However, the source of the texture data must first be determined. Data can either be loaded from a file (e.g., PNG, JPG, EXR, or HDR image file formats) or generated procedurally on the host (and even on the GPU). After the data is loaded into host memory, the data is copied to GPU memory, and optionally, OpenGL state associated with textures can be set. OpenGL texture data is loaded as a linear buffer of memory containing the data used for textures. Texture lookups on the hardware can be 1D, 2D, or 3D queries. Regardless of the texture dimension query, the data is loaded onto the <span aria-label="494" epub:type="pagebreak" id="pg_494" role="doc-pagebreak"/>memory in the same way, using linearly allocated memory on the host. In the following example, the process of loading data from an image file (or generating it procedurally) is left to the reader, but variable names are provided that match what might be present if an image is loaded (e.g., imgData, imgWidth, imgHeight).</p>
<pre class="pre">float *imgData = new float[ imgHeight * imgWidth * 3 ];<br/>...<br/>GLuint texID;<br/>glGenTextures(1, &amp;texID);<br/>glBindTexture(GL_TEXTURE_2D, texID);<br/>glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP);<br/>glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP);<br/>glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);<br/>glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);<br/>glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, imgWidth, imgHeight, 0,<br/>             GL_RGB, GL_fLOAT, imgData);<br/>glBindTexture(GL_TEXTURE_2D, 0);<br/><br/>delete [] imgData;</pre>
<p>The example presented here highlights how to set up and use basic 2D OpenGL textures with shader programs. The process for creating OpenGL objects should be familiar by now. A handle (or ID) must be generated on the device to refer to the texture object (e.g., in this case, <span class="monospace">texID</span>). The id is then bound to allow any subsequent texture state operations to affect the state of the texture. A fairly extensive set of OpenGL texture state and parameters exist that affect texture coordinate interpretation and texture lookup filtering. Various texture targets exist with graphics hardware. In this case, the texture target is specified as <span class="monospace">GL_TEXTURE_2D</span> and will appear as the first argument in the texture-related functions. For OpenGL this particular texture target implies that texture coordinates will be specified in a device normalized manner (i.e., in the range of [0, 1]). Moreover, texture data must be allocated so that the width and height dimensions are powers of two (e.g., 512 × 512, 1024 × 512, etc.). Texture parameters are set for the currently bound texture by calling <span class="monospace">glTexParameter</span>. This signature for this function takes on a variety of forms depending on the types of data being set. In this case, texture coordinates will be clamped by the hardware to the explicit range [0, 1]. The minifying and magnifying filters of OpenGL texture objects are set to use linear filtering (rather than nearest neighbor - <span class="monospace">GL_NEAREST</span>) automatically when performing texture lookups. <a href="C16_chapter11.xhtml#c11">Chapter 11</a> provides substantial details on texturing, including details about the filtering that can occur with texture lookups. Graphics hardware can perform many of these operations automatically by setting the associated texture state.</p>
<p>Finally, the call to <span class="monospace">glTexImage2D</span> performs the host to device copy for the texture. There are several arguments to this function, but the overall operation is to <a id="term-776"/><a id="term-878"/><span aria-label="495" epub:type="pagebreak" id="pg_495" role="doc-pagebreak"/>allocate space on the graphics card (e.g., <em>imageWidth</em> X <em>imgHeight</em>)ofthree floats (7th and 8th arguments: <span class="monospace">GL_RGB</span> and <span class="monospace">GL_FLOAT</span>) and copy the linear texture data to the hardware (e.g., <em>imgData</em> pointer). The remaining arguments deal with setting the mipmap level of detail (2nd argument), specifying the internal format (e.g., 3rd argument’s <span class="monospace">GL_RGB</span>) and whether the texture has a border or not (6th argument). When learning OpenGL textures it is safe to keep these as the defaults listed here. However, the reader is advised to learn more about mipmaps and the potential internal formats of textures as more advanced graphics processing is required.</p>
<p>Texture object allocation and initialization happens with the code above. Additional modifications must be made to vertex buffers and vertex array objects to link in the correct texture coordinates with the geometric description. Following the previous examples, the storage for texture coordinates is a straightforward modification to the vertex data structure:</p>
<pre class="pre">struct vertexData<br/>{<br/>    glm::vec3 pos;<br/>    glm::vec3 normal;<br/>    glm::vec2 texCoord;<br/>};</pre>
<p>As a result, the vertex buffer object will increase in size and the interleaving of texture coordinates will require a change to the stride in the vertex attribute specification for the vertex array objects. <a href="C22_chapter17.xhtml#f17_10">Figure 17.10</a> illustrates the basic interleaving of data within the vertex buffer.</p>
<figure id="f17_10" tabindex="0">
<img alt="" src="../images/fig17_10.jpg"/>
<figcaption><p><span class="blue">Figure 17.10.</span> Data layout after adding the texture coordinate to the vertex buffer. Each block represents a GLfloat, which is 4 bytes. The position is encoded as a white block, the normals as purple, and the texture coordinates as orange.</p></figcaption>
</figure>
<pre class="pre">glBindBuffer(GL_ARRAY_BUFFER, m_triangleVBO[0]);<br/><br/>glEnableVertexAttribArray(0);<br/>glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), 0);<br/><br/>glEnableVertexAttribArray(1);<br/>glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (const GLvoid *)12);<br/><br/>glEnableVertexAttribArray(2);<br/>glVertexAttribPointer(2, 2, GL_FLOAT, GL_FALSE, 8 * sizeof(GLfloat), (const GLvoid *)24);<br/><br/>glBindVertexArray(0);</pre>
<p class="noindent"><span aria-label="496" epub:type="pagebreak" id="pg_496" role="doc-pagebreak"/>With the code snippet above, the texture coordinates are placed at vertex attribute location 2. Note the change in size of the texture coordinate’s size (e.g., 2nd argument of <span class="monospace">glVertexAttribPointer</span> is 2 for texture coordinates to coincide with the <span class="monospace">vec2</span> type in the structure). At this point, all initialization will have been completed for the texture object.</p>
<p>The texture object must be enabled (or bound) prior to rendering the vertex array object with your shaders. In general, graphics hardware allows the use of multiple texture objects when executing a shader program. In this way, shader programs can apply sophisticated texturing and visual effects. Thus, to bind a texture for use with a shader, it must be associated to one of potentially many <em>texture units</em>. Texture units represent the mechanism by which shaders can use multiple textures. In the sample below, only one texture is used so texture unit 0 will be made active and bound to our texture.</p>
<p>The function that activates a texture unit is <span class="monospace">glActiveTexture</span>. Its only argument is the texture unit to make active. It is set to <span class="monospace">GL_TEXTURE0</span> below, but it could be <span class="monospace">GL_TEXTURE1</span> or <span class="monospace">GL_TEXTURE2</span>, for instance, if multiple textures were needed in the shader. Once a texture unit is made active, a texture object can be bound to it using the <span class="monospace">glBindTexture</span> call.</p>
<pre class="pre">glUseProgram(shaderID);<br/><br/>glActiveTexture(GL_TEXTURE0);<br/>glBindTexture(GL_TEXTURE_2D, texID);<br/>glUniform1i(texUnitID, 0);<br/><br/>glBindVertexArray(VAO);<br/>glDrawArrays(GL_TRIANGLES, 0, 3);<br/>glBindVertexArray(0);<br/><br/>glBindTexture(GL_TEXTURE_2D, 0);<br/><br/>glUseProgram(0);</pre>
<p class="noindent">Most of the code above should be logical extensions to what you’ve developed thus far. Note the call to <span class="monospace">glUniform</span> prior to rendering the vertex array object. In modern graphics hardware programming, shaders perform the work of texture lookups and blending, and therefore, must have data about which texture units hold the textures used in the shader. The active texture units are supplied to shaders using uniform variables. In this case, 0 is set to indicate that the texture lookups will come from texture unit 0. This will be expanded upon in the following section.</p>
<section>
<h3 id="sec17_15_1"><span aria-label="497" epub:type="pagebreak" id="pg_497" role="doc-pagebreak"/><span class="green">17.15.1 Texture Lookup in Shaders</span></h3>
<p>Shader programs perform the lookup and any blending that may be required. The bulk of that computation typically goes into the fragment shader, but the vertex shader often stages the fragment computation by passing the texture coordinate out to the fragment shader. In this way, the texture coordinates will be interpolated and afford per-fragment lookup of texture data.</p>
<p>Simple changes are required to use texture data in shader programs. Using the Blinn-Phong vertex shader provided previously, only three changes are needed:</p>
<ol class="list-order">
<li>
<p class="list">The texture coordinates are a per-vertex attribute stored within the vertex array object. They are associated with vertex attribute index 2 (or location 2).</p>
<pre class="pre">layout(location=2) in vec2 in_TexCoord;</pre>
</li>
<li>
<p class="list">The fragment shader will perform the texture lookup and will need an interpolated texture coordinate. This variable will be added as an output variable that gets passed to the fragment shader.</p>
<pre class="pre">out vec2 tCoord;</pre>
</li>
<li>
<p class="list">Copy the the incoming vertex attribute to the output variable in the main function.</p>
<pre class="pre">// Pass the texture coordinate to the fragment shader tCoord = in_TexCoord;</pre>
</li>
</ol>
<p class="indent">The fragment shader also requires simple changes. First, the incoming interpolated texture coordinates passed from the vertex shader must be declared. Also recall that a uniform variable should store the texture unit to which the texture is bound. This must be communicated to the shader as a <em>sampler</em> type. Samplers are a shading language type that allows the lookup of data from a single texture object. In this example, only one sampler is required, but in shaders in which multiple texture lookups are used, multiple sampler variables will be used. There are also multiple sampler types depending upon the type of texture object. In the example presented here, a <span class="monospace">GL_TEXTURE 2D</span> type was used to create the texture state. The associated sampler within the fragment shader is of type <span class="monospace">sampler2D</span>. The following two variable declarations must be added to the fragment shader:</p>
<pre class="pre">in vec2 tCoord;<br/>uniform sampler2D textureUnit;</pre>
<p><a id="term-777"/><a id="term-879"/><span aria-label="498" epub:type="pagebreak" id="pg_498" role="doc-pagebreak"/>The final modification goes into the main function of the fragment shader code. The texture is sampled using the GLSL <span class="monospace">texture</span> lookup function and (in this case), replaces the diffuse coefficient of the geometry. The first argument to <span class="monospace">texture</span> takes the sampler type which holds the texture unit to which the texture is bound. The second argument is the texture coordinate. The function returns a <span class="monospace">vec4</span> type. In the code snippet below, no alpha values are utilized in the final computation so the resulting texture lookup value is component-wise selected to only the RGB components. The diffuse coefficient from the texture lookup is set to a <span class="monospace">vec3</span> type that is used in the illumination equation.</p>
<pre class="pre">vec3 kdTexel = texture(textureUnit, tCoord).rgb;<br/>vec3 intensity = ka * Ia + kdTexel * light.intensity<br/>                 * max( 0.0, dot(n, l) ) + ks * light.intensity<br/>                 * pow( max( 0.0, dot(n, h) ), phongExp );</pre>
<p class="noindent"><a href="C22_chapter17.xhtml#f17_11">Figure 17.11</a> illustrates the results of using these shader modifications. The right-most image in the figure extends the example code by enabling texture tiling with the OpenGL state. Note that these changes are only done in host code and the shaders do not change. To enable this tiling, which allows for texture coordinates outside of the device normalized ranges, the texture parameters for <span class="monospace">GL_TEXTURE WRAP S</span> and <span class="monospace">GL_TEXTURE_WRAP_T</span> are changed from <span class="monospace">GL_CLAMP</span> to <span class="monospace">GL_REPEAT</span>. Additionally, the host code that sets the texture coordinates now ranges from [0, 5].</p>
<figure id="f17_11" tabindex="0">
<img alt="" src="../images/fig17_11.jpg"/>
<figcaption><p><span class="blue">Figure 17.11.</span> The left-most image shows the texture, a 1024 × 1024 pixel image. The middle image shows the scene with the texture applied using texture coordinates in the range of [0, 1] so that only one image is tiled onto the ground plane. The right-most image modifies the texture parameters so that <span class="monospace">GL_REPEAT</span> is used for <span class="monospace">GL_TEXTURE_WRAP_S</span> and <span class="monospace">GL_TEXTURE_WRAP_T</span> and the texture coordinate range from [0, 5]. The result is a tiled texture repeat five times in both texture dimensions.</p></figcaption>
</figure>
<p>As a side note, another texture target that may be useful for various applications is the <span class="monospace">GL_TEXTURE_RECTANGLE</span>. Texture rectangle are unique texture objects that are not constrained with the power-of-two width and height image requirements and use non-normalized texture coordinates. Furthermore, they do <a id="term-363"/><a id="term-364"/><a id="term-366"/><a id="term-468"/><a id="term-469"/><a id="term-559"/><a id="term-560"/><span aria-label="499" epub:type="pagebreak" id="pg_499" role="doc-pagebreak"/>not allow repeated tiling. If texture rectangles are used, shaders must reference them using the special sampler type: <span class="monospace">sampler2DRect</span>.</p>
</section>
</section>
<section>
<h2 id="sec17_16"><a id="index_term791"/><a id="index_term480"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_16" role="doc-backlink"><span class="green">17.16 Object-Oriented Design for Graphics Hardware Programming</span></a></h2>
<p>As your familiarity with OpenGL increases, it becomes wise to encapsulate most of what is described in this chapter into class structures that can contain the model specific data and afford rendering of a variety of objects within the scene. For instance, in <a href="C22_chapter17.xhtml#f17_12">Figure 17.12</a>, a single sphere is instanced six times to create the three ellipsoids and three spheres. Each model uses the same underlying geometry yet has different material properties and model transforms. If you’ve followed through the book and implemented the ray tracer, as detailed in <a href="C09_chapter4.xhtml#c4">Chapter 4</a>, then it is likely that your implementation is based on a solid object-oriented design. That design can be leveraged to make developing a graphics hardware program with OpenGL easier. A typical ray tracer software architecture will include several classes that map directly into graphics hardware as well as software rasterization applications. The abstract base classes in the ray tracer that represent surfaces, materials, lights, shaders, and cameras can be adapted to initialize the graphics hardware state, update that state, and if necessary render the class data to the framebuffer. The interfaces to these virtual functions will likely need to be <a id="term-360"/><span aria-label="500" epub:type="pagebreak" id="pg_500" role="doc-pagebreak"/>adapted to your specific implementation, but a first pass that extends the surface class design might resemble the following:</p>
<ul class="list-simple">
<li>
<p class="list-item">class surface</p>
</li>
<li>
<p class="list-item">virtual bool initializeOpenGL()</p>
</li>
<li>
<p class="list-item">virtual bool renderOpenGL( glm::mat4&amp; <strong>M</strong><sub>p</sub>, glm::mat4&amp; <strong>M</strong><sub>cam</sub>)</p>
</li>
</ul>
<figure id="f17_12" tabindex="0">
<img alt="" src="../images/fig17_12.jpg"/>
<figcaption><p><span class="blue">Figure 17.12.</span> On the left, a single tessellated sphere is instanced six times using different model transforms to create this scene using the per-fragment shader program. The image on the right is rendered using a basic Whitted ray tracer. Notice the effect that shadows have on the perception of the scene. Per-fragment shading allows the specular highlight to be similar in both rendering styles.</p></figcaption>
</figure>
<p>Passing the projection and view matrices to the render functions affords an indirection for how these matrices are managed. These matrices would come from the camera classes which may be manipulated by interpreting keyboard, mouse, or joystick input. The initialization functions (at least for the surface derivatives) would contain the vertex buffer object and vertex array object allocation and initialization code. Aside from initiating the draw arrays for any vertex array objects, the render function would also need to activate shader programs and pass in the necessary matrices into the shaders, as illustrated previously in the dragon model example. As you work to integrate the image-order and object-order (hardware and software) algorithms into the same underlying data framework, a few software design challenges will pop up, mostly related to data access and organization. However, this is a highly useful exercise to become adept at software engineering for graphics programming and eventually gain solid experience hybridizing your rendering algorithms.</p>
</section>
<section>
<h2 id="sec17_17"><a epub:type="backlink" href="C02a_toc.xhtml#rsec17_17" role="doc-backlink"><span class="green">17.17 Continued Learning</span></a></h2>
<p>This chapter was designed to provide an introductory glimpse into graphics hardware programming, influenced by the OpenGL API. There are many directions that your continued learning could go. Many topics, such as framebuffer objects, render to texture, environment mapping, geometry shaders, compute shaders, and advanced illumination shaders were not covered. These areas represent the next stages in learning about graphics hardware, but even within the areas covered, there are many directions that one could go to develop stronger graphics hardware understanding. Graphics hardware programming will continue to evolve and change. Interested readers should expect these changes and look to the specification documents for OpenGL and the OpenGL Shading Language for many more details about what OpenGL is capable of doing and how the hardware relates to those computations.</p>
</section>
<section>
<h2 id="sec67"><a id="term-374"/><a id="term-745"/><a id="term-746"/><a id="term-751"/><span aria-label="501" epub:type="pagebreak" id="pg_501" role="doc-pagebreak"/><span class="green">Frequently Asked Questions</span></h2>
<ul class="list-bullet">
<li>
<p class="list"><span class="green">How do I debug shader programs?</span></p></li>
</ul>
<p class="noindent1b">On most platforms, debugging both vertex shaders and fragment shaders is not simple. However, more and more support is available through various drivers, operating system extensions, and IDEs to provide pertinent information to the developer. It still can be challenging, so use the shaders to visually debug your code. If nothing comes up on the screen, try rendering the normal vectors, the half vector, or anything that give you a sense for where the error might be (or not be). <a href="C22_chapter17.xhtml#f17_13">Figure 17.13</a> illustrates a normal shader in operation. If images do appear on your window, make sure they are what you expect (refer to <a href="C22_chapter17.xhtml#f17_14">Figure 17.14</a>)!</p>
<figure id="f17_13" tabindex="0">
<img alt="" src="../images/fig17_13.jpg"/>
<figcaption><p><span class="blue">Figure 17.13.</span> Applying the normal shader to a complex model for debugging purposes.</p></figcaption>
</figure>
<figure id="f17_14" tabindex="0">
<img alt="" src="../images/fig17_14.jpg"/>
<figcaption><p><span class="blue">Figure 17.14.</span> Visual debugging is important! Can you figure out what is wrong from the image or where to start <a id="index_term1039"/>debugging? When the incorrect stride is applied to the vertex array object, rendering goes awry.</p></figcaption>
</figure>
</section>
<section>
<h2 id="sec68"><span class="green">Notes</span></h2>
<p>There are many good resources available to learn more about the technical details involved with programming graphics hardware. A good starting point might be the OpenGL and GLSL specification documents. They are available for free online at the <a href="http://opengl.org">opengl.org</a> website. These documents will provide complete details for all the different and emerging versions of OpenGL.</p>
</section>
<section>
<h2 id="sec69"><span class="green">Exercises</span></h2>
<p>The sections of this chapter are roughly organized to step students through the process of creating a modern OpenGL application. Some extra effort will be required to understand the details relating to setting up windows and OpenGL contexts. However, it should be possible to following the sections for a set of weekly one hour labs:</p>
<p class="qpara"><span class="green">1.</span> <span style="color:green">Lab 1: Basic code setup for OpenGL applications.</span> This includes installing the necessary drivers and related software such as GLM and GLfW. Students can then write code to open a window and clear the color buffers.</p>
<p class="qpara"><span class="green">2.</span> <span style="color:green">Lab 2: Creating a shader.</span> Since a rudimentary shader is necessary to visualize the output in modern OpenGL, starting with efforts to create a very basic shader will go a long way. In this lab, or labs, students could build (or use provided) classes to load, compile, and link shaders into <a id="index_term1035"/>shader programs.</p>
<p class="qpara"><span class="green">3.</span> <span aria-label="502" epub:type="pagebreak" id="pg_502" role="doc-pagebreak"/><span style="color:green">Lab 3: Create a clip coordinate triangle and shade.</span> Using the shader classes from the previous lab, students will add the passthrough shader and create simple geometry to render.</p>
<p class="qpara"><span class="green">4.</span> <span style="color:green">Lab 4: Introduce GLM.</span> Start using GLM to generate projection matrices and viewing matrices for viewing more generalized, yet simple, scenes.</p>
<p class="qpara"><span class="green">5.</span> <span style="color:green">Lab 5: Use GLM for local transformations.</span> Students can expand their working shader program to use local transforms, perhaps applying animations based on changing transforms.</p>
<p class="qpara"><span class="green">6.</span> <span style="color:green">Lab 6: Shader development.</span> Develop the Lambertian or Blinn-Phong shaders.</p>
<p class="qpara"><span class="green">7.</span> <span style="color:green">Lab 7: Work with materials.</span> Students can explore additional material properties and rendering styles with different shader programs.</p>
<p class="qpara"><span class="green">8.</span> <span style="color:green">Lab 8: Load 3D models.</span> Using code to load OBJ files, students can further explore the capabilities of their graphics hardware including the limits of hardware processing for real-time applications.</p>
<p class="qpara"><span class="green">9.</span> <span style="color:green">Lab 9: Textures.</span> Using PNG (or other formats), students can load images onto the hardware and practice a variety of texture-mapping strategies.</p>
<p class="qpara1"><span class="green">10.</span> <span style="color:green">Lab 10: Integration with rendering code.</span> If scene files are used to describe scenes for the ray tracer (or rasterizer), students’ OpenGL code can be integrated into a complete rendering framework using common structures and classes to build a complete system.</p>
<p class="noindent1">This list is only a guide. In labs for my computer graphics course, students are provided material to get them started on the week’s idea. After they get the basic idea working, the lab is completed once they add their spin or a creative exploration of the idea to their code. As students get familiar with graphics hardware programming, they can explore additional areas of interest, such as textures, render to texture, or more advanced shaders and graphics algorithms.</p>
</section>
</section>
</body>
</html>