<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" dir="ltr" lang="en" xml:lang="en">
<head>
<meta charset="UTF-8"/>
<title>19 Visual Perception</title>
<link href="../styles/9781000426359.css" rel="stylesheet" type="text/css"/>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX","input/MathML","output/SVG"],
extensions: ["tex2jax.js","mml2jax.js","MathEvents.js"],
TeX: {
extensions: ["noErrors.js","noUndefined.js","autoload-all.js"]
},
MathMenu: {
showRenderer: false
},
menuSettings: {
zoom: "Click"
},
messageStyle: "none"
});
</script>
<script src="../mathjax/MathJax.js" type="text/javascript"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006665500" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter">
<section epub:type="chapter" role="doc-chapter">
<header>
<p class="chap-auz"><span class="green">William B. Thompson</span></p>
<h1 class="chapz1" id="c19"><a id="index_term1331"/><span aria-label="525" epub:type="pagebreak" id="pg_525" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rc19" role="doc-backlink"><span class="green"><span class="big1">19</span><br/>Visual Perception</span></a></h1>
</header>
<p class="noindent1">The ultimate purpose of computer graphics is to produce images for viewing by people. Thus, the success of a computer graphics system depends on how well it conveys relevant information to a human observer. The intrinsic complexity of the physical world and the limitations of display devices make it impossible to present a viewer with the identical patterns of light that would occur when looking at a natural environment. When the goal of a computer graphics system is physical realism, the best we can hope for is that the system be <em>perceptually effective</em>: displayed images should “look” as intended. For applications such as technical illustration, it is often desirable to visually highlight relevant information and perceptual effectiveness becomes an explicit requirement.</p>
<p>Artists and illustrators have developed empirically a broad range of tools and techniques for effectively conveying visual information. One approach to improving the perceptual effectiveness of computer graphics is to utilize these methods in our automated systems. A second approach builds directly on knowledge of the human vision system by using perceptual effectiveness as an optimization criterion in the design of computer graphics systems. These two approaches are not completely distinct. Indeed, one of the first systematic examinations of visual perception is found in the notebooks of Leonardo da Vinci.</p>
<p>The remainder of this chapter provides a partial overview of what is known about visual perception in people. The emphasis is on aspects of human vision that are most relevant to computer graphics. The human visual system is extremely complex in both its operation and its architecture. A chapter such as this <a id="term-500"/><a id="term-1046"/><a id="term-1076"/><span aria-label="526" epub:type="pagebreak" id="pg_526" role="doc-pagebreak"/>can at best provide a summary of key points, and it is important to avoid over generalizing from what is presented here. More in-depth treatments of visual perception can be found in Wandell (1995) and Palmer (1999); Gregory (1997) and Yantis (2000) provide additional useful information. A good computer vision reference such as Forsyth and Ponce (2002) is also helpful. It is important to note that despite over 150 years of intensive research, our knowledge of many aspects of vision is still very limited and imperfect.</p>
<section>
<h2 id="sec19_1"><a id="index_term1385"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec19_1" role="doc-backlink"><span class="green">19.1 Vision Science</span></a></h2>
<p>Vision is generally agreed to be the most powerful of the senses in humans. Vision produces more useful information about the world than does hearing, touch, smell, or taste. This is a direct consequence of the physics of light (<a href="C24_chapter19.xhtml#f19_1">Figure 19.1</a>). Illumination is pervasive, especially during the day but also at night due to moonlight, starlight, and artificial sources. Surfaces reflect a substantial portion of incident illumination and do so in ways that are idiosyncratic to particular materials and that are dependent on the shape of the surface. The fact that light (mostly) travels in straight lines through the air allows vision to acquire information from distant locations.</p>
<figure id="f19_1" tabindex="0">
<img alt="" src="../images/fig19_1.jpg"/>
<figcaption><p><span class="blue">Figure 19.1.</span> The <a id="index_term1363"/><a id="index_term674"/>nature of light makes vision a powerful sense.</p></figcaption>
</figure>
<p>The study of vision has a long and rich history. Much of what we know about the eye traces back to the work of philosophers and physicists in the 1600s. Starting in the mid-1800s, there was an explosion of work by perceptual psychologists exploring the phenomenology of vision and proposing models of how vision might work. The mid-1900s saw the start of modern neuroscience, which investigates both the fine-scale workings of individual neurons and the large-scale architectural organization of the brain and nervous system. A substantial portion of neuroscience research has focused on vision. More recently, computer science has contributed to the understanding of visual perception by providing tools for precisely describing hypothesized models of visual computations and by allowing empirical examination of computer vision programs. The term <em>vision science</em> was coined to refer to the multidisciplinary study of visual perception involving perceptual psychology, neuroscience, and computational analysis.</p>
<p>Vision science views the purpose of vision as producing information about objects, locations, and events in the world from imaged patterns of light reaching the viewer. Psychologists use the term <em>distal stimulus</em> to refer to the physical world under observation and <em>proximal stimulus</em> to refer to the retinal image. <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_1" id="rfn19_1" role="doc-noteref"><sup>1</sup></a> Using <span aria-label="527" epub:type="pagebreak" id="pg_527" role="doc-pagebreak"/>this terminology, the function of vision is to generate a description of aspects of the distal stimulus given the proximal stimulus. Visual perception is said to be <em>veridical</em> when the description that is produced accurately reflects the real world. In practice, it makes little sense to think of these descriptions of objects, locations, and events in isolation. Rather, vision is better understood in the context of the motor and cognitive functions that it serves.</p>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_1" id="fn19_1"><sup>1</sup></a> In computer vision, the term<em>scene</em> is often used to refer to the external world, while the term <em>image</em> is used to refer to the projection of the scene onto a sensing plane.</p></aside>
</section>
<section>
<h2 id="sec19_2"><a id="index_term1387"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec19_2" role="doc-backlink"><span class="green">19.2 Visual Sensitivity</span></a></h2>
<p>Vision systems create descriptions of the visual environment based on properties of the incident illumination. As a result, it is important to understand what properties of incident illumination the human vision system can actually detect. One critical observation about the human vision system is that it is primarily sensitive to <em>patterns</em> of light rather than being sensitive to the absolute magnitude of light energy. The eye does not operate as a photometer. Instead, it detects spatial, temporal, and spectral patterns in the light imaged on the retina and information about these patterns of light form the basis for all of visual perception.</p>
<p>There is a clear ecological utility to the vision system’s sensitivity to variations in illumination over space and time. Being able to accurately sense changes in the environment is crucial to our survival. <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_2" id="rfn19_2" role="doc-noteref"><sup>2</sup></a> A system which measures changes in light energy rather than the magnitude of the energy itself also makes engineering sense, since it makes it easier to detect patterns of light over large ranges in light intensity. It is a good thing for computer graphics that vision operates in this manner. Display devices are physically limited in their ability to project light with the power and dynamic range typical of natural scenes. Graphical displays would not be effective if they needed to produce the identical patterns of light as the corresponding physical world. Fortunately, all that is required is that displays be able to produce similar patterns of spatial and temporal change to the real world.</p>
<section>
<h3 id="sec19_2_1"><a id="index_term1336"/><span class="green">19.2.1 Brightness and Contrast</span></h3>
<p>In bright light, the human visual system is capable of distinguishing gratings consisting of high-contrast parallel light and dark bars as fine as 50–60 cycles/degree. (In this case, a “cycle” consists of an adjacent pair of light and dark bars.) For comparison, the best currently available LCD computer monitor, at a normal viewing distance, can display patterns as fine as about 20 cycles/degree. The minimum contrast difference at an edge detectable by the human visual system in bright light is about 1% of the average luminance across the edge. In most 8-bit displays, differences of a single gray level are often noticeable over at least a portion of the range of intensities due to the nature of the mapping from gray levels to actual display luminance.</p>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_2" id="fn19_2"><sup>2</sup></a> It is sometime said that the primary goals of vision are to support eating, avoiding being eaten, reproduction, and avoidance of catastrophe while moving. Thinking about vision as a goal-directed activity is often useful, but needs to be done so at a more detailed level.</p></aside>
<p class="indent"><a id="term-999"/><a id="term-1011"/><span aria-label="528" epub:type="pagebreak" id="pg_528" role="doc-pagebreak"/>Characterizing the ability of the visual system to detect fine scale patterns (<em><a id="index_term1324"/>visual acuity</em>) and to detect changes in brightness is considerably more complicated than for cameras and similar image acquisition devices. As shown in <a href="C24_chapter19.xhtml#f19_2">Figure 19.2</a>, there is an interaction between contrast and acuity in human vision. In the figure, the scale of the pattern decreases from left to right while the contrast increases from top to bottom. If you view the figure at a normal viewing distance, it will be clear that the lowest contrast at which a pattern is visible is a function of the spatial frequency of the pattern.</p>
<figure id="f19_2" tabindex="0">
<img alt="" src="../images/fig19_2.jpg"/>
<figcaption><p><span class="blue">Figure 19.2.</span> The contrast between stripes increases in a constant manner from top to bottom, yet the threshold of visibility varies with frequency.</p></figcaption>
</figure>
<p>There is a linear relationship between the intensity of light <em>L</em> reaching the eye from a particular surface point in the world, the intensity of light <em>I</em> illuminating that surface point, and the reflectivity <em>R</em> of the surface at the point being observed:</p>
<div class="disp-formula" id="equ19_1">
<m:math xmlns:mml="http://www.w3.org/1998/Math/MathML" alttext=""><m:mrow><m:mi>L</m:mi><m:mo>=</m:mo><m:mi>α</m:mi><m:mi>I</m:mi><m:mo>⋅</m:mo><m:mi>R</m:mi><m:mo>,</m:mo></m:mrow><m:mspace width="3em"/><m:mo>(19.1)</m:mo></m:math>
</div>
<p><a id="term-499"/><a id="term-1012"/><a id="term-1038"/><a id="term-1039"/><span aria-label="529" epub:type="pagebreak" id="pg_529" role="doc-pagebreak"/>where <em>α</em> is dependent on the relationship between the surface geometry, the pattern of incident illumination, and the viewing direction. While the eye is only able to directly measure <em>L</em>, human vision is much better at estimating <em>R</em> than <em>L</em>. To see this, view <a href="C24_chapter19.xhtml#f19_3">Figure 19.3</a> in bright direct light. Use your hand to shadow one of the patterns, leaving the other directly illuminated. While the light reflected off of the two patterns will be significantly different, the apparent brightness of the two center squares will seem nearly the same. The term <em><a id="index_term1355"/>lightness</em> is often used to describe the apparent brightness of a surface, as distinct from its actual luminance. In many situations, lightness is invariant to large changes in illumination, a phenomenon referred to as <em>lightness constancy</em>.</p>
<figure id="f19_3" tabindex="0">
<img alt="" src="../images/fig19_3.jpg"/>
<figcaption><p><span class="blue">Figure 19.3.</span> <em>Lightness constancy</em>. Cast a shadow over one of the patterns with your hand and notice that the apparent brightness of the two center squares remains nearly the same.</p></figcaption>
</figure>
<p>The mechanisms by which the human visual system achieves lightness constancy are not well understood. As shown in <a href="C24_chapter19.xhtml#f19_2">Figure 19.2</a>, the vision system is relatively insensitive to slowly varying patterns of light, which may serve to discount the effects of slowly varying illumination. Apparent brightness is affected by the brightness of surrounding regions (<a href="C24_chapter19.xhtml#f19_4">Figure 19.4</a>). This can aid <a id="index_term1356"/>lightness constancy when regions are illuminated dissimilarly. While this <em><a id="index_term1376"/>simultaneous contrast</em> effect is often described as a modification of the perceived lightness of one region based on contrasting brightness in the surrounding region, it is actually much more complicated than that (<a href="C24_chapter19.xhtml#f19_5">Figures 19.5</a> and <a href="C24_chapter19.xhtml#f19_6">19.6</a>). For more on lightness perception, see (Gilchrist et al., 1999) and (Adelson, 1999).</p>
<figure id="f19_4" tabindex="0">
<img alt="" src="../images/fig19_4.jpg"/>
<figcaption><p><span class="blue">Figure 19.4.</span> (a) Simultaneous contrast: the apparent brightness of the center bar is affected by the brightness of the surrounding area; (b) The same bar without a variable surround.</p></figcaption>
</figure>
<figure id="f19_5" tabindex="0">
<img alt="" src="../images/fig19_5.jpg"/>
<figcaption><p><span class="blue">Figure 19.5.</span> <a id="term-391"/><a id="term-415"/><a id="term-416"/><a id="term-1027"/><a id="term-1028"/><a id="term-1037"/><a id="term-1064"/><span aria-label="530" epub:type="pagebreak" id="pg_530" role="doc-pagebreak"/>The Munker-White illusion shows the complexity of simultaneous contrast. In Figure19.4, the central region looked lighter when the surrounding area was darker. In (a), the gray strips on the left look <em>lighter</em> than the gray strips on the right, even though they are nearly surrounded by regions of white; (b) shows the gray strips without the black lines.</p></figcaption>
</figure>
<figure id="f19_6" tabindex="0">
<img alt="" src="../images/fig19_6.jpg"/>
<figcaption><p><span class="blue">Figure 19.6.</span> The perception of lightness is affected by the perception of 3D structure. The two surfaces marked (a) have the same brightness, as do the two surfaces marked (b) (after Adelson (1999)).</p></figcaption>
</figure>
<p>While the visual system largely ignores slowly varying intensity patterns, it is extremely sensitive to <em><a id="index_term1341"/>edges</em> consisting of lines of discontinuity in brightness. Edges in imaged light intensity often correspond to surface boundaries or other important features in the environment (<a href="C24_chapter19.xhtml#f19_7">Figure 19.7</a>). The vision system can also detect localized differences in motion, stereo disparity, texture, and several other image properties. The vision system has very little ability, however, to detect spatial discontinuities in color when not accompanied by differences in one of these other properties.</p>
<figure id="f19_7" tabindex="0">
<img alt="" src="../images/fig19_7.jpg"/>
<figcaption><p><span class="blue">Figure 19.7.</span> (a) Original <a id="index_term1348"/>gray scale image, (b) image <em>edges</em>, which are lines of high spatial variability in some direction.</p></figcaption>
</figure>
<p><a id="term-1018"/><a id="term-1068"/><a id="term-1069"/><span aria-label="531" epub:type="pagebreak" id="pg_531" role="doc-pagebreak"/>Perception of edges seems to interact with perception of form. While edges give the visual system the information it needs to recognize shapes, slowly varying brightness can appear as a sharp edge if the resulting edge creates a more complete form (<a href="C24_chapter19.xhtml#f19_8">Figure 19.8</a>). <a href="C24_chapter19.xhtml#f19_9">Figure 19.9</a> shows a <em>subjective contour</em>, an extreme form of this effect in which a closed contour is seen even though no such contour exists in the actual image. Finally, the vision system’s sensitivity to edges also appears to be part of the mechanism involved in lightness perception. Note that the region enclosed by the subjective contour in <a href="C24_chapter19.xhtml#f19_9">Figure 19.9</a> appears a bit brighter than the surrounding area of the page. <a href="C24_chapter19.xhtml#f19_10">Figure 19.10</a> shows a different interaction between edges and lightness. In this case, a particular brightness profile at the edge has a dramatic effect on the apparent brightness of the surfaces to either side of the edge.</p>
<figure id="f19_8" tabindex="0">
<img alt="" src="../images/fig19_8.jpg"/>
<figcaption><p><span class="blue">Figure 19.8.</span> The visual system sometimes sees “edges” even when there are no sharp discontinuities in brightness, as is the case at the right side of the central pattern in this image.</p></figcaption>
</figure>
<figure id="f19_9" tabindex="0">
<img alt="" src="../images/fig19_9.jpg"/>
<figcaption><p><span class="blue">Figure 19.9.</span> Sometimes, the visual system will “see” <em><a id="index_term1380"/>subjective contours</em> without any associated change in brightness.</p></figcaption>
</figure>
<figure id="f19_10" tabindex="0">
<img alt="" src="../images/fig19_10.jpg"/>
<figcaption><p><span class="blue">Figure 19.10.</span> <a id="term-102"/><a id="term-1013"/><span aria-label="532" epub:type="pagebreak" id="pg_532" role="doc-pagebreak"/>Perceived lightness depends more on local contrast at edges than on brightness across surfaces. Try covering the vertical edge in the middle of the figure with a pencil. This figure is an instance of the <em>Craik-O’Brien-Cornsweet illusion</em>.</p></figcaption>
</figure>
<p>As indicated above, people can detect differences in the brightness between two adjacent regions if the difference is at least 1% of the average brightness. This is an example of <em>Weber’s law</em>, which states that there is a constant ratio between the <em>just noticeable differences</em> (jnd) in a stimulus and the magnitude of the stimulus:</p>
<div class="disp-formula" id="equ19_2">
<m:math alttext=""><m:mrow><m:mfrac><m:mrow><m:mi mathvariant="normal">Δ</m:mi><m:mi>I</m:mi></m:mrow><m:mrow><m:mi>I</m:mi></m:mrow></m:mfrac><m:mo>=</m:mo><m:msub><m:mrow><m:mi>k</m:mi></m:mrow><m:mrow><m:mn>1</m:mn></m:mrow></m:msub><m:mo>,</m:mo></m:mrow><m:mspace width="3em"/><m:mo>(19.2)</m:mo></m:math>
</div>
<p>where <em>I</em> is the magnitude of the stimulus, Δ<em>I</em> is the magnitude of the just noticeable difference, and <em>k</em><sub>1</sub> is a constant particular to the stimulus. Weber’s law was postulated in 1846 and still remains a useful characterization of many perceptual effects. <em><a id="index_term357"/>Fechner’s law</em>, proposed in 1860, generalized <a id="index_term1462"/>Weber’s law in a way that allowed for the description of the strength of any sensory experience, not just jnd’s:</p>
<div class="disp-formula" id="equ19_3">
<m:math alttext=""><m:mrow><m:mi>S</m:mi><m:mo>=</m:mo><m:msub><m:mrow><m:mi>k</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msub><m:mtext>log</m:mtext><m:mo>(</m:mo><m:mi>I</m:mi><m:mo>)</m:mo><m:mo>,</m:mo></m:mrow><m:mspace width="3em"/><m:mo>(19.3)</m:mo></m:math>
</div>
<p>where <em>S</em> is the perceptual strength of the sensory experience, <em>I</em> is the physical magnitude of the corresponding stimulus, and <em>k</em><sub>2</sub> is a scaling constant specific to the stimulus. Current practice is to model the association between perceived and actual strength of a stimulus using a power function (<em>Stevens’s law</em>):</p>
<div class="disp-formula" id="equ19_4">
<m:math alttext=""><m:mrow><m:mi>S</m:mi><m:mo>=</m:mo><m:msub><m:mrow><m:mi>k</m:mi></m:mrow><m:mrow><m:mn>3</m:mn></m:mrow></m:msub><m:msup><m:mrow><m:mi>I</m:mi></m:mrow><m:mrow><m:mi>b</m:mi></m:mrow></m:msup><m:mo>,</m:mo></m:mrow><m:mspace width="3em"/><m:mo>(19.4)</m:mo></m:math>
</div>
<p>where <em>S</em> and <em>I</em> are as before, <em>k</em><sub>3</sub> is another scaling constant, and <em>b</em> is an exponent specific to the stimulus. For a large number of perceptual quantities involving vision, <em>b &lt;</em> 1. The CIE <em>L</em>*<em>a</em>*<em>b</em>* color space, described elsewhere, uses a modified Stevens’s law representation to characterize perceptual differences between brightness values. Note that in the first two characterizations of the perceptual strength of a stimulus and in <a id="index_term1131"/>Stevens’s Law when <em>b &lt;</em> 1, changes in the stimulus when it has a small average magnitude create larger perceptual effects than do the same physical change in the stimulus when it has a larger magnitude.</p>
<p><a id="term-119"/><a id="term-126"/><a id="term-139"/><a id="term-140"/><a id="term-820"/><span aria-label="533" epub:type="pagebreak" id="pg_533" role="doc-pagebreak"/>The “laws” described above are not physical constraints on how perception operates. Rather, they are generalizations about how the perceptual system responds to particular physical stimuli. In the field of perceptual psychology, the quantitative study of the relationships between physical stimuli and their perceptual effects is called <em><a id="index_term1373"/>psychophysics</em>. While <a id="index_term911"/>psychophysical laws are empirically derived observations rather than mechanistic accounts, the fact that so many perceptual effects are well modeled by simple power functions is striking and may provide insights into the mechanisms involved.</p>
</section>
<section>
<h3 id="sec19_2_2"><a id="index_term1337"/><span class="green">19.2.2 Color</span></h3>
<p>In 1666, Isaac Newton used <a id="index_term897"/>prisms to show that apparently white sunlight could be decomposed into a <em><a id="index_term177"/>spectrum</em> of colors and that these colors could be recombined to produce light that appeared white. We now know that light energy is made up of a collection of photons, each with a particular wavelength. The <em>spectral distribution</em> of light is a measure of the average energy of the light at each wavelength. For natural illumination, the spectral distribution of light reflected off of surfaces varies significantly depending on the surface material. Characterizations of this spectral distribution can therefore provide visual information for the nature of surfaces in the environment.</p>
<p>Most people have a pervasive sense of color when they view the world. Color perception depends on the frequency distribution of light, with the visible spectrum for humans ranging from a wavelength of about 370 nm to a wavelength of about 730 nm (see <a href="C24_chapter19.xhtml#f19_11">Figure 19.11</a>). The manner in which the visual systems derives a sense of color from this spectral distribution was first systematically examined in 1801 and remained extremely controversial for 150 years. The problem is that the visual system responds to patterns of <a id="index_term1110"/>spectral distribution very differently than patterns of luminance distribution.</p>
<figure id="f19_11" tabindex="0">
<img alt="" src="../images/fig19_11.jpg"/>
<figcaption><p><span class="blue">Figure 19.11.</span> The visible spectrum. Wavelengths are in nanometers.</p></figcaption>
</figure>
<p>Even accounting for phenomena such as lightness constancy, distinctly different spatial distributions almost always look distinctly different. More importantly given that the purpose of the visual system is to produce descriptions of the distal stimulus given the proximal stimulus, perceived patterns of lightness correspond at least approximately to patterns of brightness over surfaces in the environment. <a id="term-117"/><a id="term-118"/><span aria-label="534" epub:type="pagebreak" id="pg_534" role="doc-pagebreak"/>The same is not true of color perception. Many quite different spectral distributions of light can produce a sense of any specific color. Correspondingly, the sense that a surface is a specific color provides little direct information about the spectral distribution of light coming from the surface. For example, a spectral distribution consisting of a combination of light at wavelengths of 700 nm and 540 nm, with appropriately chosen relative strengths, will look indistinguishable from light at the single wavelength of 580 nm. (Perceptually indistinguishable colors with different spectral compositions are referred to as <em>metamers</em>.) If we see the color “yellow,” we have no way of knowing if it was generated by one or the other of these distributions or an infinite family of other spectral distributions. For this reason, in the context of vision the term <em>color</em> refers to a purely perceptual quality, not a physical property.</p>
<aside class="boxed-text" epub:type="sidebar">
<p class="noindent">
<em>“The history of the investigation of colour vision is remarkable for its acrimony.”</em>
</p>
<p class="noindent">—Richard Gregory (1997)</p>
</aside>
<p class="indent">There are two classes of photoreceptors in the human retina. <em>Cones</em> are involved in color perception, while <em>rods</em> are sensitive to light energy across the visible range and do not provide information about color. There are three types of cones, each with a different spectral sensitivity (<a href="C24_chapter19.xhtml#f19_12">Figure 19.12</a>). <em>S-cones</em> respond to short wavelengths in the blue range of the visible spectrum. <em>M-cones</em> respond to wavelengths in the middle (greenish) region of the visible spectrum. <em>L-cones</em> respond to somewhat longer wavelengths covering the green and red portions of the visible spectrum.</p>
<figure id="f19_12" tabindex="0">
<img alt="" src="../images/fig19_12.jpg"/>
<figcaption><p><span class="blue">Figure 19.12.</span> Spectral sensitivity of the short, medium, and long cones in the human retina.</p></figcaption>
</figure>
<p>While it is common to describe the three types of cones as <em>red</em>, <em>green</em>, and <em>blue</em>, this is neither correct terminology nor does it accurately reflect the cone sensitivities shown in <a href="C24_chapter19.xhtml#f19_12">Figure 19.12</a>. The <em>L-cones</em> and <em>M-cones</em> are broadly tuned, meaning that they respond to a wide range of frequencies. There is also substantial overlap between the sensitivity curves of the three cone types. Taken together, these two properties mean that it is not possible to reconstruct an approximation <a id="term-135"/><span aria-label="535" epub:type="pagebreak" id="pg_535" role="doc-pagebreak"/>to the original spectral distribution given the responses of the three cone types. This is in contrast to spatial sampling in the retina (and in digital cameras), where the receptors are narrowly tuned in their spatial sensitivity in order to be able to detect fine detail in local contrast.</p>
<p>The fact that there are are only three types of color sensitive photoreceptors in the human retina greatly simplifies the task of displaying colors on computer monitors and in other graphical displays. Computer monitors display colors as a weighted combination of three fixed-color distributions. Most often, the three colors are a distinct red, a distinct green, and a distinct blue. As a result, in computer graphics, color is often represented by a <em>red-green-blue</em> (RGB) triple, representing the intensities of red, green, and blue primaries needed to display a particular color. Three <em>basis colors</em> are sufficient to display most perceptible colors, since appropriately weighted combinations of three appropriately chosen colors can produce metamers for these perceptible colors.</p>
<p>There are at least two significant problems with the <a id="index_term172"/><a id="index_term997"/>RGB color representation. The first is that different monitors have different spectral distributions for their red, green, and blue primaries. As a result, perceptually correct color rendition involves remapping RGB values for each monitor. This is, of course, only possible if the original RGB values satisfy some well-defined standard, which is often not the case. (See <a href="C23_chapter18.xhtml#c18">Chapter 18</a> for more information on this issue.) The second problem is that RGB values do not define a particular color in a way that corresponds to subjective perception. When we see the color “yellow,” we do not have the sense that it is made up of equal parts of red and green light. Rather, it looks like a single color, with additional properties involving brightness and the “amount” of color. Representing color as the output of the S-cones, M-cones, and L-cones is no help either, since we have no more phenomenological sense of color as characterized by these properties than we do as characterized by RGB display properties.</p>
<p>There are two different approaches to characterizing color in a way that more closely reflects human perception. The various CIE <a id="index_term183"/>color spaces aim to to be “perceptually uniform” so that the magnitude of the difference in the represented values of two colors is proportional to the perceived difference in color (Wyszecki &amp; Stiles, 2000). This turns out to be a difficult goal to accomplish, and there have been several modifications to the CIE model over the years. Furthermore, while one of the dimensions of the <a id="index_term141"/>CIE color spaces corresponds to perceived brightness, the other two dimensions that specify chromaticity have no intuitive meaning.</p>
<p>The second approach to characterizing color in a more natural manner starts with the observation that there are three distinct and independent properties that dominate the subjective sense of color. <em>Lightness</em>, the apparent brightness of a surface, has already been discussed. <em>Saturation</em> refers to the purity or vividness of a color. Colors can range from totally unsaturated gray to partially saturated pastels to fully saturated “pure” colors. The third property, <em>hue</em>, corresponds most closely to the informal sense of the word “color” and is characterized in a manner similar to colors in the visible spectrum, ranging from dark violet to dark red. <a href="C24_chapter19.xhtml#f19_13">Figure 19.13</a> shows a plot of the hue-saturation-lightness (HSV) color space. Since the relationship between brightness and lightness is both complex and not well understood, HSV color spaces almost always use brightness instead of attempting to estimate lightness. Unlike wavelengths in the spectrum, however, hue is usually represented in a manner that reflects the fact that the extremes of the visible spectrum are actually similar in appearance (<a href="C24_chapter19.xhtml#f19_14">Figure 19.14</a>). Simple transformations exist between RGB and HSV representations of a particular color value. As a result, while the HSV color space is motivated by perceptual considerations, it contains no more information than does an RGB representation.</p>
<figure id="f19_13" tabindex="0">
<img alt="" src="../images/fig19_13.jpg"/>
<figcaption><p><span class="blue">Figure 19.13.</span> <a id="term-113"/><a id="term-136"/><a id="term-401"/><a id="term-402"/><span aria-label="536" epub:type="pagebreak" id="pg_536" role="doc-pagebreak"/>HSV color space. Hue varies around the circle, <a id="index_term175"/>saturation varies with radius, and value varies with height.</p></figcaption>
</figure>
<figure id="f19_14" tabindex="0">
<img alt="" src="../images/fig19_14.jpg"/>
<figcaption><p><span class="blue">Figure 19.14.</span> Which color is closer to red: green or violet?</p></figcaption>
</figure>
<p><a id="term-821"/><a id="term-1036"/><span aria-label="537" epub:type="pagebreak" id="pg_537" role="doc-pagebreak"/>The hue-saturation-lightness approach to describing color is based on the spectral distribution at a single point and so only approximates the perceptual response to spectral distributions of light distributed over space. Color perception is subject to similar constancy and simultaneous contrast effects as is light-ness/brightness, neither of which are captured in the RGB representation and as a result are not captured in the HSV representation. For an example of color constancy, look at a piece of white paper indoors under incandescent light and outdoors under direct sunlight. The paper will look “white” in both cases, even though incandescent light has a distinctly yellow hue and so the light reflected off of the paper will also have a yellow hue, while sunlight has a much more uniform <a id="index_term186"/>color spectrum.</p>
<p>Another aspect of color perception not captured by either the CIE color spaces or HSV encoding is the fact that we see a small number of distinct colors when looking at a continuous spectrum of visible light (<a href="C24_chapter19.xhtml#f19_11">Figure 19.11</a>) or in a naturally occurring rainbow. For most people, the visible <a id="index_term1116"/>spectrum appears to be divided into four to six distinct colors: red, yellow, green, and blue, plus perhaps light blue and purple. Considering non-spectral colors as well, there are only 11 basic color terms commonly used in English: <em>red</em>, <em>green</em>, <em>blue</em>, <em>yellow</em>, <em>black</em>, <em>white</em>, <em>gray</em>, <em>orange</em>, <em>purple</em>, <em>brown</em>, and <em>pink</em>. The partitioning of the intrinsically continuous space of spectral distributions into a relatively small set of perceptual categories associated with well-defined linguistic terms seems to be a basic property of perception, not just a cultural artifact (Berlin &amp; Kay, 1969). The exact nature of the process, however, is not well understood.</p>
</section>
<section>
<h3 id="sec19_2_3"><span class="green">19.2.3 Dynamic Range</span></h3>
<p>Natural illumination varies in intensity over 6 orders of magnitude (<a href="C24_chapter19.xhtml#f19_15">Figure 19.15</a>). The human vision system is able to operate over this full range of brightness levels. However, at any one point in time, the visual system is only able to detect variations in light intensity over a much smaller range. As the average brightness to which the visual system is exposed changes over time, the range of discriminable brightnesses changes in a corresponding manner. This effect is most obvious if we move rapidly from a brightly lit outdoor area to a very dark room. At first, we are able to see little. After a while, however, details in the room start to become apparent. The <em>dark <a id="index_term671"/>adaptation</em> that occurs involves a number of physiological changes in the eye. It takes several minutes for significant dark adaptation to occur and 40 minutes or so for complete dark adaptation. If we then move back into the bright light, not only is vision difficult but it can actually be painful. <em>Light adaptation</em> is required before it is again possible to see clearly. Light adaptation <a id="term-1000"/><a id="term-1020"/><a id="term-1021"/><span aria-label="538" epub:type="pagebreak" id="pg_538" role="doc-pagebreak"/>occurs much more quickly than dark adaptation, typically requiring less than a minute.</p>
<figure id="f19_15" tabindex="0">
<img alt="" src="../images/fig19_15.jpg"/>
<figcaption><p><span class="blue">Figure 19.15.</span> Approximate luminance level of a white surface under different types of illumination in candelas per meter squared (cd/m<sup>2</sup>). (Wandell, 1995).</p></figcaption>
</figure>
<p>The two classes of photoreceptors in the human retina are sensitive to different ranges of brightness. The cones provide visual information over most of what we consider normal lighting conditions, ranging from bright sunlight to dim indoor lighting. The rods are only effective at very low light levels. <em>Photopic</em> vision involves bright light in which only the cones are effective. <em>Scotopic</em> vision involves dark light in which only the rods are effective. There is a range of intensities within which both cones and rods are sensitive to changes in light, which is referred to as <em>mesopic</em> conditions (see <a href="C26_chapter21.xhtml#c21">Chapter 21</a>).</p>
</section>
<section>
<h3 id="sec19_2_4"><a id="index_term359"/><a id="index_term1343"/><span class="green">19.2.4 Field-of-View and Acuity</span></h3>
<p>Each eye in the human visual system has a field-of-view of approximately 160° horizontal by 135° vertical. With binocular viewing, there is only partial overlap between the fields-of-view of the two eyes. This results in a wider overall field-of-view (approximately 200° horizontal by 135° vertical), with the region of overlap being approximately 120° horizontal by 135° vertical.</p>
<p>With normal or corrected-to-normal vision, we usually have the subjective experience of being able to see relatively fine detail wherever we look. This is an illusion, however. Only a small portion of the visual field of each eye is actually sensitive to fine detail. To see this, hold a piece of paper covered with normalsized text at arm’s length, as shown in <a href="C24_chapter19.xhtml#f19_16">Figure 19.16</a>. Cover one eye with the hand not holding the paper. While staring at your thumb and not moving your eye, note that the text immediately above your thumb is readable while the text to either side is not. High acuity vision is limited to a visual angle slightly larger than your thumb held at arm’s length. We do not normally notice this because the eyes usually move frequently, allowing different regions of the visual field to be viewed at high resolution. The visual system then integrates this information over time to produce the subjective experience of the whole visual field being seen at high resolution.</p>
<figure id="f19_16" tabindex="0">
<img alt="" src="../images/fig19_16.jpg"/>
<figcaption><p><span class="blue">Figure 19.16.</span> If you hold a page of text at arm’s length and stare at your thumb, only the text near your thumb will be readable. <em>Photo by Peter Shirley.</em></p></figcaption>
</figure>
<p><span aria-label="539" epub:type="pagebreak" id="pg_539" role="doc-pagebreak"/>There is not enough bandwidth in the human visual cortex to process the information that would result if there was a dense sampling of image intensity over the whole of the retina. The combination of variable density photoreceptor packing in the retina and a mechanism for rapid eye movements to point at areas of interest provides a way to simultaneously optimize acuity and field-of-view. Other animals have evolved different ways of balancing acuity and field-of-view that are not dependent on rapid eye movements. Some have only high acuity vision, but limited to a narrow field-of-view. Others have wide field-of-view vision, but limited ability to see detail.</p>
<p>The eye motions which focus areas of interest in the environment on the fovea are called <em>saccades</em>. Saccades occur very quickly. The time from a triggering stimulus to the completion of the eye movement is 150–200 ms. Most of this time is spent in the vision system planning the saccade. The actual motion takes 20 ms or so on average. The eyes are moving very quickly during a saccade, with the maximum rotational velocity often exceeding 500°/second. Between saccades, the eyes point toward an area of interest (<em>fixate</em>), taking 300 ms or so to acquire fine detail visual information. The mechanism by which multiple fixations are integrated to form an overall subjective sense of fine detail over a wide field of view is not well understood.</p>
<p><a href="C24_chapter19.xhtml#f19_17">Figure 19.17</a> shows the variable packing density of cones and rods in the human retina. The cones, which are responsible for vision under normal lighting, are packed most closely at the <em>fovea</em> of the retina (<a href="C24_chapter19.xhtml#f19_17">Figure 19.17</a>). When the eye is fixated at a particular point in the environment, the image of that point falls on the fovea. The higher packing density of cones at the fovea results in a higher sampling frequency of the imaged light (see <a href="C14_chapter9.xhtml#c9">Chapter 9</a>) and hence greater detail in the sampled pattern. Foveal vision encompasses about 1.7°, which is the same visual angle as the width of your thumb held at arm’s length.</p>
<figure id="f19_17" tabindex="0">
<img alt="" src="../images/fig19_17.jpg"/>
<figcaption><p><span class="blue">Figure 19.17.</span> Density of rods and cone in the human retina (after Osterberg (1935)).</p></figcaption>
</figure>
<p>While a version of <a href="C24_chapter19.xhtml#f19_17">Figure 19.17</a> appears in most introductory texts on human visual perception, it provides only a partial explanation for the neurophysiological limitations on <a id="index_term1325"/>visual acuity. The output of individual rods and cones is pooled in various ways by neural interconnects in the eye, before the information is shipped along the optic nerve to the visual cortex. <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_3" id="rfn19_3" role="doc-noteref"><sup>3</sup></a> This pooling filters the signal provided by the pattern of incident illumination in ways that have important impacts on the patterns of light that are detectable. In particular, the farther away from the fovea, the larger the area over which brightness is averaged. As a consequence, spatial acuity drops sharply away from the fovea. Most figures showing rod and cone packing density indicate the location of the retinal <em>blind spot</em>, where the nerve bundle carrying optical information from the eye to the brain passes through the retina, and there is no sensitivity to light. By and large, the only practical impact of the blind spot on real-world perception is its use as an illusion in introductory perception texts, since normal eye movements otherwise compensate for the temporary loss of information.</p>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_3" id="fn19_3"><sup>3</sup></a> All of the cells in the optic nerve and almost all cells in the visual cortex have an associated retinal <em>receptive field</em>. Patterns of light hitting the retina outside of a cell’s receptive field have no effect on the firing rate of that cell.</p></aside>
<p class="indent"><a id="term-54"/><a id="term-281"/><a id="term-1010"/><a id="term-1022"/><span aria-label="540" epub:type="pagebreak" id="pg_540" role="doc-pagebreak"/>As shown in <a href="C24_chapter19.xhtml#f19_17">Figure 19.17</a>, the packing density of rods drops to zero at the center of the fovea. Away from the fovea, the rod density first increases and then decreases. One result of this is that there is no foveal vision when illumination is very low. The lack of rods in the fovea can be demonstrated by observing a night sky on a moonless night, well away from any city lights. Some stars will be so dim that they will be visible if you look at a point in the sky slightly to the side of the star, but they will disappear if you look directly at them. This occurs because when you look directly at these features, the image of the features falls only on the cones in the retina, which are not sufficiently light sensitive to detect the feature. Looking slightly to the side causes the image to fall on the more light-sensitive cones. Scotopic vision is also limited in acuity, in part because <a id="term-1006"/><span aria-label="541" epub:type="pagebreak" id="pg_541" role="doc-pagebreak"/>of the lower density of rods over much of the retina and in part because greater pooling of signals from the rods occurs in the retina in order to increase the light sensitivity of the visual information passed back to the brain.</p>
</section>
<section>
<h3 id="sec19_2_5"><a id="index_term1358"/><span class="green">19.2.5 Motion</span></h3>
<p>When reading about <a id="index_term758"/>visual perception and looking at static figures on a printed page, it is easy to forget that motion is pervasive in our visual experience. The patterns of light that fall on the retina are constantly changing due to eye and body motion and the movement of objects in the world. This section covers our ability to detect visual motion. <a href="C24_chapter19.xhtml#sec19_3_4">Section 19.3.4</a> describes how visual motion can be used to determine geometric information about the environment. <a href="C24_chapter19.xhtml#sec19_4_3">Section 19.4.3</a> deals with the use of motion to guide our movement through the environment.</p>
<p>The detectability of motion in a particular pattern of light falling on the retina is a complex function of speed, direction, pattern size, and contrast. The issue is further complicated because simultaneous contrast effects occur for motion perception in a manner similar to that observed in brightness perception. In the extreme case of a single small pattern moving against a contrasting, homogenous background, perceivable motion requires a rate of motion corresponding to 0.2°–0.3°/second of visual angle. Motion of the same pattern moving against a textured pattern is detectable at about a tenth this speed.</p>
<p>With this sensitivity to retinal motion, combined with the frequency and velocity of saccadic eye movements, it is surprising that the world usually appears stable and stationary when we view it. The vision system accomplishes this in three ways. Contrast sensitivity is reduced during saccades, reducing the visual effects generated by these rapid changes in eye position. Between saccades, a variety of sophisticated and complex mechanisms adjust eye position to compensate for head and body motion and the motion of objects of interest in the world. Finally, the visual system exploits information about the position of the eyes to assemble a mosaic of small patches of high-resolution imagery from multiple fixations into a single, stable whole.</p>
<p>The motion of straight lines and edges is ambiguous if no endpoints or corners are visible, a phenomenon referred to as the <em><a id="index_term1333"/>aperture problem</em> (<a href="C24_chapter19.xhtml#f19_18">Figure 19.18</a>). The aperture problem arises because the component of motion parallel to the line or edge does not produce any visual changes. The geometry of the real world is sufficiently complex that this rarely causes difficulties in practice, except for intentional illusions such as barber poles. The simplified geometry and texturing found in some computer graphics renderings, however, has the potential to introduce inaccuracies in perceived motion.</p>
<figure id="f19_18" tabindex="0">
<img alt="" src="../images/fig19_18.jpg"/>
<figcaption><p><span class="blue">Figure 19.18.</span> <a id="term-25"/><a id="term-1007"/><a id="term-1023"/><span aria-label="542" epub:type="pagebreak" id="pg_542" role="doc-pagebreak"/>The aperture problem: (a) If a straight line or edge moves in such a way that its endpoints are hidden, the visual information is not sufficient to determine the actual motion of the line. (b) 2D motion of a line is unambiguous if there are any corners or other distinctive markings on the line.</p></figcaption>
</figure>
<p>Real-time computer graphics, film, and video would not be possible without an important perceptual phenomena: discontinuous motion, in which a series of static images are visible for discrete intervals in time and then move by discrete intervals in space, can be nearly indistinguishable from continuous motion. The effect is called <em>apparent motion</em> to highlight that the appearance of continuous motion is an illusion.</p>
<p><a href="C24_chapter19.xhtml#f19_19">Figure 19.19</a> illustrates the difference between continuous motion, which is typical of the real world, and apparent motion, which is generated by almost all dynamic image display devices. The motion plotted in <a href="C24_chapter19.xhtml#f19_19">Figure 19.19</a> (b) consists of an average motion comparable to that shown in <a href="C24_chapter19.xhtml#f19_19">Figure 19.19</a> (a), modulated by a high space-time frequency that accounts for the alternation between a stationary pattern and one that moves discontinuously to a new location. Apparent perception of continuous motion occurs because the visual system is insensitive to the high-frequency component of the motion.</p>
<figure id="f19_19" tabindex="0">
<img alt="" src="../images/fig19_19.jpg"/>
<figcaption><p><span class="blue">Figure 19.19.</span> (a) Continuous motion. (b) Discontinuous motion with the same average velocity. Under some circumstances, the perception of these two motion patterns may be similar.</p></figcaption>
</figure>
<p>A compelling sense of apparent motion occurs when the rate at which individual images appear is above about 10 Hz, as long as the positional changes between successive images is not too great. This rate is not fast enough, however, to produce a satisfying sense of continuous motion for most image display devices. Almost all such devices introduce brightness variation as one image is switched to the next. In well-lit conditions, the human visual system is sensitive to this varying brightness for rates of variations up to about 80 Hz. In lower light, detectability is present up to about 40 Hz. When the rate of alternating brightness is sufficiently high, <em><a id="index_term1344"/>flicker fusion</em> occurs and the variation is no longer visible.</p>
<p><a id="term-541"/><a id="term-1026"/><a id="term-1040"/><a id="term-1061"/><span aria-label="543" epub:type="pagebreak" id="pg_543" role="doc-pagebreak"/>To produce a compelling sense of visual motion, an image display must therefore satisfy two separate constraints:</p>
<ul class="list-bullet">
<li>
<p class="list">images must be updated at a rate ≥ 10 Hz;</p>
</li>
<li>
<p class="list">any flicker introduced in the process of updating images must occur at a rate ≥ 60–80 Hz.</p>
</li>
</ul>
<p>One solution is to require that the image update rate be greater than or equal to 60–80 Hz. In many situations, however, this is simply not possible. For computer graphics displays, the frame computation time is often substantially greater than 12–15 msec. Transmission bandwidth and limitations of older monitor technologies limit normal broadcast television to 25–30 images per second. (Some HDTV formats operate at 60 images/sec.) Movies update images at 24 frames/second due to exposure time requirements and the mechanical difficulties of physically moving film any faster than that.</p>
<p>Different display technologies solve this problem in different ways. Computer displays refresh the displayed image at ~70–80 Hz, regardless of how often the contents of the image change. The term <em><a id="index_term1346"/>frame rate</em> is ambiguous for such displays, since two values are required to characterize this display: <em>refresh rate</em>, which indicates the rate at which the image is redisplayed and <em>frame update rate</em>,which indicates the rate at which new images are generated for display. Standard nonHDTV broadcast television uses a refresh rate of 60 Hz (NTSC, used in North America and some other locations) or 50 Hz (PAL, used in most of the rest of the world). The frame update rate is half the <a id="index_term1374"/>refresh rate. Instead of displaying each new image twice, the display is <em>interlaced</em> by dividing alternating horizontal image lines into even and odd <em>fields</em> and alternating the display of these even and <a id="term-1033"/><a id="term-1067"/><span aria-label="544" epub:type="pagebreak" id="pg_544" role="doc-pagebreak"/>odd fields. Flicker is avoided in movies by using a mechanical shutter to blink each frame of the film three times before moving to the next frame, producing a refresh rate of 72 Hz while maintaining the frame update rate of 24 Hz.</p>
<p>The use of apparent motion to simulate continuous motion occasionally produces undesirable artifacts. Best known of these is the <em>wagon wheel illusion</em> in which the spokes of a rotating wheel appear to revolve in the opposite direction from what would be expected given the translational motion of the wheel. The wagon wheel illusion is an example of temporal aliasing. Spokes, or other spatially periodic patterns on a rotating disk, produce a temporally periodic signal for viewing locations that are fixed with respect to the center of the wheel or disk. Fixed frame update rates have the effect of sampling this temporally periodic signal in time. If the temporal frequency of the sampled pattern is too high, undersampling results in an aliased, lower temporal frequency appearing when the image is displayed. Under some circumstances, this distortion of temporal frequency causes a spatial distortion in which the wheel appears to move backwards. Wagon wheel illusions are more likely to occur with movies than with video, since the temporal sampling rate is lower.</p>
<p>Problems can also occur when apparent motion imagery is converted from one medium to another. This is of particular concern when 24 Hz movies are transferred to video. Not only does a non-interlaced format need to be translated to an interlaced format, but there is no straightforward way to move from 24 frames per second to 50 or 60 fields per second. Some high-end display devices have the ability to partially compensate for the artifacts introduced when film is converted to video.</p>
</section>
</section>
<section>
<h2 id="sec19_3"><a id="index_term1379"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec19_3" role="doc-backlink"><span class="green">19.3 Spatial Vision</span></a></h2>
<p>One of the critical operations performed by the visual system is the estimation of geometric properties of the visible environment, since these are central to determining information about objects, locations, and events. Vision has sometimes been described as <em><a id="index_term1352"/>inverse optics</em>, to emphasize that one function of the visual system is to invert the image formation process in order to determine the geometry, materials, and lighting in the world that produced a particular pattern on light on the retina. The central problem for a vision system is that properties of the visible environment are confounded in the patterns of light imaged on the retina. Brightness is a function of both illumination and reflectance, and can depend on environmental properties across large regions of space due to the complexities of light transport. Image locations of a projected environmental location at best can <a id="term-1041"/><a id="term-1050"/><a id="term-1057"/><a id="term-1070"/><a id="term-1077"/><span aria-label="545" epub:type="pagebreak" id="pg_545" role="doc-pagebreak"/>be used to constrain the position of that location to a half-line. As a consequence, it is rarely possible to uniquely determine the nature of the world that produced a particular imaged pattern of light.</p>
<p>Determining <em>surface layout</em>—the location and orientation of visible surfaces in the environment—is thought to be a key step in human vision. Most discussions of how the vision system extracts information about surface layout from the patterns of light it receives divide the problem into a set of <em><a id="index_term1386"/>visual cues</em>, with each cue describing a particular visual pattern which can be used to infer properties of surface layout along with the needed rules of inference. Since surface layout can rarely be determined accurately and unambiguously from vision alone, the process of inferring <a id="index_term1381"/>surface layout usually requires additional, nonvisual information. This can come from other senses or assumptions about what is likely to occur in the real world.</p>
<p>Visual cues are typically categorized into four categories. <em><a id="index_term1366"/>Ocularmotor cues</em> involve information about the position and focus of the eyes. <em>Disparity cues</em> involve information extracted from viewing the same surface point with two eyes, beyond that available just from the positioning of the eyes. <em><a id="index_term1359"/><a id="index_term1360"/>Motion cues</em> provide information about the world that arises from either the movement of the observer or the movement of objects. <em><a id="index_term1369"/>Pictorial cues</em> result from the process of projecting 3D surface shapes onto a 2D pattern of light that falls on the retina. This section deals with the visual cues relevant to the extraction of geometric information about individual points on surfaces. More general extraction of location and shape information is covered in <a href="C24_chapter19.xhtml#sec19_4">Section 19.4</a>.</p>
<section>
<h3 id="sec19_3_1"><a id="index_term1347"/><a id="index_term1357"/><span class="green">19.3.1 Frames of Reference and Measurement Scales</span></h3>
<p>Descriptions of the location and orientation of points on a visible surface must be done within the context of a particular frame of references that specifies the origin, orientation, and scaling of the coordinate system used in representing the geometric information. The human vision system uses multiple frames of reference, partially because of the different sorts of information available from different visual cues and partly because of the different purposes to which the information is put (Klatzky, 1998). <em>Egocentric</em> representations are defined with respect to the viewer’s body. They can be subdivided into coordinate systems fixed to the eyes, head, or body. <em>Allocentric</em> representations, also called <em>exocentric</em> representations, are defined with respect to something external to the viewer. Allocentric frames of reference can be local to some configuration of objects in the environment or can be globally defined in terms of distinctive locations, gravity, or geographic properties.</p>
<p><a id="term-1004"/><a id="term-1078"/><span aria-label="546" epub:type="pagebreak" id="pg_546" role="doc-pagebreak"/>The distance from the viewer to a particular visible location in the environment, expressed in an egocentric representation, is often referred to as <em>depth</em> in the perception literature. Surface orientation can be represented in either egocentric or allocentric coordinates. In egocentric representations of orientation, the term <em>slant</em> is used to refer to the angle between the line of sight to the point and the surface normal at the point, while the term <em>tilt</em> refers to the orientation of the projection of the surface normal onto a plane perpendicular to the line of sight.</p>
<p>Distance and orientation can be expressed in a variety of <em>measurement scales</em>. <em>Absolute</em> descriptions are specified using a standard that is not part of the sensed information itself. These can be culturally defined standards (e.g., meters), or standards relative to the viewer’s body (e.g., eye height, the width of one’s shoulders). <em>Relative</em> descriptions relate one perceived geometric property to another (e.g., point <span class="monospace"> a</span> is twice as far away as point <span class="monospace"> b</span>). <em>Ordinal</em> descriptions are a special case of relative measure in which the sign, but not the magnitude, of the relation is all that is represented. <a href="C24_chapter19.xhtml#t19_1">Table 19.1</a> provides a list of the most commonly considered visual cues, along with a characterization of the sorts of information they can potentially provide.</p>
<table class="table3" id="t19_1">
<caption><strong><span class="blue">Table 19.1</span></strong> Common visual cues for absolute (<span class="monospace">a</span>), relative (<span class="monospace">r</span>), and ordinal (<span class="monospace">o</span>) depth.</caption>
<thead>
<tr>
<th class="borderr" scope="col"><p class="tabtexth">Cue</p></th>
<th class="borderr" scope="col"><p class="tabtexthc"><span class="monospace">a</span></p></th>
<th class="borderr" scope="col"><p class="tabtexthc"><span class="monospace">r</span></p></th>
<th class="borderr" scope="col"><p class="tabtexthc"><span class="monospace">o</span></p></th>
<th class="borderr" scope="col"><p class="tabtexth">Requirements for Absolute Depth</p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="borderr"><p class="tabtext">Accommodation</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext">very limited range</p></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Binocular convergence</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext">limited range</p></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Binocular disparity</p></td>
<td class="borderr"><p class="tabtextc">-</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext">limited range</p></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Linear perspective, height in picture, horizon ratio</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext">requires viewpoint height</p></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Familiar size</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext"/></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Relative size</p></td>
<td class="borderr"><p class="tabtextc">-</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext"/></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Aerial perspective</p></td>
<td class="borderr"><p class="tabtextc">?</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext">adaptation to local conditions</p></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Absolute motion parallax</p></td>
<td class="borderr"><p class="tabtextc">?</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext">requires viewpoint velocity</p></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Relative motion parallax</p></td>
<td class="borderr"><p class="tabtextc">-</p></td>
<td class="borderr"><p class="tabtextc">-</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext"/></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Texture gradients</p></td>
<td class="borderr"><p class="tabtextc">-</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext"/></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Shading</p></td>
<td class="borderr"><p class="tabtextc">-</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext"/></td>
</tr>
<tr>
<td class="borderr"><p class="tabtext">Occlusion</p></td>
<td class="borderr"><p class="tabtextc">-</p></td>
<td class="borderr"><p class="tabtextc">-</p></td>
<td class="borderr"><p class="tabtextc">x</p></td>
<td class="borderr"><p class="tabtext"/></td>
</tr>
</tbody>
</table>
</section>
<section>
<h3 id="sec19_3_2"><span class="green">19.3.2 Ocularmotor Cues</span></h3>
<p>Ocularmotor information about depth results directly from the muscular control of the eyes. There are two distinct types of ocularmotor information. <em>Accommodation</em> <a id="term-566"/><a id="term-1051"/><span aria-label="547" epub:type="pagebreak" id="pg_547" role="doc-pagebreak"/>is the process by which the eye optically focuses at a particular distance. <em>Convergence</em> (often referred to as <em>vergence</em>) is the process by which the two eyes are pointed toward the same point in three-dimensional space. Both accommodation and convergence have the potential to provide absolute information about depth.</p>
<p>Physiologically, focusing in the human eye is accomplished by distorting the shape of the lens at the front of the eye. The vision system can infer depth from the amount of this distortion. Accommodation is a relatively weak cue to distance and is ineffective beyond about 2 m. Most people have increasing difficulty in focusing over a range of distances as they get beyond about 45 years old. For them, accommodation becomes even less effective.</p>
<p>Those not familiar with the specifics of visual perception sometimes confuse depth estimation from accommodation with depth information arising out of the blur associated with limited depth-of-field in the eye. The accommodation depth cue provides information about the distance to that portion of the visual field that it is in focus. It does not depend on the degree to which other portions of the visual field are out of focus, other than that blur is used by the visual system to adjust focus. Depth-of-field does seem to provide a degree of ordinal depth information (<a href="C24_chapter19.xhtml#f19_20">Figure 19.20</a>), though this effect has received only limited investigation.</p>
<figure id="f19_20" tabindex="0">
<img alt="" src="../images/fig19_20.jpg"/>
<figcaption><p><span class="blue">Figure 19.20.</span> Does the central square appear in front of the pattern of circles or is it seen as appearing through a square hole in the pattern of circles? The only difference in the two images is the sharpness of the edge between the line and circle patterns (Marshall, Burbeck, Arely, Rolland, and Martin (1999), used by permission).</p></figcaption>
</figure>
<p>If two eyes fixate on the same point in space, trigonometry can be used to determine the distance from the viewer to the viewed location (<a href="C24_chapter19.xhtml#f19_21">Figure 19.21</a>). For the simplest case, in which the point of interest is directly in front of the viewer,</p>
<figure id="f19_21" tabindex="0">
<img alt="" src="../images/fig19_21.jpg"/>
<figcaption><p><span class="blue">Figure 19.21.</span> <a id="term-1008"/><a id="term-1032"/><a id="term-1075"/><span aria-label="548" epub:type="pagebreak" id="pg_548" role="doc-pagebreak"/>The <em>vergence</em> of the two eyes provides information about the distance to the point on which the eyes are fixated.</p></figcaption>
</figure>
<div class="disp-formula" id="equ19_5">
<m:math alttext=""><m:mrow><m:mi>z</m:mi><m:mo>=</m:mo><m:mfrac><m:mrow><m:mi>i</m:mi><m:mi>p</m:mi><m:mi>d</m:mi><m:mo>/</m:mo><m:mn>2</m:mn></m:mrow><m:mrow><m:mtext>tan</m:mtext><m:mi>θ</m:mi></m:mrow></m:mfrac><m:mo>,</m:mo></m:mrow><m:mspace width="3em"/><m:mo>(19.5)</m:mo></m:math>
</div>
<p>where <em>z</em> is the distance to a point in the world, <em>ipd</em> is the <em><a id="index_term1351"/>interpupillary distance</em> indicating the distance between the eyes, and θ is the <em>vergence angle</em> indicating the orientation of the eyes relative to straight ahead. For small θ, which is the case for the geometric configuration of human eyes, tan<em>θ ≈ θ</em> when θ is expressed in radians. Thus, differences in vergence angle specify differences in depth by the following relationship:</p>
<div class="disp-formula" id="equ19_6">
<m:math alttext=""><m:mrow><m:mi mathvariant="normal">Δ</m:mi><m:mi>θ</m:mi><m:mo>≈</m:mo><m:mfrac><m:mrow><m:mi>i</m:mi><m:mi>p</m:mi><m:mi>d</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac><m:mo>⋅</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mi mathvariant="normal">Δ</m:mi><m:mi>z</m:mi></m:mrow></m:mfrac><m:mn>.</m:mn></m:mrow><m:mspace width="3em"/><m:mo>(19.6)</m:mo></m:math>
</div>
<p>As <em>θ</em> → 0 in uniform steps, Δ<em>z</em> gets increasingly larger. This means that stereo vision is less sensitive to changes in depth as the overall depth increases. Convergence in fact only provides information on absolute depth for distances out to a few meters. Beyond that, changes in distance produce changes in vergence angle that are too small to be useful.</p>
<p>There is an interaction between accommodation and convergence in the human visual system: accommodation is used to help determine the appropriate vergence angle, while vergence angle is used to help set the focus distance. Normally, this helps the visual system when there is uncertainty is setting either accommodation or vergence. However, stereographic computer displays break the relationship between focus and convergence that occurs in the real world, leading to a number of perceptual difficulties (Wann, Rushton, &amp; Mon-Williams, 1995).</p>
</section>
<section>
<h3 id="sec19_3_3"><a id="index_term1334"/><span class="green">19.3.3 Binocular Disparity</span></h3>
<p>The <a id="index_term1384"/>vergence angle of the eyes, when fixated on a common point in space, is only one of the ways that the visual system is able to determine depth from binocular stereo. A second mechanism involves a comparison of the retinal images in the two eyes and does not require information about where the eyes are pointed. A simple example demonstrates the effect. Hold your arm straight out in front of you, with your thumb pointed up. Stare at your thumb and then close one eye. Now, simultaneously open the closed eye and close the open eye. Your thumb will appear to be more or less stationary, while the more distant surfaces seen behind your thumb will appear to move from side to side (<a href="C24_chapter19.xhtml#f19_22">Figure 19.22</a>). The change in retinal position of points in the scene between the left and right eyes is called <em>disparity</em>.</p>
<figure id="f19_22" tabindex="0">
<img alt="" src="../images/fig19_22.jpg"/>
<figcaption><p><span class="blue">Figure 19.22.</span> <a id="term-52"/><a id="term-1009"/><span aria-label="549" epub:type="pagebreak" id="pg_549" role="doc-pagebreak"/>Binocular disparity. The view from the left and right eyes shows an offset for surface points at depths different from the point of fixation. <em>Images courtesy Peter Shirley.</em></p></figcaption>
</figure>
<p>The binocular disparity cue requires that the vision system be able to match the image of points in the world in one eye with the imaged locations of those points in the other eye, a process referred to as the <em>correspondence problem</em>. This is a relatively complicated process and is only partially understood. Once correspondences have been established, the relative positions on which particular points in the world project onto the left and right retinas indicate whether the points are closer than or farther away than the point of fixation. <em>Crossed disparity</em> occurs when the corresponding points are displaced outward relative to the fovea and indicates that the surface point is closer than the point of fixation. <em>Uncrossed disparity</em> occurs when the corresponding points are displaced inward relative to the fovea and indicates that the surface point is farther away than the point of fixation (<a href="C24_chapter19.xhtml#f19_23">Figure 19.23</a>). <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_4" id="rfn19_4" role="doc-noteref"><sup>4</sup></a> Binocular disparity is a relative depth cue, but it can provide information about absolute depth when scaled by convergence. Equation(19.5) applies to binocular disparity as well as binocular convergence. As with <a id="term-540"/><a id="term-1015"/><a id="term-1042"/><a id="term-1052"/><span aria-label="550" epub:type="pagebreak" id="pg_550" role="doc-pagebreak"/>convergence, the sensitivity of binocular disparity to changes in depth decreases with depth.</p>
<figure id="f19_23" tabindex="0">
<img alt="" src="../images/fig19_23.jpg"/>
<figcaption><p><span class="blue">Figure 19.23.</span> Near the line of sight, surface points nearer than the fixation point produce <a id="index_term1339"/>disparities in the opposite direction from those associated with surface points more distant than the fixation point.</p></figcaption>
</figure>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_4" id="fn19_4"><sup>4</sup></a> Technically, crossed and uncrossed disparities indicate that the surface point generating the disparity is closer to or farther away from the <em>horopter</em>. The horopter is not a fixed distance away from the eyes but rather it is a curved surface passing through the point of fixation.</p></aside>
</section>
<section>
<h3 id="sec19_3_4"><a id="index_term1361"/><span class="green">19.3.4 Motion Cues</span></h3>
<p>Relative motion between the eyes and visible surfaces will produce changes in the image of those surfaces on the retina. Three-dimensional relative motion between the eye and a surface point produces two-dimensional motion of the projection of the surface point on the retina. This retinal motion is given the name <em><a id="index_term1367"/>optic flow</em>. Optic flow serves as the basis for several types of depth cues. In addition, optic flow can be used to determine information about how a person is moving in the world and whether or not a collision is imminent (<a href="C24_chapter19.xhtml#sec19_4_3">Section 19.4.3</a>).</p>
<p>If a person moves to the side while continuing to fixate on some surface point, then optic flow provides information about depth similar to stereo disparity. This is referred to as <em>motion parallax</em>. For other surface points that project to retinal locations near the fixation point, zero optic flow indicates a depth equivalent to the fixation point; flow in the opposite direction to head translation indicates nearer points, equivalent to crossed disparity; and flow in the same direction as head translation indicates farther points, equivalent to uncrossed disparity (<a href="C24_chapter19.xhtml#f19_24">Figure 19.24</a>). Motion parallax is a powerful cue to relative depth. In principle, motion parallax can provide absolute depth information if the visual system has access to information about the velocity of head motion. In practice, motion parallax appears at best to be a weak cue for absolute depth.</p>
<p>In addition to egocentric depth information due to motion parallax, visual motion can also provide information about the three-dimensional shape of objects moving relative to the viewer. In the perception literature, this is known as the <em><a id="index_term650"/>kinetic depth effect</em>. In computer vision, it is referred to as <em><a id="index_term1133"/>structure-from-motion</em>. The kinetic depth effect presumes that one component of object motion is <em>rotation in depth</em>, meaning that there is a component of rotation around an axis perpendicular to the line of sight.</p>
<figure id="f19_24" tabindex="0">
<img alt="" src="../images/fig19_24.jpg"/>
<figcaption><p><span class="blue">Figure 19.24.</span> <a id="term-545"/><a id="term-569"/><a id="term-1043"/><a id="term-1053"/><span aria-label="551" epub:type="pagebreak" id="pg_551" role="doc-pagebreak"/>(a) Motion parallax generated by sideways movement to the right while looking at an extended ground plane. (b) The same motion, with eye tracking of the fixation point.</p></figcaption>
</figure>
<p>Optic flow can also provide information about the shape and location of surface boundaries, as shown in <a href="C24_chapter19.xhtml#f19_25">Figure 19.25</a>. <a id="index_term1378"/>Spatial discontinuities in optic flow almost always either correspond to depth discontinuities or result from independently moving objects. Simple comparisons of the magnitude of optic flow are insufficient to determine the sign of depth changes, except in the special case of a viewer moving through an otherwise static world. Even when independently moving objects are present, however, the sign of the change in depth across surface boundaries can often be determined by other means. Motion often changes the portion of the more distant surface visible at surface boundaries. The appearance ( <em>accretion</em>) or disappearance ( <em>deletion</em>) of surface texture occurs because the nearer, occlud <em>ing</em> surface progressively uncovers or covers portions of the more distant, occlud <em>ed</em> surface. Comparisons of the motion of surface texture to either side of a boundary can also be used to infer ordinal depth, even in the absence of accretion or deletion of the texture. Discontinuities in optic flow and accretion/deletion of surface texture are referred to as <em>dynamic occlusion</em> cues and are another powerful source of visual information about the spatial structure of the environment.</p>
<figure id="f19_25" tabindex="0">
<img alt="" src="../images/fig19_25.jpg"/>
<figcaption><p><span class="blue">Figure 19.25.</span> Discontinuities in optic flow signal surface boundaries. In many cases, the sign of the depth change (i.e., the ordinal depth) can be determined.</p></figcaption>
</figure>
<p>The speed that a viewer is traveling relative to points in the world cannot be determined from visual motion alone (see <a href="C24_chapter19.xhtml#sec19_4_3">Section 19.4.3</a>). Despite this limitation, it is possible to use visual information to determine the time it will take to reach a visible point in the world, even when speed cannot be determined. When velocity is constant, <em>time-to-contact</em> (often referred to as <em>time-to-collision</em>)isgivenbythe retinal size of an entity toward which the observer is moving, divided by the rate at which that image size is increasing. <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_5" id="rfn19_5" role="doc-noteref"><sup>5</sup></a> In the biological vision literature, this is <a id="term-508"/><a id="term-1031"/><a id="term-1049"/><a id="term-1058"/><span aria-label="552" epub:type="pagebreak" id="pg_552" role="doc-pagebreak"/>often called the <em>τ</em> <em>function</em> (Lee &amp; Reddish, 1981). If distance information to the structure in the world on which the time-to-collision estimate is based is available, then this can be used to determine speed.</p>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_5" id="fn19_5"><sup>5</sup></a> The terms time-to-collision and time-to-contact are misleading, since contact will only occur if the viewer’s trajectory actually passes through or near the entity under view.</p></aside>
</section>
<section>
<h3 id="sec19_3_5"><a id="index_term1370"/><span class="green">19.3.5 Pictorial Cues</span></h3>
<p>An image can contain much information about the spatial structure of the world from which it arose, even in the absence of binocular stereo or motion. As evidence for this, note that the world still appears three-dimensional even if we close one eye, hold our head stationary, and nothing moves in the environment. (As discussed in <a href="C24_chapter19.xhtml#sec19_5">Section 19.5</a>, the situation is more complicated in the case of photographs and other displayed images.) There are three classes of such <em>pictorial depth cues</em>. The best known of these involve <em>linear perspective</em>. There are also a number of <em><a id="index_term1365"/>occlusion cues</em> that provide information about ordinal depth even in the absence of perspective. Finally, <em><a id="index_term1350"/>illumination cues</em> involving shading, shadows and interreflections, and aerial perspective also provide visual information about spatial layout.</p>
<p>The term <em>linear perspective</em> is often used to refer to properties of images involving object size in the image scaled by distance, the convergence of parallel lines, the ground plane extending to a visible horizon, and the relationship between the distance to objects on the ground plane and the image location of those objects relative to the horizon (<a href="C24_chapter19.xhtml#f19_26">Figure 19.26</a>). More formally, linear perspective cues are those visual cues which exploit the fact that under perspective projection, the image location onto which points in the world are projected is scaled by <span class="inline-formula"><m:math alttext=""><m:mrow><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac></m:mrow></m:math></span>, where <em>z</em> is the distance from the point of projection to the point in the environment. Direct consequences of this relationship are that points that are farther away are projected to points closer to the center of the image (convergence of parallel lines) and that the spacing between the image of points in the world decreases for more distant world points (object size in the image is scaled by distance). <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_6" id="rfn19_6" role="doc-noteref"><sup>6</sup></a> The fact that the image of an infinite flat surface in the world ends at a finite horizon is explained by examining the perspective projection equation as <em>z</em> → ∞.</p>
<figure id="f19_26" tabindex="0">
<img alt="" src="../images/fig19_26.jpg"/>
<figcaption><p><span class="blue">Figure 19.26.</span> The classical linear perspective effects include object size scaled by distance, the convergence of parallel lines, the ground plane extending to a visible horizon, and position on the ground plane relative to the horizon. <em>Image courtesy Sam Pullara.</em></p></figcaption>
</figure>
<p>With the exception of size-related effects described in <a href="C24_chapter19.xhtml#sec19_4_2">Section 19.4.2</a>, most pictorial depth cues involving <a id="index_term693"/>linear perspective depend on objects of interest being in contact with a ground plane. In effect, these cues estimate not the distance to the objects but, instead, the distance to the contact point on the ground plane. Assuming observer and object are both on top of a horizontal ground plane, then locations on the ground plane lower in the view will be close. <a href="C24_chapter19.xhtml#f19_27">Figure 19.27</a> illustrates this effect quantitatively. For a viewpoint <em>h</em> above the ground and an <em>angle of declination θ</em> between the horizon and a point of interest on the ground, the point in question is a distance <em>d</em> =<em>h</em> cotθ from the point at which the observer is standing. The angle of declination provides relative depth information for arbitrary fixed viewpoints and can provide absolute depth when scaling by eye height ( <em>h</em>) is possible.</p>
<figure id="f19_27" tabindex="0">
<img alt="" src="../images/fig19_27.jpg"/>
<figcaption><p><span class="blue">Figure 19.27.</span> Absolute distance to locations on the ground plane can be determined based on <a id="index_term1338"/>declination angle from the horizon and eye height.</p></figcaption>
</figure>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_6" id="fn19_6"><sup>6</sup></a> The actual mathematics for analyzing the specifics of biological vision are different, since eyes are not well approximated by the planar projection formulation used in computer graphics and most other imaging applications.</p></aside>
<p class="indent"><a id="term-1014"/><span aria-label="553" epub:type="pagebreak" id="pg_553" role="doc-pagebreak"/>While the human visual system almost certainly makes use of angle of declination as a depth cue, the exact mechanisms used to acquire the needed information are not clear. The angle θ could be obtained relative to either gravity or the visible horizon. There is some evidence that both are used in human vision. Eye height <em>h</em> could be based on posture, visually determined by looking at the ground at one’s feet, or learned by experience and presumed to be constant. While a number of researchers have investigated this issue, if and how these values are determined is not yet known with certainty.</p>
<p><a id="index_term1070"/><a id="index_term1375"/>Shadows provide a variety of types of information about three-dimensional spatial layout. <em>Attached shadows</em> indicate that an object is in contact with another surface, often consisting of the ground plane. <em>Detached shadows</em> indicate that an object is close to some surface, but not in contact with that surface. Shadows can serve as an indirect depth cue by causing an object to appear at the depth of the location of the shadow on the ground plane (Yonas, Goldsmith, &amp; Hallstrom, 1978). When utilizing this cue, the visual system seems to make the assumption that light is coming from directly above (<a href="C24_chapter19.xhtml#f19_28">Figure 19.28</a>).</p>
<figure id="f19_28" tabindex="0">
<img alt="" src="../images/fig19_28.jpg"/>
<figcaption><p><span class="blue">Figure 19.28.</span> Shadows can indirectly function as a depth cue by associating the depth of an object with a location on the ground plane (after Kersten, Mamassian, and Knill (1997)).</p></figcaption>
</figure>
<p>Vision provides information about surface orientation as well as distance. It is convenient to represent visually determined surface orientation in terms of <em>tilt</em>, defined as the orientation in the image of the projection of the surface normal, and <em>slant</em>, defined as the angle between the surface normal and the line of sight.</p>
<p><a id="term-778"/><a id="term-783"/><a id="term-1062"/><span aria-label="554" epub:type="pagebreak" id="pg_554" role="doc-pagebreak"/>A visible surface horizon can be used to find the orientation of an (effectively infinite) surface relative to the viewer. Determining tilt is straightforward, since the tilt of the surface is the orientation of the visible horizon. Slant can be recovered as well, since the lines of sight from the eye point to the horizon define a plane parallel to the surface. In many situations, either the surface horizon is not visible or the surface is small enough that its far edge does not correspond to an actual horizon. In such cases, visible <a id="index_term1383"/>texture can still be used to estimate orientation.</p>
<p>In the context of <a id="index_term1187"/>perception, the term <em>texture</em> refers to visual patterns consisting of sub-patterns replicated over a surface. The sub-patterns and their distribution can be fixed and regular, as for a checkerboard, or consistent in a more statistical sense, as in the view of a grassy field. <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_7" id="rfn19_7" role="doc-noteref"><sup>7</sup></a> When a textured surface is viewed from an oblique angle, the projected view of the texture is distorted relative to the actual markings on the surface. Two quite distinct types of distortions occur (Knill, 1998), both affected by the amount of slant. The position and size of texture elements are subject to the linear perspective effects described above. This produces a <em>texture gradient</em> (Gibson, 1950) due to both element size and spacing decreasing with distance (<a href="C24_chapter19.xhtml#f19_29">Figure 19.29(a)</a>). Both the image of individual texture elements and the distribution of elements are <em>foreshortened</em> under oblique viewing (<a href="C24_chapter19.xhtml#f19_29">Figure 19.29(b)</a>). This produces a compression in the direction of tilt. For example, an obliquely viewed circle appears as an ellipse, with the ratio of the minor to major axes equal to the cosine of the slant. Note that foreshortening itself is not a result of linear perspective, though in practice both linear perspective and foreshortening provide information about slant. <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_8" id="rfn19_8" role="doc-noteref"><sup>8</sup></a></p>
<figure id="f19_29" tabindex="0">
<img alt="" src="../images/fig19_29.jpg"/>
<figcaption><p><span class="blue">Figure 19.29.</span> Texture cues for slant. (a) Near surface exhibiting compression and texture gradient;(b) distant surface exhibiting only compression; (c) variability in appearance of near surface with regular geometric variability.</p></figcaption>
</figure>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_7" id="fn19_7"><sup>7</sup></a> In computer graphics, the term <em>texture</em> has a different meaning, referring to any image that is applied to a surface as part of the rendering process.</p></aside>
<aside class="footnotez" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_8" id="fn19_8"><sup>8</sup></a> A third form of visual distortion occurs when surfaces with distinct 3D surface relief are viewed obliquely (Leung &amp; Malik, 1997), as shown in <a href="C24_chapter19.xhtml#f19_29">Figure 19.29(c)</a>. Nothing is currently known about if or how this effect might be used by the human vision system to determine slant.</p></aside>
<p><a id="term-779"/><a id="term-881"/><a id="term-1073"/><span aria-label="555" epub:type="pagebreak" id="pg_555" role="doc-pagebreak"/>For texture gradients to serve as a cue to surface slant, the average size and spacing of texture elements must be constant over the textured surface. If spatial variability in size and spacing in the image is not due in its entirely to the projection process, then attempts to invert the effects of projection will produce incorrect inferences about surface orientation. Likewise, the foreshortening cue fails if the shape of texture elements is not isotropic, since then asymmetric texture element image shapes would occur in situations not associated with oblique viewing. These are examples of the assumptions often required in order for spatial visual cues to be effective. Such assumptions are reasonable to the degree that they reflect commonly occurring properties of the world.</p>
<p>Shading also provides information about surface shape (<a href="C24_chapter19.xhtml#f19_30">Figure 19.30</a>). The brightness of viewed points on a surface depends on the surface reflectance and the orientation of the surface with respect to directional light sources and the observation point. When the relative position of an object, viewing direction, and illumination direction remain fixed, changes in brightness over a constant reflectance surface are indications of changes in the orientation of the surface of the object. <em>Shape-from-shading</em> is the process of recovering surface shape from these variations in observed brightness. It is almost never possible to recover the actual orientation of surfaces from shading alone, though shading can often be combined with other cues to provide an effective indication of surface shape. For surfaces with fine-scale geometric variability, shading can provide a compelling three-dimensional appearance, even for an image rendered on a two-dimensional surface (<a href="C24_chapter19.xhtml#f19_31">Figure 19.31</a>).</p>
<figure id="f19_30" tabindex="0">
<img alt="" src="../images/fig19_30.jpg"/>
<figcaption><p><span class="blue">Figure 19.30.</span> Shape-from-shading. The images in (a) and (b) appear to have different 3D shapes because of differences in the rate of change of brightness over their surfaces.</p></figcaption>
</figure>
<figure id="f19_31" tabindex="0">
<img alt="" src="../images/fig19_31.jpg"/>
<figcaption><p><span class="blue">Figure 19.31.</span> Shading can generate a strong perception of three-dimensional shape. In this figure, the effect is stronger if you view the image from several meters away using one eye. It becomes yet stronger if you place a piece of cardboard in front of the figure with a hole cut out slightly smaller than the picture (see <a href="C24_chapter19.xhtml#sec19_5">Section 19.5</a>). <em>Image courtesy Albert Yonas.</em></p></figcaption>
</figure>
<p><a id="term-780"/><a id="term-882"/><a id="term-1034"/><a id="term-1059"/><a id="term-1074"/><span aria-label="556" epub:type="pagebreak" id="pg_556" role="doc-pagebreak"/>There are a number of <a id="index_term1371"/>pictorial cues that yield ordinal information about depth, without directly indicating actual distance. In line drawings, different types of junctions provide constraints on the 3D geometry that could have generated the drawing (<a href="C24_chapter19.xhtml#f19_32">Figure 19.32</a>). Many of these effects occur in more natural images as well. Most perceptually effective of the <a id="index_term1353"/>junction cues are <em>T-junctions</em>, which are strong indicators that the surface opposite the stem of the T is occluding at least one more distant surface. T-junctions often generate a sense of <em>amodal completion</em>, in which one surface is seen to continue behind a nearer, occluding surface (<a href="C24_chapter19.xhtml#f19_33">Figure 19.33</a>).</p>
<figure id="f19_32" tabindex="0">
<img alt="" src="../images/fig19_32.jpg"/>
<figcaption><p><span class="blue">Figure 19.32.</span> (a) Junctions provide information about occlusion and the convexity or concavity of corners. (b) Common junction types for planar surface objects.</p></figcaption>
</figure>
<figure id="f19_33" tabindex="0">
<img alt="" src="../images/fig19_33.jpg"/>
<figcaption><p><span class="blue">Figure 19.33.</span> T-junctions cause the left disk to appear to be continuing behind the rectangle, while the right disk appears in front of the rectangle, which is seen to continue behind the disk.</p></figcaption>
</figure>
<p>Atmospheric effects cause visual changes that can provide information about depth, particularly outdoors over long distances. Leonardo da Vinci was the first to describe <em>aerial perspective</em> (also called <em>atmospheric perspective</em>), in which scattering reduces the contrast of distant portions of the scene and causes them to appear more bluish than if they were nearer (da Vinci, 1970) (see <a href="C24_chapter19.xhtml#f19_34">Figure 19.34</a>). Aerial perspective is predominately a relative depth cue, though there is some speculation that it may affect perception of absolute distance as well. While many people believe that more distant objects look blurrier due to atmospheric effects, atmospheric scattering actually causes little blur.</p>
<figure id="f19_34" tabindex="0">
<img alt="" src="../images/fig19_34.jpg"/>
<figcaption><p><span class="blue">Figure 19.34.</span> Aerial perspective, in which atmospheric effects reduce contrast and shift colors toward blue, provides a depth cue over long distances.</p></figcaption>
</figure>
</section>
</section>
<section>
<h2 id="sec19_4"><a id="term-3"/><a id="term-28"/><a id="term-483"/><a id="term-615"/><a id="term-1005"/><a id="term-1035"/><a id="term-1047"/><a id="term-1060"/><span aria-label="557" epub:type="pagebreak" id="pg_557" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec19_4" role="doc-backlink"><span class="green">19.4 Objects, Locations, and Events</span></a></h2>
<p>While there is fairly wide agreement among current vision scientists that the purpose of vision is to extract information about objects, locations, and events, there is little consensus on the key features of what information is extracted, how it is extracted, or how the information is used to perform tasks. Significant controversies exist about the nature of <a id="index_term1364"/>object recognition and the potential interactions between object recognition and other aspects of perception. Most of what we know about location involves low-level spatial vision, not issues associated with spatial relationships between complex objects or the visual processes required to navigate in complex environments. We know a fair amount about how people perceive their speed and heading as they move through the world, but have only a limited understanding of actual event perception. Visual attention involves aspects of the perception of objects, locations, and events. While there is much data about the phenomenology of visual attention for relatively simple and well-controlled stimuli, we know much less about how visual attention serves high-level perceptual goals.</p>
<section>
<h3 id="sec19_4_1"><a id="term-4"/><a id="term-29"/><span aria-label="558" epub:type="pagebreak" id="pg_558" role="doc-pagebreak"/><span class="green">19.4.1 Object Recognition</span></h3>
<p>Object recognition involves segregating an image into constituent parts corresponding to distinct physical entities and determining the identity of those entities. <a href="C24_chapter19.xhtml#f19_35">Figure 19.35</a> illustrates a few of the complexities associated with this process. We have little difficulty recognizing that the image on the left is some sort of vehicle, even though we have never before seen this particular view of a vehicle nor do most of us typically associate vehicles with this context. The image on the right is less easily recognizable until the page is turned upside down, indicating an orientational preference in human object recognition.</p>
<figure id="f19_35" tabindex="0">
<img alt="" src="../images/fig19_35.jpg"/>
<figcaption><p><span class="blue">Figure 19.35.</span> The complexities of object recognition. (a) We recognize a vehicle-like object even though we have likely never seen this particular view of a vehicle before. (b) The image is hard to recognize based on a quick view. It becomes much easier to recognize if the book is turned upside down.</p></figcaption>
</figure>
<p>Object recognition is thought to involve two, fairly distinct steps. The first step organizes the visual field into <em>groupings</em> likely to correspond to objects and surfaces. These grouping processes are very powerful (see <a href="C24_chapter19.xhtml#f19_36">Figure 19.36</a>), though there is little or no conscious awareness of the low-level image features that generate the grouping effect. <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_9" id="rfn19_9" role="doc-noteref"><sup>9</sup></a> Grouping is based on the complex interaction of proximity, similarities in the brightness, color, shape, and orientation of primitive structures in the image, common motion, and a variety of more complex relationships.</p>
<figure id="f19_36" tabindex="0">
<img alt="" src="../images/fig19_36.jpg"/>
<figcaption><p><span class="blue">Figure 19.36.</span> <a id="term-562"/><a id="term-1048"/><a id="term-1071"/><span aria-label="559" epub:type="pagebreak" id="pg_559" role="doc-pagebreak"/>Images are perceptually organized into groupings based on a complex set of similarity and organizational criteria. (a) Similarity in brightness results in four horizontal groupings. (b) Proximity resulting in three vertical groupings.</p></figcaption>
</figure>
<p>The second step in object recognition is to interpret groupings as identified objects. A computational analysis suggests that there are a number of distinctly different ways in which an object can be identified. The perceptual data is unclear as to which of these are actually used in human vision. Object recognition requires that the vision system have available to it descriptions of each class of object sufficient to discriminate each class from all others. Theories of object recognition differ in the nature of the information describing each class and the mechanisms used to match these descriptions to actual views of the world.</p>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_9" id="fn19_9"><sup>9</sup></a> The most common form of visual camouflage involves adding visual textures that fool the perceptual grouping processes so that the view of the world cannot be organized in a way that separates out the object being camouflaged.</p></aside>
<p class="indent"><a id="term-845"/><a id="term-1072"/><span aria-label="560" epub:type="pagebreak" id="pg_560" role="doc-pagebreak"/>Three general types of descriptions are possible. <em>Templates</em> represent object classes in terms of prototypical views of objects in each class. <a href="C24_chapter19.xhtml#f19_37">Figure 19.37</a> shows asimpleexample. <em>Structural descriptions</em> represent object classes in terms of distinctive features of each class likely to be easily detected in views of the object, along with information about the geometric relationships between the features. Structural descriptions can either be represented in 2D or 3D. For 2D models of objects types, there must be a separate description for each distinctly different potential view of the object. For 3D models, two distinct forms of matching strategies are possible. In one, the three-dimensional structure of the viewed object is determined prior to classification using whatever spatial cues are available, and then this 3D description of the view is matched to 3D prototypes of known objects. The other possibility is that some mechanism allows the determination of the orientation of the yet-to-be identified object under view. This orientation information is used to rotate and project potential 3D descriptions in a way that allows a 2D matching of the description and the viewed object. Finally, the last option for describing the properties of object classes involves <em>invariant features</em> which describe classes of objects in terms of more generic geometric properties, particularly those that are likely be be insensitive to different views of the object.</p>
<figure id="f19_37" tabindex="0">
<img alt="" src="../images/fig19_37.jpg"/>
<figcaption><p><span class="blue">Figure 19.37.</span> <a id="index_term1382"/>Template matching. The bright spot in the right image indicates the best match location to the template in the left image. <em>Image courtesy National Archives and Records Administration.</em></p></figcaption>
</figure>
</section>
<section>
<h3 id="sec19_4_2"><a id="index_term1377"/><a id="index_term1340"/><span class="green">19.4.2 Size and Distance</span></h3>
<p>In the absence of more definitive information about depth, objects which project onto a larger area of the retina are seen as closer compared with objects which project to a smaller retinal area, an effect called <em>relative size</em>. A more powerful cue involves <em>familiar size</em>, which can provide information for absolute distance to recognizable objects of known size. The strength of familiar size as a depth cue can be seen in illusions such as <a href="C24_chapter19.xhtml#f19_38">Figure 19.38</a>, in which it is put in conflict with ground-plane, perspective-based depth cues. Familiar size is one part of the <em>size-distance</em> relationship, relating the physical size of an object, the optical size of the same object projected onto the retina, and the distance of the object from the eye (<a href="C24_chapter19.xhtml#f19_39">Figure 19.39</a>).</p>
<figure id="f19_38" tabindex="0">
<img alt="" src="../images/fig19_38.jpg"/>
<figcaption><p><span class="blue">Figure 19.38.</span> <a id="term-801"/><a id="term-1016"/><a id="term-1065"/><span aria-label="561" epub:type="pagebreak" id="pg_561" role="doc-pagebreak"/>Left: perspective and familiar size cues are consistent. Right: perspective and familiar size cues are inconsistent. <em>Images courtesy Peter Shirley, Scott Kuhl, and J. Dylan Lacewell.</em></p></figcaption>
</figure>
<figure id="f19_39" tabindex="0">
<img alt="" src="../images/fig19_39.jpg"/>
<figcaption><p><span class="blue">Figure 19.39.</span> The <em><a id="index_term1092"/>size-distance relationship</em> allows the distance to objects of known size to be determined based on the visual angle subtended by the object. Likewise, the size of an object at a know distance can be determined based on the visual angle subtended by the object.</p></figcaption>
</figure>
<p>When objects are sitting on top of a flat-ground plane, additional sources for depth information become available, particularly when the horizon is either visible or can be derived from other perspective information. The angle of declination to the contact point on the ground is a relative depth cue and provides absolute egocentric distance when scaled by eye height, as previously shown in <a href="C24_chapter19.xhtml#f19_27">Figure 19.27</a>. The <em>horizon ratio</em>, in which the total visible height of an object is compared with the visible extent of that portion of the object appearing below the horizon, can be used to determine the actual size of objects, even when the distance to the objects is not known (<a href="C24_chapter19.xhtml#f19_40">Figure 19.40</a>). Underlying the horizon ratio is the fact that for a flat-ground plane, the line of sight to the horizon intersects objects at a position that is exactly an eye height above the ground.</p>
<figure id="f19_40" tabindex="0">
<img alt="" src="../images/fig19_40.jpg"/>
<figcaption><p><span class="blue">Figure 19.40.</span> <a id="term-400"/><a id="term-800"/><a id="term-802"/><a id="term-1017"/><a id="term-1029"/><a id="term-1030"/><a id="term-1066"/><span aria-label="562" epub:type="pagebreak" id="pg_562" role="doc-pagebreak"/>(a) The <em><a id="index_term1349"/>horizon ratio</em> can be used to determine depth by comparing the visible portion of an object below the horizon to the total vertical visible extent of the object. (b) A real-world example.</p></figcaption>
</figure>
<p>The human visual system is sufficiently able to determine the absolute size of most viewed objects; our perception of size is dominated by the the actual physical <a id="term-784"/><a id="term-1019"/><a id="term-1024"/><a id="term-1054"/><a id="term-1063"/><span aria-label="563" epub:type="pagebreak" id="pg_563" role="doc-pagebreak"/>size, and we have almost no conscious awareness of the corresponding retinal size of objects. This is similar to lightness constancy, discussed earlier, in that our perception is dominated by inferred properties of the world, not the low level features actually sensed by photoreceptors in the retina. Gregory (1997) describes a simple example of <em><a id="index_term1091"/>size constancy</em>. Hold your two hands out in front of you, one at arm’s length and the other at half that distance away from you (<a href="C24_chapter19.xhtml#f19_41">Figure 19.41(a)</a>). Your two hands will look almost the same size, even though the retinal sizes differ by a factor of two. The effect is much less strong if the nearer hand partially occludes the more distant hand, particularly if you close one eye (<a href="C24_chapter19.xhtml#f19_41">Figure 19.41(b)</a>). The visual system also exhibits <em><a id="index_term1071"/>shape constancy</em>, where the perception of geometric structure is close to actual object geometry than might be expected given the distortions of the retinal image due to perspective (<a href="C24_chapter19.xhtml#f19_42">Figure 19.42</a>).</p>
<figure id="f19_41" tabindex="0">
<img alt="" src="../images/fig19_41.jpg"/>
<figcaption><p><span class="blue">Figure 19.41.</span> (a) Size constancy makes hands positioned at different distances from the eye appear to be nearly the same size for real-world viewing, even though the retinal sizes are quite different. (b) The effect is less strong when one hand is partially occluded by the other, particularly when one eye is closed. <em>Images courtesy Peter Shirley and Pat Moulis.</em></p></figcaption>
</figure>
<figure id="f19_42" tabindex="0">
<img alt="" src="../images/fig19_42.jpg"/>
<figcaption><p><span class="blue">Figure 19.42.</span> Shape constancy—the table looks rectangular even though its shape in the image is an irregular four-sided polygon.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec19_4_3"><span class="green">19.4.3 Events</span></h3>
<p>Most aspects of <a id="index_term1342"/>event perception are beyond the scope of this chapter, since they involve complex nonvisual cognitive processes. Three types of event perception are primarily visual, however, and are also of clear relevance to computer graphics. Vision is capable of providing information about how a person is moving in the world, the existence of independently moving objects in the world, and the potential for collisions either due to observer motion or due to objects moving toward the observer.</p>
<p>Vision can be used to determine rotation and the direction of translation relative to the environment. The simplest case involves movement toward a flat surface oriented perpendicularly to the line of sight. Presuming that there is sufficient surface texture to enable the recovery of <a id="index_term1368"/>optic flow, the flow field will form a symmetric pattern as shown in <a href="C24_chapter19.xhtml#f19_43">Figure 19.43(a)</a>. The location in the field of view of the <em><a id="index_term1345"/>focus of expansion</em> of the flow field will have an associated line of sight corresponding to the direction of translation. While optic flow can be used to visually determine the direction of motion, it does not contain enough information to determine speed. To see this, consider the situation in which the world is made twice as large and the viewer moves twice as fast. The decrease in the magnitude of flow values due to the doubling of distances is exactly compensated for by the increase in the magnitude of flow values due to the doubling of velocity, resulting in an identical flow field.</p>
<figure id="f19_43" tabindex="0">
<img alt="" src="../images/fig19_43.jpg"/>
<figcaption><p><span class="blue">Figure 19.43.</span> (a) Movement toward a flat, textured surface produces an expanding flow field, with the <em>focus of expansion</em> indicating the line of sight corresponding to the direction of motion. (b) The flow field resulting from rotation around the vertical axis while viewing a flat surface oriented perpendicularly to the line of sight. (c) The flow field resulting from translation parallel to a flat, textured surface.</p></figcaption>
</figure>
<p><a href="C24_chapter19.xhtml#f19_43">Figure 19.43(b)</a> shows the optic flow field resulting from the viewer (or more accurately, the viewer’s eyes) rotating around the vertical axis. Unlike the situation with respect to translational motion, optic flow provides sufficient information to determine both the axis of rotation and the (angular) speed of rotation. The practical problem in exploiting this is that the flow resulting from pure rotational motion around an axis perpendicular to the line of sight is quite similar to the flow resulting from pure translation in the direction that is perpendicular to both the line of sight and this rotational axis, making it difficult to visually discriminate between the two very different types of motion (<a href="C24_chapter19.xhtml#f19_43">Figure 19.43(c)</a>). <a href="C24_chapter19.xhtml#f19_44">Figure 19.44</a> shows the optical flow patterns generated by movement through a more realistic environment.</p>
<figure id="f19_44" tabindex="0">
<img alt="" src="../images/fig19_44.jpg"/>
<figcaption><p><span class="blue">Figure 19.44.</span> The optic flow generated by moving through an otherwise static environment provides information about both the motion relative to the environment and the distances to points in the environment. In this case, the direction of view is depressed from the horizon, but as indicated by the focus of expansion, the motion is parallel to the ground plane.</p></figcaption>
</figure>
<p><a id="term-286"/><a id="term-570"/><a id="term-1025"/><a id="term-1055"/><span aria-label="564" epub:type="pagebreak" id="pg_564" role="doc-pagebreak"/>If a viewer is completely stationary, visual detection of moving objects is easy, since such objects will be associated with the only nonzero optic flow in the field of view. The situation is considerably more complicated when the observer is moving, since the visual field will be dominated by nonzero flow, most or all of <a id="term-546"/><a id="term-571"/><a id="term-1001"/><a id="term-1044"/><a id="term-1045"/><a id="term-1056"/><span aria-label="565" epub:type="pagebreak" id="pg_565" role="doc-pagebreak"/>which is due to relative motion between the observer and the static environment (Thompson &amp; Pong, 1990). In such cases, the visual system must be sensitive to patterns in the optic flow field that are inconsistent with flow fields associated with observer movement relative to a static environment (<a href="C24_chapter19.xhtml#f19_45">Figure 19.45</a>).</p>
<figure id="f19_45" tabindex="0">
<img alt="" src="../images/fig19_45.jpg"/>
<figcaption><p><span class="blue">Figure 19.45.</span> Visual detection of moving objects from a moving observation point requires recognizing patterns in the optic flow that cannot be associated with motion through a static environment.</p></figcaption>
</figure>
<p><a href="C24_chapter19.xhtml#sec19_3_4">Section 19.3.4</a> described how vision can be used to determine time to contact with a point in the environment even when the speed of motion is not known. Assuming a viewer moving with a straight, constant-speed trajectory and no independently moving objects in the world, contact will be made with whatever surface is in the direction of the line of sight corresponding to the focus of expansion at a time indicated by the <em>τ</em> relationship. An independently moving object complicates the matter of determining if a collision will in fact occur. Sailors use a method for detecting potential collisions that may also be employed in the human visual system: for non-accelerating straight-line motion, collisions will occur with objects that are visually expanding but otherwise remain visually stationary in the egocentric frame of reference.</p>
<p>One form of more complex event perception merits discussion here, since it is so important in interactive computer graphics. People are particularly sensitive to motion corresponding to human movement. Locomotion can be recognized when the only features visible are lights on the walker’s joints (Johansson, 1973). Such <em><a id="index_term1362"/>moving light displays</em> are often even sufficient to recognize properties such as the sex of the walker and the weight of the load that the walker may be carrying. In computer graphics renderings, viewers will notice even small inaccuracies in animated characters, particularly if they are intended to mimic human motion.</p>
<p>The term <em><a id="index_term1326"/>visual attention</em> covers a range of phenomenon from where we point our eyes to cognitive effects involving what we notice in a complex scene and how we interpret what we notice (Pashler, 1998). <a href="C24_chapter19.xhtml#f19_46">Figure 19.46</a> provides an example of how attentional processes affect vision, even for very simple images. In the left <span aria-label="566" epub:type="pagebreak" id="pg_566" role="doc-pagebreak"/>two panels, the one pattern differing in shape or color from the rest immediately “pops out” and is easily noticed. In the panel on the right, the one pattern differing in both shape and color is harder to find. The reason for this is that the visual system can do a parallel search for items distinguished by individual properties, but requires more cognitive, sequential search when looking for items that are indicated by the simultaneous presence of two distinguishing features. Graphically based human-computer interfaces should be (but often are not!) designed with an understanding of how to take advantage of visual attention processes in people so as to communicate important information quickly and effectively.</p>
<figure id="f19_46" tabindex="0">
<img alt="" src="../images/fig19_46.jpg"/>
<figcaption><p><span class="blue">Figure 19.46.</span> In (a) and (b), visual attention is quickly drawn to the item of different shape or color. In (c), sequential search appears to be necessary in order to find the one item that differs in both shape and color.</p></figcaption>
</figure>
</section>
</section>
<section>
<h2 id="sec19_5"><a id="index_term1372"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec19_5" role="doc-backlink"><span class="green">19.5 Picture Perception</span></a></h2>
<p>So far, this chapter has dealt with the visual perception that occurs when the world is directly imaged by the human eye. When we view the results of computer graphics, of course, we are looking at rendered images and not the real world. This has important perceptual implications. In principle, it should be possible to generate computer graphics that appear indistinguishable from the real world, at least for monocular viewing without either object or observer motion. Imagine looking out at the world through a glass window. Now, consider coloring each point on the window to exactly match the color of the world originally seen at that point. <a epub:type="noteref" href="C24_chapter19.xhtml#fn19_10" id="rfn19_10" role="doc-noteref"><sup>10</sup></a> The light reaching the eye is unchanged by this operation, meaning that perception should be the same whether the painted glass is viewed or the real world is viewed through the window. The goal of computer graphics can be thought of as producing the colored window without actually having the equivalent real-world view available.</p>
<p>The problem for computer graphics and other visual arts is that we can’t in practice match a view of the real world by coloring a flat surface. The brightness and dynamic range of light in the real world is impossible to re-create using any current display technology. Resolution of rendered images is also often less that the finest detail perceivable by human vision. Lightness and color constancy are much less apparent in pictures than in the real world, likely because the visual system attempts to compensate for variability in the brightness and color of the illumination based on the ambient illumination in the viewing environment, rather than the illumination associated with the rendered image. This is why the realistic appearance of color in photographs depends on film color balanced for the nature of the light source present when the photograph was taken and why <span aria-label="567" epub:type="pagebreak" id="pg_567" role="doc-pagebreak"/>realistic color in video requires a white-balancing step. While much is known about how limitations in resolution, brightness, and dynamic range affect the detectability of simple patterns, almost nothing is known about how these display properties affect spatial vision or object identification.</p>
<aside class="footnote1" epub:type="footnote" role="doc-footnote"><p><a href="#rfn19_10" id="fn19_10"><sup>10</sup></a> This idea was first described by the painter Leon Battista Alberti in 1435 and is now known as <em>Alberti’s Window</em>. It is closely related to the <em>camera obscura</em>.</p></aside>
<p class="indent">We have a better understanding of other aspects of this problem, which psychologists refer to as the perception of <em>pictorial space</em> (S. Rogers, 1995). One difference between viewing images and viewing the real world is that accommodation, binocular stereo, motion parallax, and perhaps other depth cues may indicate that the surface under view is much different from the distances in the world that it is intended to represent. The depths that are seen in such a situation tend to be somewhere between the depths indicated by the pictorial cues in the image and the distance to the image itself. When looking at a photograph or computer display, this often results in a sense of scale smaller than intended. On the other hand, seeing a movie in a big-screen theater produces a more compelling sense of spaciousness than does seeing the same movie on television, even if the distance to the TV is such that the visual angles are the same, since the movie screen is farther away.</p>
<p>Computer graphics rendered using perspective projection has a viewpoint, specified as a position and direction in model space, and a view frustum, which specifies the horizontal and vertical field of view and several other aspects of the viewing transform. If the rendered image is not viewed from the correct location, the visual angles to the borders of the image will not match the frustum used in creating the image. All visual angles within the image will be distorted as well, causing a distortion in all of the pictorial depth and orientation cues based on linear perspective. This effect occurs frequently in practice, when a viewer is positioned either too close or too far away from a photograph or display surface. If the viewer is too close, the perspective cues for depth will be compressed, and the cues for surface slant will indicate that the surface is closer to perpendicular to the line of sight than is actually the case. The situation is reversed if the viewer is too far from the photograph or screen. The situation is even more complicated if the line of sight does not go through the center of the viewing area, as is commonly the case in a wide variety of viewing situations.</p>
<p>The human visual system is able to partially compensate for perspective distortions arising from viewing an image at the wrong location, which is why we are able to sit in different seats at a movie theater and experience a similar sense of the depicted space. When controlling viewing position is particularly important, <em>viewing tubes</em> can be used. These are appropriately sized tubes, mounted in a fixed position relative to the display, and through which the viewer sees the display. The viewing tube constrains the observation point to the (hopefully) correct <span aria-label="568" epub:type="pagebreak" id="pg_568" role="doc-pagebreak"/>position. Viewing tubes are also quite effective at reducing the conflict in depth information between the pictorial cues in the image and the actual display surface. They eliminate both stereo and motion parallax, which, if present, would correspond to the display surface, not the rendered view. If they are small enough in diameter, they also reduce other cues to the location of the display surface by hiding the picture frame or edge of the display device. Exotic visually immersive display devices such as head-mounted displays (HMDs) go further in attempting to hide visual cues to the position of the display surface while adding binocular stereo and motion parallax consistent with the geometry of the world being rendered.</p>
</section>
</section>
</body>
</html>