<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:svg="http://www.w3.org/2000/svg" dir="ltr" lang="en" xml:lang="en">
<head>
<meta charset="UTF-8"/>
<title>23 Visualization</title>
<link href="../styles/9781000426359.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006665500" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter">
<section epub:type="chapter" role="doc-chapter">
<header>
<p class="chap-auz"><span class="green">Tamara Munzner</span></p>
<h1 class="chapz1" id="c23"><a id="index_term1388"/><a id="term-1097"/><span aria-label="645" epub:type="pagebreak" id="pg_645" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rc23" role="doc-backlink"><span class="green"><span class="big1">23</span><br/>Visualization</span></a></h1>
</header>
<p class="noindent1">A major application area of computer graphics is <em>visualization</em>, where computer-generated images are used to help people understand both spatial and nonspatial data. Visualization is used when the goal is to augment human capabilities in situations where the problem is not sufficiently well defined for a computer to handle algorithmically. If a totally automatic solution can completely replace human judgment, then visualization is not typically required. Visualization can be used to generate new hypotheses when exploring a completely unfamiliar dataset, to confirm existing hypotheses in a partially understood dataset, or to present information about a known dataset to another audience.</p>
<p>Visualization allows people to offload cognition to the perceptual system, using carefully designed images as a form of <em>external memory</em>. The human visual system is a very high-bandwidth channel to the brain, with a significant amount of processing occurring in parallel and at the pre-conscious level. We can thus use external images as a substitute for keeping track of things inside our own heads. For an example, let us consider the task of understanding the relationships between a subset of the topics in the splendid book <em>Gödel, Escher, Bach: The Eternal Golden Braid</em> (Hofstadter, 1979); see <a href="C28_chapter23.xhtml#f23_1">Figure 23.1</a>.</p>
<figure id="f23_1" tabindex="0">
<img alt="" src="../images/fig23_1.jpg"/>
<figcaption><p><span class="blue">Figure 23.1.</span> Keeping track of relationships between topics is difficult using a <a id="index_term1451"/>text list.</p></figcaption>
</figure>
<p>When we see the dataset as a text list, at the low level we must read words and compare them to memories of previously read words. It is hard to keep track of just these dozen topics using cognition and <a id="index_term1432"/>memory alone, let alone the hundreds of topics in the full book. The higher-level problem of identifying neighborhoods, for instance finding all the topics two hops away from the target topic <em>Paradoxes</em>, is very difficult.</p>
<p><a id="term-1098"/><a id="term-1129"/><span aria-label="646" epub:type="pagebreak" id="pg_646" role="doc-pagebreak"/><a href="C28_chapter23.xhtml#f23_2">Figure 23.2</a> shows an external visual representation of the same dataset as a node-link graph, where each topic is a node and the linkage between two topics is shown directly with a line. Following the lines by moving our eyes around the image is a fast low-level operation with minimal cognitive load, so higher-level neighborhood finding becomes possible. The placement of the nodes and the routing of the links between them was created automatically by the <em>dot</em> graph drawing program (Gansner, Koutsofois, North, &amp; Vo, 1993).</p>
<figure id="f23_2" tabindex="0">
<img alt="" src="../images/fig23_2.jpg"/>
<figcaption><p><span class="blue">Figure 23.2.</span> Substituting perception for cognition and memory allows us to understand relationships between book topics quickly.</p></figcaption>
</figure>
<p>We call the mapping of dataset attributes to a visual representation a <em>visual encoding</em>. One of the central problems in visualization is choosing appropriate encodings from the enormous space of possible visual representations, taking into account the characteristics of the human perceptual system, the dataset in question, and the task at hand.</p>
<section>
<h2 id="sec23_1"><a id="term-1079"/><span aria-label="647" epub:type="pagebreak" id="pg_647" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec23_1" role="doc-backlink"><span class="green">23.1 Background</span></a></h2>
<section>
<h3 id="sec23_1_1"><span class="green">23.1.1 History</span></h3>
<p>People have a long history of conveying meaning through static images, dating back to the oldest known cave paintings from over thirty thousand years ago. We continue to visually communicate today in ways ranging from rough sketches on the back of a napkin to the slick graphic design of advertisements. For thousands of years, cartographers have studied the problem of making maps that represent some aspect of the world around us. The first visual representations of abstract, nonspatial datasets were created in the 18th century by William Playfair (Friendly, 2008).</p>
<p>Although we have had the power to create moving images for over one hundred and fifty years, creating dynamic images interactively is a more recent development only made possible by the widespread availability of fast computer graphics hardware and algorithms in the past few decades. Static visualizations of tiny datasets can be created by hand, but computer graphics enables interactive visualization of large datasets.</p>
</section>
<section>
<h3 id="sec23_1_2"><a id="index_term1438"/><span class="green">23.1.2 Resource Limitations</span></h3>
<p>When designing a visualization system, we must consider three different kinds of limitations: computational capacity, human perceptual and cognitive capacity, and display capacity.</p>
<p>As with any application of computer graphics, computer time and memory are limited resources and we often have hard constraints. If the visualization system needs to deliver interactive response, then it must use algorithms that can run in a fraction of a second rather than minutes or hours.</p>
<p>On the human side, memory and attention must be considered as finite resources. Human memory is notoriously limited, both for long-term recall and for shorter-term working memory. In <a href="C28_chapter23.xhtml#sec23_4">Section 23.4</a>, we discuss some of the power and limitations of the low-level visual attention mechanisms that carry out massively parallel processing of the visual field. We store surprisingly little information internally in visual working memory, leaving us vulnerable to <em>change blindness</em>, the phenomenon where even very large changes are not noticed if we are attending to something else in our view (Simons, 2000). Moreover, vigilance is also a highly limited resource; our ability to perform visual search tasks degrades quickly, with far worse results after several hours than in the first few minutes (Ware, 2000).</p>
<p><span aria-label="648" epub:type="pagebreak" id="pg_648" role="doc-pagebreak"/>Display capacity is a third kind of limitation to consider. Visualization designers often “run out of pixels,” where the resolution of the screen is not large enough to show all desired information simultaneously. The <em>information density</em> of a particular frame is a measure of the amount of information encoded versus the amount of unused space. There is a tradeoff between the benefits of showing as much as possible at once, to minimize the need for navigation and exploration, and the costs of showing too much at once, where the user is overwhelmed by visual clutter.</p>
</section>
</section>
<section>
<h2 id="sec23_2"><a id="index_term1404"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec23_2" role="doc-backlink"><span class="green">23.2 Data Types</span></a></h2>
<p>Many aspects of a visualization design are driven by the type of the data that we need to look at. For example, is it a table of numbers, or a set of relations between items, or inherently spatial data such as a location on the Earth’s surface or a collection of documents?</p>
<p>We start by considering a table of data. We call the rows <em>items</em> of data and the columns are <em>dimensions</em>, also known as <em>attributes</em>. For example, the rows might represent people, and the columns might be names, age, height, shirt size, and favorite fruit.</p>
<p>We distinguish between three types of dimensions: quantitative, ordered, and categorical. <em>Quantitative</em> data, such as age or height, is numerical and we can do arithmetic on it. For example, the quantity of 68 inches minus 42 inches is 26 inches. With <em>ordered</em> data, such as shirt size, we cannot do full-fledged arithmetic, but there is a well-defined ordering. For example, large minus medium is not a meaningful concept, but we know that medium falls between small and large. <em>Categorical</em> data, such as favorite fruit or names, does not have an implicit ordering. We can only distinguish whether two things are the same (apples) or different (apples vs. bananas).</p>
<p>Relational data, or <em>graphs</em>, are another data type where <em>nodes</em> are connected by <em>links</em>. One specific kind of graph is a <em>tree</em>, which is typically used for hierarchical data. Both nodes and edges can have associated attributes. The word <em>graph</em> is unfortunately overloaded in visualization. The node-link graphs we discuss here, following the terminology of graph drawing and graph theory, could also be called <em>networks</em>. In the field of statistical graphics, graph is often used for <em>chart</em>, as in the line charts for time-series data shown in <a href="C28_chapter23.xhtml#f23_10">Figure 23.10</a>.</p>
<p>Some data is inherently spatial, such as geographic location or a field of measurements at positions in three-dimensional space as in the MRI or CT scans used by doctors to see the internal structure of a person’s body. The information associated <span aria-label="649" epub:type="pagebreak" id="pg_649" role="doc-pagebreak"/>with each point in space may be an unordered set of scalar quantities, or indexed vectors, or tensors. In contrast, nonspatial data can be visually encoded using spatial position, but that encoding is chosen by the designer rather than given implicitly in the semantics of the dataset itself. This choice is one of the most central and difficult problems of visualization design.</p>
<section>
<h3 id="sec23_2_1"><a id="index_term1430"/><span class="green">23.2.1 Dimension and Item Count</span></h3>
<p>The number of data dimensions that need to be visually encoded is one of the most fundamental aspects of the visualization design problem. Techniques that work for a low-dimensional dataset with a few columns will often fail for very high-dimensional datasets with dozens or hundreds of columns. A data dimension may have hierarchical structure, for example with a time series dataset where there are interesting patterns at multiple temporal scales.</p>
<p>The number of data items is also important: a visualization that performs well for a few hundred items often does not scale to millions of items. In some cases the difficulty is purely algorithmic, where a computation would take too long; in others it is an even deeper perceptual problem that even an instantaneous algorithm could not solve, where visual clutter makes the representation unusable by a person. The range of possible values within a dimension may also be relevant.</p>
</section>
<section>
<h3 id="sec23_2_2"><a id="index_term1403"/><a id="index_term1406"/><span class="green">23.2.2 Data Transformation and Derived Dimensions</span></h3>
<p>Data is often transformed from one type to another as part of a visualization pipeline for solving the domain problem. For example, an original data dimension might be made up of quantitative data: floating point numbers that represent temperature. For some tasks, like finding anomalies in local weather patterns, the raw data might be used directly. For another task, like deciding whether water is an appropriate temperature for a shower, the data might be transformed into an ordered dimension: hot, warm, or cold. In this transformation, most of the detail is aggregated away. In a third example, when making toast, an even more lossy transformation into a categorical dimension might suffice: burned or not burned.</p>
<p>The principle of transforming data into <em>derived <a id="index_term1411"/>dimensions</em>, rather than simply visually encoding the data in its original form, is a powerful idea. In <a href="C28_chapter23.xhtml#f23_10">Figure 23.10</a>, the original data was an ordered collection of time-series curves. The transformation was to cluster the data, reducing the amount of information to visually encode to a few highly meaningful curves.</p>
</section>
</section>
<section>
<h2 id="sec23_3"><a id="index_term1407"/><a id="term-1089"/><a id="term-1111"/><a id="term-1132"/><span aria-label="650" epub:type="pagebreak" id="pg_650" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec23_3" role="doc-backlink"><span class="green">23.3 Human-Centered Design Process</span></a></h2>
<p>The visualization design process can be split into a cascading set of layers, as shown in <a href="C28_chapter23.xhtml#f23_3">Figure 23.3</a>. These layers all depend on each other; the output of the level above is input into the level below.</p>
<figure id="f23_3" tabindex="0">
<img alt="" src="../images/fig23_3.jpg"/>
<figcaption><p><span class="blue">Figure 23.3.</span> Four nested layers of validation for visualization.</p></figcaption>
</figure>
<section>
<h3 id="sec23_3_1"><a id="index_term1448"/><span class="green">23.3.1 Task Characterization</span></h3>
<p>A given dataset has many possible visual encodings. Choosing which visual encoding to use can be guided by the specific needs of some intended user. Different questions, or <em>tasks</em>, require very different visual encodings. For example, consider the domain of software engineering. The task of understanding the coverage of a test suite is well supported by the Tarantula interface shown in <a href="C28_chapter23.xhtml#f23_11">Figure 23.11</a>. However, the task of understanding the modular decomposition of the software while refactoring the code might be better served by showing its hierarchical structure more directly as a node-link graph.</p>
<p>Understanding the requirements of some target audience is a tricky problem. In a human-centered design approach, the visualization designer works with a group of target users over time (C. Lewis &amp; Rieman, 1993). In most cases, users know they need to somehow view their data but cannot directly articulate their needs as clear-cut tasks in terms of operations on data types. The iterative design process includes gathering information from the target users about their problems through interviews and observation of them at work, creating prototypes, and observing how users interact with those prototypes to see how well the proposed solution actually works. The software engineering methodology of requirements analysis can also be useful (Kovitz, 1999).</p>
</section>
<section>
<h3 id="sec23_3_2"><a id="index_term1391"/><span class="green">23.3.2 Abstraction</span></h3>
<p>After the specific domain problem has been identified in the first layer, the next layer requires abstracting it into a more generic representation as operations on <a id="term-1128"/><span aria-label="651" epub:type="pagebreak" id="pg_651" role="doc-pagebreak"/>the data types discussed in the previous section. Problems from very different domains can map to the same visualization abstraction. These generic operations include sorting, filtering, characterizing trends and distributions, finding anomalies and outliers, and finding correlation (Amar, Eagan, &amp; Stasko, 2005). They also include operations that are specific to a particular data type, for example following a path for relational data in the form of graphs or trees.</p>
<p>This abstraction step often involves data transformations from the original raw data into derived dimensions. These derived dimensions are often of a different type than the original data: a graph may be converted into a tree, tabular data may be converted into a graph by using a threshold to decide whether a link should exist based on the field values, and so on.</p>
</section>
<section>
<h3 id="sec23_3_3"><a id="index_term1394"/><span class="green">23.3.3 Technique and Algorithm Design</span></h3>
<p>Once an abstraction has been chosen, the next layer is to design appropriate visual encoding and interaction techniques. <a href="C28_chapter23.xhtml#sec23_4">Section 23.4</a> covers the principles of visual encoding, and we discuss interaction principles in <a href="C28_chapter23.xhtml#sec23_5">Sections 23.5</a>. We present techniques that take these principles into account in <a href="C28_chapter23.xhtml#sec23_6">Sections 23.6</a> and <a href="C28_chapter23.xhtml#sec23_7">23.7</a>.</p>
<p>A detailed discussion of visualization algorithms is unfortunately beyond the scope of this chapter.</p>
</section>
<section>
<h3 id="sec23_3_4"><a id="index_term1453"/><span class="green">23.3.4 Validation</span></h3>
<p>Each of the four layers has different validation requirements.</p>
<p>The first layer is designed to determine whether the problem is correctly characterized: is there really a target audience performing particular tasks that would benefit from the proposed tool? An immediate way to test assumptions and conjectures is to observe or interview members of the target audience, to ensure that the visualization designer fully understands their tasks. A measurement that cannot be done until a tool has been built and deployed is to monitor its adoption rate within that community, although of course many other factors in addition to utility affect adoption.</p>
<p>The next layer is used to determine whether the abstraction from the domain problem into operations on specific data types actually solves the desired problem. After a prototype or finished tool has been deployed, a field study can be carried out to observe whether and how it is used by its intended audience. Also, images produced by the system can be analyzed both qualitatively and quantitatively.</p>
<p>The purpose of the third layer is to verify that the visual encoding and interaction techniques chosen by the designer effectively communicate the chosen <a id="term-1133"/><span aria-label="652" epub:type="pagebreak" id="pg_652" role="doc-pagebreak"/>abstraction to the users. An immediate test is to justify that individual design choices do not violate known perceptual and cognitive principles. Such a justification is necessary but not sufficient, since visualization design involves many tradeoffs between interacting choices. After a system is built, it can be tested through formal laboratory studies where many people are asked to do assigned tasks so that measurements of the time required for them to complete the tasks and their error rates can be statistically analyzed.</p>
<p>A fourth layer is employed to verify that the algorithm designed to carry out the encoding and interaction choices is faster or takes less memory than previous algorithms. An immediate test is to analyze the computational complexity of the proposed algorithm. After implementation, the actual time performance and memory usage of the system can be directly measured.</p>
</section>
</section>
<section>
<h2 id="sec23_4"><a id="index_term1454"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec23_4" role="doc-backlink"><span class="green">23.4 Visual Encoding Principles</span></a></h2>
<p>We can describe visual encodings as graphical elements, called <em>marks</em>, that convey information through visual channels. A zero-dimensional mark is a point, a one-dimensional mark is a line, a two-dimensional mark is an area, and a three-dimensional mark is a volume. Many <em>visual channels</em> can encode information, including spatial position, color, size, shape, orientation, and direction of motion. Multiple visual channels can be used to simultaneously encode different data dimensions; for example, <a href="C28_chapter23.xhtml#f23_4">Figure 23.4</a> shows the use of horizontal and vertical spatial position, color, and size to display four data dimensions. More than one channel can be used to redundantly code the same dimension, for a design that displays less information but shows it more clearly.</p>
<figure id="f23_4" tabindex="0">
<img alt="" src="../images/fig23_4.jpg"/>
<figcaption><p><span class="blue">Figure 23.4.</span> The four visual channels of horizontal and vertical spatial position, color, and size are used to encode information in this scatterplot chart <em>Image courtesy George Robertson</em> (Robertson, Fernandez, Fisher, Lee, &amp; Stasko, 2008), © IEEE 2008.</p></figcaption>
</figure>
<section>
<h3 id="sec23_4_1"><a id="index_term1327"/><a id="index_term1397"/><span class="green">23.4.1 Visual Channel Characteristics</span></h3>
<p>Important characteristics of visual channels are distinguishability, separability, and popout.</p>
<p>Channels are not all equally distinguishable. Many psychophysical experiments have been carried out to measure the ability of people to make precise distinctions about information encoded by the different visual channels. Our abilities depend on whether the data type is quantitative, ordered, or categorical. <a href="C28_chapter23.xhtml#f23_5">Figure 23.5</a> shows the rankings of visual channels for the three data types. <a href="C28_chapter23.xhtml#f23_6">Figure 23.6</a> shows some of the default mappings for visual channels in the Tableau/Polaris system, which take into account the data type.</p>
<figure id="f23_5" tabindex="0">
<img alt="" src="../images/fig23_5.jpg"/>
<figcaption><p><span class="blue">Figure 23.5.</span> Our ability to perceive information encoded by a visual channel depends on the type of data used, from most accurate at the top to least at the bottom. <em>Redrawn and adapted from (Mackinlay, 1986)</em>.</p></figcaption>
</figure>
<figure id="f23_6" tabindex="0">
<img alt="" src="../images/fig23_6.jpg"/>
<figcaption><p><span class="blue">Figure 23.6.</span> The Tableau/Polaris system default mappings for four visual channels according to data type. <em>Image courtesy Chris Stolte</em> (Stolte, Tang, &amp; Hanrahan, 2008), © 2008 IEEE.</p></figcaption>
</figure>
<p><a id="term-1002"/><a id="term-1080"/><a id="term-1088"/><span aria-label="653" epub:type="pagebreak" id="pg_653" role="doc-pagebreak"/><a id="term-128"/><a id="term-129"/><a id="term-1003"/><a id="term-1081"/><a id="term-1082"/><a id="term-1119"/><span aria-label="654" epub:type="pagebreak" id="pg_654" role="doc-pagebreak"/>Spatial position is the most accurate visual channel for all three types of data, and it dominates our perception of a visual encoding. Thus, the two most important data dimensions are often mapped to horizontal and vertical spatial positions.</p>
<p>However, the other channels differ strongly between types. The channels of length and angle are highly discriminable for quantitative data but poor for ordered and categorical, while in contrast hue is very accurate for categorical data but mediocre for quantitative data.</p>
<p>We must always consider whether there is a good match between the dynamic range necessary to show the data dimension and the dynamic range available in the channel. For example, encoding with line width uses a one-dimensional mark and the size channel. There are a limited number of width steps that we can reliably use to visually encode information: a minimum thinness of one pixel is enforced by the screen resolution (ignoring antialiasing to simplify this discussion), and there is a maximum thickness beyond which the object will be perceived as a polygon rather than a line. Line width can work very well to show three or four different values in a data dimension, but it would be a poor choice for dozens or hundreds of values.</p>
<p>Some visual channels are <em>integral</em>, fused together at a pre-conscious level, so they are not good choices for visually encoding different data dimensions. Others are <em>separable</em>, without interactions between them during visual processing, and are safe to use for encoding multiple dimensions. <a href="C28_chapter23.xhtml#f23_7">Figure 23.7</a> shows two channel pairs. <a id="index_term1398"/>Color and <a id="index_term1436"/>position are highly separable. We can see that horizontal size and vertical size are not so easy to separate, because our visual system automatically integrates these together into a unified perception of area. Size interacts with many channels: as the size of an object grows smaller, it becomes more difficult to distinguish its shape or color.</p>
<figure id="f23_7" tabindex="0">
<img alt="" src="../images/fig23_7.jpg"/>
<figcaption><p><span class="blue">Figure 23.7.</span> Color and location are separable channels well suited to encode different data dimensions, but the horizontal size and and vertical size channels are automatically fused into an integrated perception of area. <em>Redrawn after (Ware, 2000).</em></p></figcaption>
</figure>
<p><a id="term-150"/><span aria-label="655" epub:type="pagebreak" id="pg_655" role="doc-pagebreak"/>We can selectively attend to a channel so that items of a particular type “pop out” visually, as discussed in <a href="C24_chapter19.xhtml#sec19_4_3">Section 19.4.3</a>. An example of visual popout is when we immediately spot the red item amidst a sea of blue ones, or distinguish the circle from the squares. Visual popout is powerful and scalable because it occurs in parallel, without the need for conscious processing of the items one by one. Many visual channels have this popout property, including not only the list above but also curvature, flicker, stereoscopic depth, and even the direction of lighting. However, in general we can only take advantage of popout for one channel at a time. For example, a white circle does not pop out from a group of circles and squares that can be white or black, as shown in <a href="C24_chapter19.xhtml#f19_46">Figure 19.46</a>. When we need to search across more than one channel simultaneously, the length of time it takes to find the target object depends linearly on the number of objects in the scene.</p>
</section>
<section>
<h3 id="sec23_4_2"><span class="green">23.4.2 Color</span></h3>
<p>Color can be a very powerful channel, but many people do not understand its properties and use it improperly. As discussed in <a href="C24_chapter19.xhtml#sec19_2_2">Section 19.2.2</a>, we can consider color in terms of three separate visual channels: hue, saturation, and lightness. Region size strongly affects our ability to sense color. Color in small regions is relatively difficult to perceive, and designers should use bright, highly saturated colors to ensure that the color coding is distinguishable. The inverse situation is true when colored regions are large, as in backgrounds, where low saturation pastel colors should be used to avoid blinding the viewer.</p>
<p>Hue is a very strong cue for encoding categorical data. However, the available dynamic range is very limited. People can reliably distinguish only around a dozen hues when the colored regions are small and scattered around the display. A good guideline for color coding is to keep the number of categories less than eight, keeping in mind that the background and the neutral object color also count in the total.</p>
<p>For ordered data, lightness and saturation are effective because they have an implicit perceptual ordering. People can reliably order by lightness, always placing gray in between black and white. With saturation, people reliably place the less saturated pink between fully saturated red and zero-saturation white. However, hue is not as as good a channel for ordered data because it does not have an implicit perceptual ordering. When asked to create an ordering of red, blue, green, and yellow, people do not all give the same answer. People can and do learn conventions, such as green-yellow-red for traffic lights, or the order of colors in the rainbow, but these constructions are at a higher level than pure perception. Ordered data is typically shown with a discrete set of color values.</p>
<p><a id="term-114"/><a id="term-130"/><a id="term-151"/><a id="term-1083"/><a id="term-1084"/><span aria-label="656" epub:type="pagebreak" id="pg_656" role="doc-pagebreak"/>Quantitative data is shown with a <em><a id="index_term194"/>colormap</em>, a range of color values that can be continuous or discrete. A very unfortunate default in many software packages is the rainbow colormap, as shown in <a href="C28_chapter23.xhtml#f23_8">Figure 23.8</a>. The standard rainbow scale suffers from three problems. First, hue is used to indicate order. A better choice would be to use lightness because it has an implicit perceptual ordering. Even more importantly, the human eye responds most strongly to luminance. Second, the scale is not perceptually linear: equal steps in the continuous range are not perceived as equal steps by our eyes. <a href="C28_chapter23.xhtml#f23_8">Figure 23.8</a> shows an example, where the rainbow colormap obfuscates the data. While the range from <em>–</em>2000 to <em>–</em>1000 has three distinct colors (cyan, green, and yellow), a range of the same size from –1000 to 0 simply looks yellow throughout. The graphs on the right show that the perceived value is strongly tied to the luminance, which is not even monotonically increasing in this scale.</p>
<figure id="f23_8" tabindex="0">
<img alt="" src="../images/fig23_8.jpg"/>
<figcaption><p><span class="blue">Figure 23.8.</span> The standard rainbow colormap has two defects: it uses hue to denote ordering, and it is not perceptually isolinear. <em>Image courtesy Bernice Rogowitz.</em></p></figcaption>
</figure>
<p>In contrast, <a href="C28_chapter23.xhtml#f23_9">Figure 23.9</a> shows the same data with a more appropriate colormap, where the lightness increases monotonically. Hue is used to create a semantically meaningful categorization: the viewer can discuss structure in the dataset, such as the dark blue sea, the cyan continental shelf, the green lowlands, and the white mountains.</p>
<figure id="f23_9" tabindex="0">
<img alt="" src="../images/fig23_9.jpg"/>
<figcaption><p><span class="blue">Figure 23.9.</span> The structure of the same dataset is far more clear with a colormap where monotonically increasing lightness is used to show ordering and hue is used instead for segmenting into categorical regions. <em>Image courtesy Bernice Rogowitz.</em></p></figcaption>
</figure>
<p>In both the discrete and continuous cases, <a id="index_term1399"/>colormaps should take into account whether the data is sequential or diverging. The ColorBrewer application (<a href="http://www.colorbrewer.org"><span class="monospace">www.colorbrewer.org</span></a>) is an excellent resource for colormap construction (Brewer, 1999).</p>
<p>Another important issue when encoding with color is that a significant fraction of the population, roughly 10% of men, is red-green color deficient. If a coding using red and green is chosen because of conventions in the target domain, redundantly <a id="term-115"/><a id="term-152"/><a id="term-565"/><a id="term-888"/><a id="term-988"/><a id="term-1085"/><a id="term-1124"/><span aria-label="657" epub:type="pagebreak" id="pg_657" role="doc-pagebreak"/>coding lightness or saturation in addition to hue is wise. Tools such as the website <a href="http://www.vischeck.com"><span class="monospace">http://www.vischeck.com</span></a> should be used to check whether a color scheme is distinguishable to people with color deficient vision.</p>
</section>
<section>
<h3 id="sec23_4_3"><a id="index_term1444"/><span class="green">23.4.3 2D vs. 3D Spatial Layouts</span></h3>
<p>The question of whether to use two or three channels for spatial position has been extensively studied. When computer-based visualization began in the late 1980s, and interactive 3D graphics was a new capability, there was a lot of enthusiasm for 3D representations. As the field matured, researchers began to understand the costs of 3D approaches when used for abstract datasets (Ware, 2001).</p>
<p><em>Occlusion</em>, where some parts of the dataset are hidden behind others, is a major problem with 3D. Although hidden surface removal algorithms such as z-buffers and BSP trees allow fast computation of a correct 2D image, people must still synthesize many of these images into an internal mental map. When people look at realistic scenes made from familiar objects, usually they can quickly understand what they see. However, when they see an unfamiliar dataset, where a chosen visual encoding maps abstract dimensions into spatial positions, understanding the details of its 3D structure can be challenging even when they can use interactive navigation controls to change their 3D viewpoint. The reason is once again the limited capacity of human working memory (Plumlee &amp; Ware, 2006).</p>
<p><a id="term-1117"/><span aria-label="658" epub:type="pagebreak" id="pg_658" role="doc-pagebreak"/>Another problem with 3D is <em>perspective distortion</em>. Although real-world objects do indeed appear smaller when they are further from our eyes, foreshortening makes direct comparison of object heights difficult (Tory, Kirkpatrick, Atkins, &amp; Möller, 2006). Once again, although we can often judge the heights of familiar objects in the real world based on past experience, we cannot necessarily do so with completely abstract data that has a visual encoding where the height conveys meaning. For example, it is more difficult to judge bar heights in a 3D bar chart than in multiple horizontally aligned 2D bar charts.</p>
<p>Another problem with unconstrained 3D representations is that text at arbitrary orientations in 3D space is far more difficult to read than text aligned in the 2D image plane (Grossman, Wigdor, &amp; Balakrishnan, 2007).</p>
<p><a href="C28_chapter23.xhtml#f23_10">Figure 23.10</a> illustrates how carefully chosen 2D views of an abstract dataset can avoid the problems with occlusion and perspective distortion inherent in 3D views. The top view shows a 3D representation created directly from the original time-series data, where each cross-section is a 2D time-series curve showing power consumption for one day, with one curve for each day of the year along the extruded third axis. Although this representation is straightforward to create, we can only see large-scale patterns such as the higher consumption during working hours and the seasonal variation between winter and summer. To create the 2D linked views at the bottom, the curves were hierarchically clustered, and only aggregate curves representing the top clusters are drawn superimposed in the same 2D frame. Direct comparison between the curve heights at all times of the day is easy because there is no perspective distortion or occlusion. The same color coding is used in the calendar view, which is very effective for understanding temporal patterns.</p>
<figure id="f23_10" tabindex="0">
<img alt="" src="../images/fig23_10.jpg"/>
<figcaption><p><span class="blue">Figure 23.10.</span> Top: A 3D representation of this time series dataset introduces the problems of occlusion and <a id="index_term1435"/>perspective distortion. Bottom: The linked 2D views of derived aggregate curves and the calendar allow direct comparison and show more fine-grained patterns. <em>Image courtesy Jarke van Wijk</em> (van Wijk &amp; van Selow, 1999), © 1999 IEEE.</p></figcaption>
</figure>
<p>In contrast, if a dataset consists of inherently 3D spatial data, such as showing fluid flow over an airplane wing or a medical imaging dataset from an MRI scan, then the costs of a 3D view are outweighed by its benefits in helping the user construct a useful mental model of the dataset structure.</p>
</section>
<section>
<h3 id="sec23_4_4"><a id="index_term1450"/><span class="green">23.4.4 Text Labels</span></h3>
<p>Text in the form of labels and legends is a very important factor in creating visualizations that are useful rather than simply pretty. Axes and tick marks should be labeled. Legends should indicate the meaning of colors, whether used as discrete patches or in continuous color ramps. Individual items in a dataset typically have meaningful text labels associated with them. In many cases showing all labels at all times would result in too much visual clutter, so labels can be shown for a subset of the items using label positioning algorithms that show labels at a desired density while avoiding overlap (Luboschik, Schumann, &amp; Cords, 2008). A straightforward way to choose the best label to represent a group of items is to use a greedy algorithm based on some measure of label importance, but synthesizing a new label based on the characteristics of the group remains a difficult problem. A more interaction-centric approach is to only show labels for individual items based on an interactive indication from the user.<a id="term-597"/><a id="term-1118"/><span aria-label="659" epub:type="pagebreak" id="pg_659" role="doc-pagebreak"/></p>
</section>
</section>
<section>
<h2 id="sec23_5"><a id="index_term1428"/><a id="term-1134"/><span aria-label="660" epub:type="pagebreak" id="pg_660" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec23_5" role="doc-backlink"><span class="green">23.5 Interaction Principles</span></a></h2>
<p>Several principles of interaction are important when designing a visualization. Low-latency visual feedback allows users to explore more fluidly, for example by showing more detail when the cursor simply hovers over an object rather than requiring the user to explicitly click. Selecting items is a fundamental operation when interacting with large datasets, as is visually indicating the selected set with highlighting. Color coding is a common form of highlighting, but other channels can also be used.</p>
<p>Many forms of interaction can be considered in terms of what aspect of the display they change. Navigation can be considered a change of viewport. Sorting is a change to the spatial ordering; that is, changing how data is mapped to the spatial position visual channel. The entire visual encoding can also be changed.</p>
<section>
<h3 id="sec23_5_1"><span class="green">23.5.1 Overview First, Zoom and Filter, Details on Demand</span></h3>
<p>The influential mantra “Overview first, zoom and filter, <a id="index_term1408"/>details on demand”(Shneiderman, 1996) elucidates the role of interaction and navigation in visualization design. Overviews help the user notice regions where further investigation might be productive, whether through spatial navigation or through <a id="index_term1416"/>filtering. As we discuss below, details can be presented in many ways: with popups from clicking or cursor hovering, in a separate window, and by changing the layout on the fly to make room to show additional information.</p>
</section>
<section>
<h3 id="sec23_5_2"><a id="index_term1429"/><span class="green">23.5.2 Interactivity Costs</span></h3>
<p>Interactivity has both power and cost. The benefit of interaction is that people can explore a larger information space than can be understood in a single static image. However, a cost to interaction is that it requires human time and attention. If the user must exhaustively check every possibility, use of the <a id="index_term1389"/>visualization system <a id="term-184"/><a id="term-1087"/><span aria-label="661" epub:type="pagebreak" id="pg_661" role="doc-pagebreak"/>may degenerate into human-powered search. Automatically detecting features of interest to explicitly bring to the user’s attention via the visual encoding is a useful goal for the visualization designer. However, if the task at hand could be completely solved by automatic means, there would be no need for a visualization in the first place. Thus, there is always a tradeoff between finding automatable aspects and relying on the human in the loop to detect patterns.</p>
</section>
<section>
<h3 id="sec23_5_3"><span class="green">23.5.3 Animation</span></h3>
<p>Animation shows change using time. We distinguish animation, where successive frames can only be played, paused, or stopped, from true interactive control. There is considerable evidence that animated transitions can be more effective than jump cuts, by helping people track changes in object positions or camera viewpoints (Heer &amp; Robertson, 2007). Although animation can be very effective for narrative and storytelling, it is often used ineffectively in a <a id="index_term223"/>visualization context (Tversky, Morrison, &amp; Betrancourt, 2002). It might seem obvious to show data that changes over time by using animation, a visual modality that changes over time. However, people have difficulty in making specific comparisons between individual frames that are not contiguous when they see an animation consisting of many frames. The very limited capacity of human visual memory means that we are much worse at comparing memories of things that we have seen in the past than at comparing things that are in our current field of view. For tasks requiring comparison between up to several dozen frames, side-by-side comparison is often more effective than animation. Moreover, if the number of objects that change between frames is large, people will have a hard time tracking everything that occurs (Robertson et al., 2008). Narrative animations are carefully designed to avoid having too many actions occurring simultaneously, whereas a dataset being visualized has no such constraint. For the special case of just two frames with a limited amount of change, the very simple <a id="index_term1395"/>animation of flipping back and forth between the two can be a useful way to identify the differences between them.</p>
</section>
</section>
<section>
<h2 id="sec23_6"><a id="index_term1392"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec23_6" role="doc-backlink"><span class="green">23.6 Composite and Adjacent Views</span></a></h2>
<p>A very fundamental visual encoding choice is whether to have a single composite <em>view</em> showing everything in the same frame or window, or to have multiple views adjacent to each other.</p>
<section>
<h3 id="sec23_6_1"><a id="index_term1440"/><a id="term-1121"/><span aria-label="662" epub:type="pagebreak" id="pg_662" role="doc-pagebreak"/><span class="green">23.6.1 Single Drawing</span></h3>
<p>When there are only one or two data dimensions to encode, then horizontal and vertical spatial position are the obvious visual channel to use, because we perceive them most accurately and position has the strongest influence on our internal mental model of the dataset. The traditional statistical graphics displays of line charts, bar charts, and scatterplots all use spatial ordering of marks to encode information. These displays can be augmented with additional visual channels, such as color and size and shape, as in the scatterplot shown in <a href="C28_chapter23.xhtml#f23_4">Figure 23.4</a>.</p>
<p>The simplest possible mark is a single pixel. In <em>pixel-oriented</em> displays, the goal is to provide an overview of as many items as possible. These approaches use the spatial position and color channels at a high information density, but preclude the use of the size and shape channels. <a href="C28_chapter23.xhtml#f23_11">Figure 23.11</a> shows the Tarantula software visualization tool (Jones et al., 2002), where most of the screen is devoted to an overview of source code using one-pixel high lines (Eick, Steffen, &amp; Sumner, 1992). The color and brightness of each line shows whether it passed, failed, or had mixed results when executing a suite of test cases.</p>
<figure id="f23_11" tabindex="0">
<img alt="" src="../images/fig23_11.jpg"/>
<figcaption><p><span class="blue">Figure 23.11.</span> Tarantula shows an overview of source code using one-pixel lines color coded by execution status of a software test suite. <em>Image courtesy John Stasko</em> (Jones, Harrold, &amp; Stasko, 2002).</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec23_6_2"><a id="term-1105"/><a id="term-1113"/><a id="term-1114"/><a id="term-1126"/><span aria-label="663" epub:type="pagebreak" id="pg_663" role="doc-pagebreak"/><span class="green">23.6.2 Superimposing and Layering</span></h3>
<p>Multiple items can be superimposed in the same frame when their spatial position is compatible. Several lines can be shown in the same line chart, and many dots in the same scatterplot, when the axes are shared across all items. One benefit of a single shared view is that comparing the position of different items is very easy. If the number of items in the dataset is limited, then a single view will often suffice. Visual layering can extend the usefulness of a single view when there are enough items that visual clutter becomes a concern. <a href="C28_chapter23.xhtml#f23_12">Figure 23.12</a> shows how a redundant combination of the size, saturation, and brightness channels serves to distinguish a foreground layer from a background layer when the user moves the cursor over a block of words.</p>
<figure id="f23_12" tabindex="0">
<img alt="" src="../images/fig23_12.jpg"/>
<figcaption><p><span class="blue">Figure 23.12.</span> Visual layering with size, saturation, and brightness in the Constellation system (Munzner, 2000).</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec23_6_3"><a id="index_term1423"/><span class="green">23.6.3 Glyphs</span></h3>
<p>We have been discussing the idea of visual encoding using simple marks, where a single mark can only have one value for each visual channel used. With more complex marks, which we will call <em>glyphs</em>, there is internal structure where sub-regions have different visual channel encodings.</p>
<p><a id="term-344"/><a id="term-1106"/><span aria-label="664" epub:type="pagebreak" id="pg_664" role="doc-pagebreak"/>Designing appropriate glyphs has the same challenges as designing visual encodings. <a href="C28_chapter23.xhtml#f23_13">Figure 23.13</a> shows a variety of glyphs, including the notorious faces originally proposed by Chernoff. The danger of using faces to show abstract data dimensions is that our perceptual and emotional response to different facial features is highly nonlinear in a way that is not fully understood, but the variability is greater than between the visual channels that we have discussed so far. We are probably far more attuned to features that indicate emotional state, such as eyebrow orientation, than other features, such as nose size or face shape.</p>
<figure id="f23_13" tabindex="0">
<img alt="" src="../images/fig23_13.jpg"/>
<figcaption><p><span class="blue">Figure 23.13.</span> Complex marks, which we call <em>glyphs</em>, have subsections that visually encode different data dimensions. <em>Image courtesy Matt Ward</em> (M. O. Ward, 2002).</p></figcaption>
</figure>
<p>Complex glyphs require significant display area for each glyph, as shown in <a href="C28_chapter23.xhtml#f23_14">Figure 23.14</a> where miniature bar charts show the value of four different dimensions at many points along a spiral path. Simpler glyphs can be used to create a global visual texture, the glyph size is so small that individual values cannot be read out without zooming, but region boundaries can be discerned from the overview level. <a href="C28_chapter23.xhtml#f23_15">Figure 23.15</a> shows an example using stick figures of the kind in the upper right in <a href="C28_chapter23.xhtml#f23_13">Figure 23.13</a>. Glyphs may be placed at regular intervals, or in data-driven spatial positions using an original or derived data dimension.</p>
<figure id="f23_14" tabindex="0">
<img alt="" src="../images/fig23_14.jpg"/>
<figcaption><p><span class="blue">Figure 23.14.</span> Complex glyphs require significant display area so that the encoded information can be read. <em>Image courtesy Matt Ward, created with the SpiralGlyphics software</em> (M. O. Ward, 2002).</p></figcaption>
</figure>
<figure id="f23_15" tabindex="0">
<img alt="" src="../images/fig23_15.jpg"/>
<figcaption><p><span class="blue">Figure 23.15.</span> A dense array of simple glyphs. <em>Image courtesy Georges Grinstein</em> (S. Smith, Grinstein, &amp; Bergeron, 1991), © 1991 I EEE.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec23_6_4"><a id="index_term1433"/><span class="green">23.6.4 Multiple Views</span></h3>
<p>We now turn from approaches with only a single frame to those which use multiple views that are linked together. The most common form of linkage is linked highlighting, where items selected in one view are highlighted in all others. In linked navigation, movement in one view triggers movement in the others.</p>
<p><a id="term-345"/><a id="term-1107"/><span aria-label="665" epub:type="pagebreak" id="pg_665" role="doc-pagebreak"/><a id="term-1090"/><span aria-label="666" epub:type="pagebreak" id="pg_666" role="doc-pagebreak"/>There are many kinds of multiple-view approaches. In what is usually called simply the <em>multiple-view</em> approach, the same data is shown in several views, each of which has a different visual encoding that shows certain aspects of the dataset most clearly. The power of linked highlighting across multiple visual encodings is that items that fall in a contiguous region in one view are often distributed very differently in the other views. In the <em>small-multiples</em> approach, each view has the same visual encoding for different datasets, usually with shared axes between frames so that comparison of spatial position between them is meaningful. Side-by-side comparison with small multiples is an alternative to the visual clutter of superimposing all the data in the same view, and to the human memory limitations of remembering previously seen frames in an animation that changes over time.</p>
<p>The <em>overview-and-detail</em> approach is to have the same data and the same visual encoding in two views, where the only difference between them is the level of zooming. In most cases, the overview uses much less display space than the detail view. The combination of overview and detail views is common outside of <a id="index_term1390"/>visualization in many tools ranging from mapping software to photo editing. With a <em>detail-on-demand</em> approach, another view shows more information about some selected item, either as a popup window near the cursor or in a permanent window in another part of the display.</p>
<p>Determining the most appropriate spatial position of the views themselves with respect to each other can be as significant a problem as determining the spatial position of marks within a single view. In some systems, the location of the views is arbitrary and left up to the window system or the user. Aligning the views allows precise comparison between them, either vertically, horizontally, or with an array for both directions. Just as items can be sorted within a view, views can be sorted within a display, typically with respect to a derived variable measuring some aspect of the entire view as opposed to an individual item within it.</p>
<p><a href="C28_chapter23.xhtml#f23_16">Figure 23.16</a> shows a visualization of census data that uses many views. In addition to geographic information, the demographic information for each county includes population, density, gender, median age, percent change since 1990, and proportions of major ethnic groups. The visual encodings used include geographic, scatterplot, parallel coordinate, tabular, and matrix views. The same color encoding is used across all the views, with a legend in the bottom middle. The scatterplot matrix shows linked highlighting across all views, where the blue items are close together in some views and scattered in others. The map in the upper-left corner is an overview for the large detail map in the center. The tabular views allow direct sorting by and selection within a dimension of interest.</p>
<figure id="f23_16" tabindex="0">
<img alt="" src="../images/fig23_16.jpg"/>
<figcaption><p><span class="blue">Figure 23.16.</span> The Improvise toolkit was used to create this multiple-view visualization. <em>Image courtesy Chris Weaver.</em></p></figcaption>
</figure>
</section>
</section>
<section>
<h2 id="sec23_7"><a id="index_term1402"/><a id="term-1115"/><span aria-label="667" epub:type="pagebreak" id="pg_667" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec23_7" role="doc-backlink"><span class="green">23.7 Data Reduction</span></a></h2>
<p>The visual encoding techniques that we have discussed so far show all of the items in a dataset. However, many datasets are so large that showing everything simultaneously would result in so much visual clutter that the visual representation would be difficult or impossible for a viewer to understand. The main strategies to reduce the amount of data shown are overviews and <a id="index_term1393"/>aggregation, filtering and <a id="index_term1434"/>navigation, the focus+context techniques, and dimensionality reduction.</p>
<section>
<h3 id="sec23_7_1"><span class="green">23.7.1 Overviews and Aggregation</span></h3>
<p>With tiny datasets, a visual encoding can easily show all data dimensions for all items. For datasets of medium size, an overview that shows information about all items can be constructed by showing less detail for each item. Many datasets have internal or derivable structure at multiple scales. In these cases, a multiscale visual representation can provide many levels of overview, rather than just a single <a id="term-1102"/><span aria-label="668" epub:type="pagebreak" id="pg_668" role="doc-pagebreak"/>level. Overviews are typically used as a starting point to give users clues about where to drill down to inspect in more detail.</p>
<p>For larger datasets, creating an overview requires some kind of visual summarization. One approach to data reduction is to use an <em>aggregate</em> representation where a single visual mark in the overview explicitly represents many items.</p>
<p>The challenge of aggregation is to avoid eliminating the interesting signals in the dataset in the process of summarization. In the cartographic literature, the problem of creating maps at different scales while retaining the important distinguishing characteristics has been extensively studied under the name of <em>cartographic generalization</em> (Slocum, McMaster, Kessler, &amp; Howard, 2008).</p>
</section>
<section>
<h3 id="sec23_7_2"><span class="green">23.7.2 Filtering and Navigation</span></h3>
<p>Another approach to data reduction is to <em>filter</em> the data, showing only a subset of the items. Filtering is often carried out by directly selecting ranges of interest in one or more of the data dimensions.</p>
<p>Navigation is a specific kind of <a id="index_term1417"/>filtering based on spatial position, where changing the viewpoint changes the visible set of items. Both geometric and non-geometric <a id="index_term1456"/>zooming are used in visualization. With geometric zooming, the camera position in 2D or 3D space can be changed with standard computer graphics controls. In a realistic scene, items should be drawn at a size that depends on their distance from the camera, and only their apparent size changes based on that distance. However, in a visual encoding of an abstract space, nongeometric zooming can be useful. In <em>semantic zooming</em>, the visual appearance of an object changes dramatically based on the number of pixels available to draw it. For instance, an abstract visual representation of a text file could change from a tiny color-coded box with no label to a medium-sized box containing only the filename as a text label to a large rectangle containing a multi-line summary of the file contents. In realistic scenes, objects that are sufficiently far away from the camera are not visible in the images, for example, after they subtend less than one pixel of screen area. With <em>guaranteed visibility</em>, one of the original or derived data dimensions is used as a measure of importance, and objects of sufficient importance must have some kind of representation visible in the image plane at all times.</p>
</section>
<section>
<h3 id="sec23_7_3"><span class="green">23.7.3 Focus+Context</span></h3>
<p><em>Focus+context</em> techniques are another approach to data reduction. A subset of the dataset items are interactively chosen by the user to be the focus and are drawn <a id="term-1099"/><a id="term-1116"/><a id="term-1125"/><span aria-label="669" epub:type="pagebreak" id="pg_669" role="doc-pagebreak"/>in detail. The visual encoding also includes information about some or all of the rest of the dataset shown for context, integrated into the same view that shows the focus items. Many of these techniques use carefully chosen <a id="index_term1412"/>distortion to combine magnified focus regions and minified context regions into a unified view.</p>
<p>One common interaction metaphor is a moveable <a id="index_term1419"/>fisheye lens. Hyperbolic geometry provides an elegant mathematical framework for a single radial lens that affects all objects in the view. Another interaction metaphor is to use multiple lenses of different shapes and magnification levels that affect only local regions. Stretch and squish navigation uses the interaction metaphor of a rubber sheet where stretching one region squishes the rest, as shown in <a href="C28_chapter23.xhtml#f23_17">Figure 23.17</a>. The borders of the sheet stay fixed so that all items are within the viewport, although many items may be compressed to subpixel size. The fisheye metaphor is not limited to a geometric lens used after spatial layout; it can be used directly on structured data, such as a hierarchical document where some sections are collapsed while others are left expanded.</p>
<figure id="f23_17" tabindex="0">
<img alt="" src="../images/fig23_17.jpg"/>
<figcaption><p><span class="blue">Figure 23.17.</span> The TreeJuxtaposer system features <a id="index_term1445"/>stretch and squish navigation and guaranteed visibility of regions marked with colors (Munzner, Guimbretière, Tasiran, Zhang, &amp; Zhou, 2003).</p></figcaption>
</figure>
<p>These distortion-based approaches are another example of nonliteral navigation in the same spirit as nongeometric zooming. When navigating within a large and unfamiliar dataset with realistic camera motion, users can become disoriented <a id="term-811"/><a id="term-1122"/><a id="term-1123"/><span aria-label="670" epub:type="pagebreak" id="pg_670" role="doc-pagebreak"/>at high zoom levels when they can see only a small local region. These approaches are designed to provide more contextual information than a single undistorted view, in hopes that people can stay oriented if landmarks remain recognizeable. However, these kinds of distortion can still be confusing or difficult to follow for users. The costs and benefits of distortion, as opposed to multiple views or a single realistic view, are not yet fully understood. Standard 3D perspective is a particularly familiar kind of distortion and was explicitly used as a form of focus+context in early visualization work. However, as the costs of 3D spatial layout discussed in <a href="C28_chapter23.xhtml#sec23_4">Section 23.4</a> became more understood, this approach became less popular.</p>
<p>Other approaches to providing context around focus items do not require distortion. For instance, the SpaceTree system shown in <a href="C28_chapter23.xhtml#f23_18">Figure 23.18</a> elides most nodes in the tree, showing the path between the interactively chosen focus node and the root of the tree for context.</p>
<figure id="f23_18" tabindex="0">
<img alt="" src="../images/fig23_18.jpg"/>
<figcaption><p><span class="blue">Figure 23.18.</span> The <a id="index_term1442"/>SpaceTree system shows the path between the root and the interactively chosen focus node to provide context (Grosjean, Plaisant, &amp; Bederson, 2002).</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec23_7_4"><a id="index_term1410"/><a id="term-1091"/><span aria-label="671" epub:type="pagebreak" id="pg_671" role="doc-pagebreak"/><span class="green">23.7.4 Dimensionality Reduction</span></h3>
<p>The data reduction approaches covered so far reduce the number of items to draw. When there are many data dimensions, <em>dimensionality reduction</em> can also be effective.</p>
<p>With <em>slicing</em>, a single value is chosen from the dimension to eliminate, and only the items matching that value for the dimension are extracted to include in the lower-dimensional slice. Slicing is particularly useful with 3D spatial data, for example when inspecting slices through a CT scan of a human head at different heights along the skull. <a id="index_term1441"/>Slicing can be used to eliminate multiple dimensions at once.</p>
<p>With <em>projection</em>, no information about the eliminated dimensions is retained; the values for those dimensions are simply dropped, and all items are still shown. A familiar form of projection is the standard graphics perspective transformation which projects from 3D to 2D, losing information about depth along the way. In mathematical visualization, the structure of higher-dimensional geometric objects can be shown by projecting from 4D to 3D before the standard <a id="index_term1437"/>projection to the image plane and using color to encode information from the projected-away dimension. This technique is sometimes called <em>dimensional <a id="index_term1418"/>filtering</em> when it is used for nonspatial data.</p>
<p>In some datasets, there may be interesting hidden structure in a much lower-dimensional space than the number of original data dimensions. For instance, sometimes directly measuring the independent variables of interest is difficult or impossible, but a large set of dependent or indirect variables is available. The goal is to find a small set of dimensions that faithfully represent most of the structure or variance in the dataset. These dimensions may be the original ones, or synthesized new ones that are linear or nonlinear combinations of the originals. Principal component analysis is a fast, widely used linear method. Many nonlinear approaches have been proposed, including multidimensional scaling (MDS). These methods are usually used to determine whether there are large-scale clusters in the dataset; the fine-grained structure in the lower-dimensional plots is usually not reliable because information is lost in the reduction. <a href="C28_chapter23.xhtml#f23_19">Figure 23.19</a> shows document collection in a single scatterplot. When the true dimensionality of the dataset is far higher than two, a matrix of scatterplots showing pairs of synthetic dimensions may be necessary.</p>
<figure id="f23_19" tabindex="0">
<img alt="" src="../images/fig23_19.jpg"/>
<figcaption><p><span class="blue">Figure 23.19.</span> Dimensionality reduction with the Glimmer multidimensional scaling approach shows clusters in a document dataset (Ingram, Munzner, &amp; Olano, 2009), © 2009 IEEE.</p></figcaption>
</figure>
</section>
</section>
<section>
<h2 id="sec23_8"><a id="index_term1413"/><span aria-label="672" epub:type="pagebreak" id="pg_672" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec23_8" role="doc-backlink"><span class="green">23.8 Examples</span></a></h2>
<p>We conclude this chapter with several examples of visualizing specific types of data using the techniques discussed above.</p>
<section>
<h3 id="sec23_8_1"><span class="green">23.8.1 Tables</span></h3>
<p><a id="index_term1447"/>Tabular data is extremely common, as all spreadsheet users know. The goal in visualization is to encode this information through easily perceivable visual channels rather than forcing people to read through it as numbers and text. <a href="C28_chapter23.xhtml#f23_20">Figure 23.20</a> shows the Table Lens, a focus+context approach where quantitative values are encoded as the length of one-pixel high lines in the context regions, and shown as numbers in the focus regions. Each dimension of the dataset is shown as a column, and the rows of items can be resorted according to the values in that column with a single click in its header.</p>
<figure id="f23_20" tabindex="0">
<img alt="" src="../images/fig23_20.jpg"/>
<figcaption><p><span class="blue">Figure 23.20.</span> The Table Lens provides focus+context interaction with tabular data, immediately reorderable by the values in each dimension column. <em>Image courtesy Stuart Card</em> (Rao &amp; Card, 1994), © 1994 ACM, Inc. Included here by permission.</p></figcaption>
</figure>
<p>The traditional Cartesian approach of a scatterplot, where items are plotted as dots with respect to perpendicular axes, is only usable for two and three dimensions of data. Many tables contain far more than three dimensions of data, and the number of additional dimensions that can be encoded using other visual channels is limited. Parallel coordinates are an approach for visualizing more dimensions at once using spatial position, where the axes are parallel rather than perpendicular and an <em>n</em>-dimensional item is shown as a polyline that crosses each of the <em>n</em> axes once (Inselberg &amp; Dimsdale, 1990; Wegman, 1990). <a href="C28_chapter23.xhtml#f23_21">Figure 23.21</a> shows an eight-dimensional dataset of 230,000 items at multiple levels of detail (Fua, Ward, &amp; Rundensteiner, 1999), from a high-level view at the top to finer detail <a id="term-1092"/><a id="term-1110"/><a id="term-1127"/><span aria-label="673" epub:type="pagebreak" id="pg_673" role="doc-pagebreak"/>at the bottom. With hierarchical parallel coordinates, the items are clustered and an entire cluster of items is represented by a band of varying width and opacity, where the mean is in the middle and width at each axis depends on the values of the items in the cluster in that dimension. The coloring of each band is based on the proximity between clusters according to a similarity metric.</p>
<figure id="f23_21" tabindex="0">
<img alt="" src="../images/fig23_21.jpg"/>
<figcaption><p><span class="blue">Figure 23.21.</span> <a id="index_term1425"/>Hierarchical parallel coordinates show high-dimensional data at multiple levels of detail. <em>Image courtesy Matt Ward</em> (Fua et al., 1999), © 1999 IEEE.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec23_8_2"><a id="index_term1424"/><a id="term-284"/><a id="term-285"/><a id="term-1093"/><a id="term-1100"/><a id="term-1101"/><a id="term-1108"/><span aria-label="674" epub:type="pagebreak" id="pg_674" role="doc-pagebreak"/><span class="green">23.8.2 Graphs</span></h3>
<p>The field of graph drawing is concerned with finding a spatial position for the nodes in a graph in 2D or 3D space and routing the edges between these nodes (Di Battista, Eades, Tamassia, &amp; Tollis, 1999). In many cases the edge-routing problem is simplified by using only straight edges, or by only allowing right-angle bends for the class of <em>orthogonal</em> layouts, but some approaches handle true curves. If the graph has directed edges, a layered approach can be used to show hierarchical structure through the horizontal or vertical spatial ordering of nodes, as shown in <a href="C28_chapter23.xhtml#f23_2">Figure 23.2</a>.</p>
<p>A suite of aesthetic criteria operationalize human judgments about readable graphs as metrics that can be computed on a proposed layout (Ware, Purchase, Colpys, &amp; McGill, 2002). <a href="C28_chapter23.xhtml#f23_22">Figure 23.22</a> shows some examples. Some metrics should be minimized, such as the number of edge crossings, the total area of the layout, and the number of right-angle bends or curves. Others should be maximized, such as the angular resolution or symmetry. The problem is difficult because most of these criteria are individually NP-hard, and moreover they are mutually incompatible (Brandenburg, 1988).</p>
<figure id="f23_22" tabindex="0">
<img alt="" src="../images/fig23_22.jpg"/>
<figcaption><p><span class="blue">Figure 23.22.</span> Graph layout aesthetic criteria. Top: Edge crossings should be minimized. Middle: Angular resolution should be maximized. Bottom: Symmetry is maximized on the left, whereas crossings are minimized on the right, showing the conflict between the individually NP-hard criteria.</p></figcaption>
</figure>
<p>Many approaches to node-link graph drawing use force-directed placement, motivated by the intuitive physical metaphor of spring forces at the edges drawing together repelling particles at the nodes. Although naive approaches have high time complexity and are prone to being caught in local minima, much work has gone into developing more sophisticated algorithms such as GEM (Frick, Ludwig, &amp; Mehldau, 1994) or IPSep-CoLa (Dwyer, Koren, &amp; Marriott, 2006). <a href="C28_chapter23.xhtml#f23_23">Figure 23.23</a> shows an interactive system using the <em>r</em>-PolyLog energy model, where <a id="term-1094"/><a id="term-1109"/><a id="term-1130"/><span aria-label="675" epub:type="pagebreak" id="pg_675" role="doc-pagebreak"/>a focus+context view of the clustered graph is created with both geometric and semantic fisheye (van Ham &amp; van Wijk, 2004).</p>
<figure id="f23_23" tabindex="0">
<img alt="" src="../images/fig23_23.jpg"/>
<figcaption><p><span class="blue">Figure 23.23.</span> Force-directed placement showing a clustered graph with both geometric and semantic fisheye. <em>Image courtesy Jarke van Wijk</em> (van Ham &amp; van Wijk, 2004), © 2004 IEEE.</p></figcaption>
</figure>
<p>Graphs can also be visually encoded by showing the adjacency matrix, where all vertices are placed along each axis and the cell between two vertices is colored if there is an edge between them. The MatrixExplorer system uses linked multiple views to help social science researchers visually analyze social networks with both matrix and node-link representations (Henry &amp; Fekete, 2006). <a href="C28_chapter23.xhtml#f23_24">Figure 23.24</a> shows the different visual patterns created by the same graph structure in these two views: A represents an actor connecting several communities; B is a community; and C is a clique, or a complete sub-graph. Matrix views do not suffer from cluttered edge crossings, but many tasks including path following are more difficult with this approach.</p>
<figure id="f23_24" tabindex="0">
<img alt="" src="../images/fig23_24.jpg"/>
<figcaption><p><span class="blue">Figure 23.24.</span> Graphs can be shown with either matrix or node-link views. <em>Image courtesy Jean-Daniel Fekete</em> (Henry &amp; Fekete, 2006), © 2006 IEEE.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec23_8_3"><span class="green">23.8.3 Trees</span></h3>
<p>Trees are a special case of graphs so common that a great deal of visualization research has been devoted to them. A straightforward algorithm to lay out trees in the two-dimensional plane works well for small trees (Reingold &amp; Tilford, 1981), while a more complex but scalable approach runs in linear time (Buchheim, Jünger, &amp; Leipert, 2002). Figures 23.17 and 23.18 also show trees with different approaches to spatial layout, but all four of these methods visually encode the relationship between parent and child nodes by drawing a link connecting them.</p>
<p><a id="term-965"/><a id="term-1095"/><a id="term-1131"/><span aria-label="676" epub:type="pagebreak" id="pg_676" role="doc-pagebreak"/><a id="index_term1452"/>Treemaps use containment rather than connection to show the hierarchical relationship between parent and child nodes in a tree (B. Johnson &amp; Shneiderman, 1991). That is, treemaps show child nodes nested within the outlines of the parent node. <a href="C28_chapter23.xhtml#f23_25">Figure 23.25</a> shows a hierarchical filesystem of nearly one million files, where file size is encoded by rectangle size and file type is encoded by color (Fekete &amp; Plaisant, 2002). The size of nodes at the leaves of the tree can encode an additional data dimension, but the size of nodes in the interior does not show the value of that dimension; it is dictated by the cumulative size of their descendants. Although tasks such as understanding the topological structure of the tree or tracing paths through it are more difficult with treemaps than with node-link approaches, tasks that involve understanding an attribute tied to leaf nodes are well supported. Treemaps are space-filling representations that are usually more compact than node-link approaches.</p>
<figure id="f23_25" tabindex="0">
<img alt="" src="../images/fig23_25.jpg"/>
<figcaption><p><span class="blue">Figure 23.25.</span> Treemap showing a filesystem of nearly one million files. <em>Image courtesy Jean-Daniel Fekete</em> (Fekete &amp; Plaisant, 2002), © 2002 IEEE.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec23_8_4"><a id="term-153"/><a id="term-1086"/><a id="term-1103"/><span aria-label="677" epub:type="pagebreak" id="pg_677" role="doc-pagebreak"/><span class="green">23.8.4 Geographic</span></h3>
<p>Many kinds of analysis such as epidemiology require understanding both geographic and nonspatial data. <a href="C28_chapter23.xhtml#f23_26">Figure 23.26</a> shows a tool for the visual analysis of a cancer demographics dataset that combines many of the ideas described in this chapter (MacEachren, Dai, Hardisty, Guo, &amp; Lengerich, 2003). The top matrix of linked views features small multiples of three types of visual encodings: geographic maps showing Appalachian counties at the lower left, histograms across the diagonal of the matrix, and scatterplots on the upper right. The bottom 2 × 2 matrix, linking scatterplots with maps, includes the color legend for both. The discrete bivariate sequential <a id="index_term195"/>colormap has lightness increasing sequentially for each of two complementary hues and is effective for color-deficient people.</p>
<figure id="f23_26" tabindex="0">
<img alt="" src="../images/fig23_26.jpg"/>
<figcaption><p><span class="blue">Figure 23.26.</span> Two matrices of linked small multiples showing cancer demographic data (MacEachren et al., 2003), © 2003 IEEE.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec23_8_5"><a id="index_term1443"/><span class="green">23.8.5 Spatial Fields</span></h3>
<p>Most nongeographic spatial data is modeled as a field, where there are one or more values associated with each point in 2D or 3D space. Scalar fields, for example CT or MRI medical imaging scans, are usually visualized by finding isosurfaces or using direct volume rendering. Vector fields, for example, flows in water or air, are often visualized using arrows, streamlines (McLouglin, Laramee, Peikert, Post, &amp; Chen, 2009), and <em>line integral convolution</em> (LIC) (Laramee et al., 2004). Tensor fields, such as those describing the anisotropic diffusion of molecules through the human brain, are particularly challenging to display (Kindlmann, Weinstein, &amp; Hart, 2000).</p>
<section>
<h4 id="sec83"><span class="blue">Frequently Asked Questions</span></h4>
<ul class="list-bullet">
<li>
<p class="list"><span class="green">What conferences and journals are good places to look for further information about visualization?</span></p>
<p class="noindent1b">The IEEE VisWeek conference comprises three subconferences: InfoVis (Information Visualization), Vis (Visualization), and VAST (Visual Analytics Science and Technology). There is also a European EuroVis conference and an Asian PacificVis venue. Relevant journals include IEEE TVCG (Transactions on Visualization and Computer Graphics) and Palgrave Information Visualization.<a id="term-1104"/><span aria-label="678" epub:type="pagebreak" id="pg_678" role="doc-pagebreak"/></p>
</li>
<li>
<p class="list"><span aria-label="679" epub:type="pagebreak" id="pg_679" role="doc-pagebreak"/><span class="green">What software and toolkits are available for visualization?</span></p>
<p class="noindent1b">The most popular toolkit for spatial data is <span class="monospace">vtk</span>, a C/C++ codebase available at <span class="monospace"><a href="http://www.vtk.org">www.vtk.org</a></span>. For abstract data, the Java-based <span class="monospace">prefuse</span> (<span class="monospace"><a href="http://www.prefuse.org">http://www.prefuse.org</a></span>) and Processing (<span class="monospace">processing.org</span>) toolkits are becoming widely used. The ManyEyes site from IBM Research (<span class="monospace">www.many-eyes.com</span>) allows people to upload their own data, create interactive visualizations in a variety of formats, and carry on conversations about visual data analysis.</p>
</li>
</ul>
</section>
</section>
</section>
</section>
</body>
</html>