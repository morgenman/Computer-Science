<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" dir="ltr" lang="en" xml:lang="en">
<head>
<meta charset="UTF-8"/>
<title>11 Texture Mapping</title>
<link href="../styles/9781000426359.css" rel="stylesheet" type="text/css"/>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX","input/MathML","output/SVG"],
extensions: ["tex2jax.js","mml2jax.js","MathEvents.js"],
TeX: {
extensions: ["noErrors.js","noUndefined.js","autoload-all.js"]
},
MathMenu: {
showRenderer: false
},
menuSettings: {
zoom: "Click"
},
messageStyle: "none"
});
</script>
<script src="../mathjax/MathJax.js" type="text/javascript"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006665500" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter">
<section epub:type="chapter" role="doc-chapter">
<h1 class="chapz" id="c11"><a id="index_term1177"/><a id="term-872"/><span aria-label="255" epub:type="pagebreak" id="pg_255" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rc11" role="doc-backlink"><span class="green"><span class="big1">11</span><br/>Texture Mapping</span></a></h1>
<p>When trying to replicate the look of the real world, one quickly realizes that hardly any surfaces are featureless. Wood grows with grain; skin grows with wrinkles; cloth shows its woven structure; and paint shows the marks of the brush or roller that laid it down. Even smooth plastic is made with bumps molded into it, and smooth metal shows the marks of the machining process that made it. Materials that were once featureless quickly become covered with marks, dents, stains, scratches, fingerprints, and dirt.</p>
<p>In computer graphics, we lump all these phenomena under the heading of “spatially varying surface properties”—attributes of surfaces that vary from place to place but don’t really change the <em>shape</em> of the surface in a meaningful way. To allow for these effects, all kinds of modeling and rendering systems provide some means for <em>texture mapping</em>: using an image, called a <em>texture</em> <em>map</em>, <em>texture image</em>, or just a <em>texture</em>, to store the details that you want to go on a surface and then mathematically “mapping” the image onto the surface.</p>
<aside class="boxed-text" epub:type="sidebar" id="c011_box1">
<p class="noindent">This is mapping in the sense of <a href="C07_chapter2.xhtml#sec2_1">Section 2.1</a>.</p>
</aside>
<p class="indent">As it turns out, once the mechanism to map images onto surfaces exists, there are many less obvious ways it can be used that go beyond the basic purpose of introducing surface detail. Textures can be used to make shadows and reflections, to provide illumination, even to define surface shape. In sophisticated interactive programs, textures are used to store all kinds of data that doesn’t even have anything to do with pictures!</p>
<p>This chapter discusses the use of textures for representing surface detail, shadows, and reflections. While the basic ideas are simple, several practical problems <span aria-label="256" epub:type="pagebreak" id="pg_256" role="doc-pagebreak"/>complicate the use of textures. First of all, textures easily become distorted, and designing the functions that map textures onto surfaces is challenging. Also, <a id="index_term13"/>texture mapping is a resampling process, just like rescaling an image, and as we saw in <a href="C15_chapter10.xhtml#c10">Chapter 10</a>, resampling can very easily introduce aliasing artifacts. The use of texture mapping and animation together readily produces truly dramatic aliasing, and much of the complexity of texture mapping systems is created by the <em><a id="index_term53"/>antialiasing</em> measures that are used to tame these artifacts.</p>
<section>
<h2 id="sec11_1"><a id="index_term1186"/><a id="index_term1184"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec11_1" role="doc-backlink"><span class="green">11.1 Looking Up Texture Values</span></a></h2>
<p>To start off, let’s consider a simple application of texture mapping. We have a scene with a wood floor, and we would like the diffuse color of the floor to be controlled by an image showing floorboards with wood grain. Regardless of whether we are using ray tracing or rasterization, the shading code that computes the color for a ray–surface intersection point or for a fragment generated by the rasterizer needs to know the color of the texture at the shading point, in order to use it as the diffuse color in the Lambertian shading model from <a href="C10_chapter5.xhtml#c5">Chapter 5</a>.</p>
<p>To get this color, the shader performs a <em>texture lookup</em>: it figures out the location, in the coordinate system of the texture image, that corresponds to the shading point, and it reads out the color at that point in the image, resulting in the <em>texture sample</em>. That color is then used in shading, and since the texture lookup happens at a different place in the texture for every pixel that sees the floor, a pattern of different colors shows up in the image. The code might look like this:</p>
<pre class="pre1">Color texture_lookup(Texture t, float u, float v) {
        int i = round(u ⋆ t.width() - 0.5)
        int j = round(v ⋆ t.height() - 0.5)
        return t.get_pixel(i,j)
}
Color shade_surface_point(Surface s, Point p, Texture t) {
        Vector normal = s.get_normal(p)
        (u,v) = s.get_texcoord(p)
        Color diffuse_color = texture_lookup(u,v)
        // compute shading using diffuse_color and normal
        // return shading result
}</pre>
<p>In this code, the shader asks the surface where to look in the texture, and somehow every surface that we want to shade using a texture needs to be able to answer this query. This brings us to the first key ingredient of texture mapping: we need a function that maps from the surface to the texture that we can easily compute for every pixel. This is the <em>texture <a id="index_term1179"/>coordinate function</em> (<a href="C16_chapter11.xhtml#f11_1">Figure 11.1</a>), and we say <a id="term-870"/><span aria-label="257" epub:type="pagebreak" id="pg_257" role="doc-pagebreak"/>that it assigns texture coordinates to every point on the surface. Mathematically, it is a mapping from the surface <em>S</em> to the domain of the texture, <em>T</em>:</p>
<figure id="f11_1" tabindex="0">
<img alt="" src="../images/fig11_1.jpg"/>
<figcaption><p><span class="blue">Figure 11.1.</span> Just like the viewing projection π maps every point on an object’s surface, <em>S</em>, to a point in the image, the texture coordinate function <em>ϕ</em> maps every point on the object’s surface to a point in the texture map, <em>T</em>. Appropriately defining this function <em>ϕ</em> is fundamental to all applications of texture mapping.</p></figcaption>
</figure>
<div class="disp-formula" id="uequ11_1">
<m:math xmlns:mml="http://www.w3.org/1998/Math/MathML" alttext=""><m:mrow><m:mtable><m:mtr><m:mtd><m:mi>ϕ</m:mi></m:mtd><m:mtd><m:mo>:</m:mo></m:mtd><m:mtd columnalign="left"><m:mi>S</m:mi><m:mo>→</m:mo><m:mi>T</m:mi></m:mtd></m:mtr><m:mtr><m:mtd/><m:mtd><m:mo>:</m:mo></m:mtd><m:mtd columnalign="left"><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo>)</m:mo><m:mo>↦</m:mo><m:mo>(</m:mo><m:mi>u</m:mi><m:mo>,</m:mo><m:mi>υ</m:mi><m:mo>)</m:mo><m:mn>.</m:mn></m:mrow></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>The set <em>T</em>, often called “texture space,” is usually just a rectangle that contains the image; it is common to use the unit square (<em>u, v</em>) ∈ [0,1]<sup>2</sup> (in this book, we’ll use the names <em>u</em> and <em>v</em> for the two <em>texture coordinates</em>). In many ways, it’s similar to the viewing projection discussed in <a href="C13_chapter8.xhtml#c8">Chapter 8</a>, called π in this chapter, which maps points on surfaces in the scene to points in the image; both are 3D-to-2D mappings, and both are needed for rendering—one to know where to get the texture value from, and one to know where to put the shading result in the image. But there are some important differences, too: π is almost always a perspective or orthographic projection, whereas <em>ϕ</em> can take on many forms; and there is only one viewing projection for an image, whereas each object in the scene is likely to have a completely separate texture coordinate function.</p>
<p>It may seem surprising that <em>ϕ</em> is a mapping <em>from</em> the surface <em>to</em> the texture, when our goal is to put the texture onto the surface, but this is the function we need.</p>
<aside class="boxed-text" epub:type="sidebar" id="c011_box2">
<p class="noindent">So … the first thing you have to learn is how to think backwards?</p>
</aside>
<p class="indent">For the case of the wood floor, if the floor happens to be at constant <em>z</em> and aligned to the <em>x</em> and <em>y</em> axes, we could just use the mapping</p>
<div class="disp-formula" id="uequ11_2">
<m:math xmlns:mml="http://www.w3.org/1998/Math/MathML" alttext=""><m:mrow><m:mi>u</m:mi><m:mo>=</m:mo><m:mi>a</m:mi><m:mi>x</m:mi><m:mo>;</m:mo><m:mtext> </m:mtext><m:mi>υ</m:mi><m:mo>=</m:mo><m:mi>b</m:mi><m:mi>y</m:mi><m:mo>,</m:mo></m:mrow></m:math>
</div>
<p>for some suitably chosen scale factors <em>a</em> and <em>b</em>, to assign texture coordinates (<em>u,v</em>) to the point (<em>x,y,z</em>)<sub>floor</sub>, and then use the value of the texture pixel, or <em>texel</em>, <a id="term-857"/><a id="term-876"/><a id="term-880"/><span aria-label="258" epub:type="pagebreak" id="pg_258" role="doc-pagebreak"/>closest to (<em>u,v</em>) as the texture value at (<em>x,y</em>). In this way we rendered the image in <a href="C16_chapter11.xhtml#f11_2">Figure 11.2</a>.</p>
<figure id="f11_2" tabindex="0">
<img alt="" src="../images/fig11_2.jpg"/>
<figcaption><p><span class="blue">Figure 11.2.</span> A wood floor, textured using a texture coordinate function that simply uses the <em>x</em> and <em>y</em> coordinates of points directly.</p></figcaption>
</figure>
<p>This is pretty limiting, though: what if the room is modeled at an angle to the <em>x</em> and <em>y</em> axes, or what if we want the wood texture on the curved back of a chair? We will need some better way to compute texture coordinates for points on the surface.</p>
<p>Another problem that arises from the simplest form of texture mapping is illustrated dramatically by rendering at a high contrast texture from a very grazing angle into a low-resolution image. <a href="C16_chapter11.xhtml#f11_3">Figure 11.3</a> shows a larger plane textured using the same approach but with a high contrast grid pattern and a view toward the horizon. You can see it contains aliasing artifacts (stairsteps in the foreground, wavy and glittery patterns in the distance) similar to the ones that arise in image resampling (<a href="C15_chapter10.xhtml#c10">Chapter 10</a>) when appropriate filters are not used. Although it takes an extreme case to make these artifacts so obvious in a tiny still image printed in a book, in animations these patterns move around and are very distracting even when they are much more subtle.</p>
<figure id="f11_3" tabindex="0">
<img alt="" src="../images/fig11_3.jpg"/>
<figcaption><p><span class="blue">Figure 11.3.</span> A large horizontal plane, textured in the same way as in <a href="C16_chapter11.xhtml#f11_2">Figure 11.2</a> and displaying severe aliasing artifacts.</p></figcaption>
</figure>
<p>We have now seen the two primary issues in basic texture mapping:</p>
<ul class="list-bullet">
<li>
<p class="list">Defining texture coordinate functions, and</p>
</li>
<li>
<p class="list">Looking up texture values without introducing too much aliasing.</p>
</li>
</ul>
<p>These two concerns are fundamental to all kinds of applications of texture mapping and are discussed in <a href="C16_chapter11.xhtml#sec11_2">Sections 11.2</a> and <a href="C16_chapter11.xhtml#sec11_3">11.3</a>. Once you understand them and some of the solutions to them, you understand texture mapping. The rest is just how to apply the basic texturing machinery for a variety of different purposes, which is discussed in <a href="C16_chapter11.xhtml#sec11_4">Section 11.4</a>.</p>
</section>
<section>
<h2 id="sec11_2"><a epub:type="backlink" href="C02a_toc.xhtml#rsec11_2" role="doc-backlink"><span class="green">11.2 Texture Coordinate Functions</span></a></h2>
<p>Designing the texture coordinate function ϕ well is a key requirement for getting good results with texture mapping. You can think of this as deciding how you are going to deform a flat, rectangular image so that it conforms to the 3D surface you want to draw. Or alternatively, you are taking the surface and gently flattening it, without letting it wrinkle, tear, or fold, so that it lies flat on the image. Sometimes, this is easy: maybe the 3D surface is already a flat rectangle! In other cases, it’s very tricky: the 3D shape might be very complicated, like the surface of a character’s body.</p>
<p>The problem of defining texture coordinate functions is not new to computer graphics. Exactly, the same problem is faced by cartographers when designing <span aria-label="259" epub:type="pagebreak" id="pg_259" role="doc-pagebreak"/>maps that cover large areas of the Earth’s surface: the mapping from the curved globe to the flat map inevitably causes distortion of areas, angles, and/or distances that can easily make maps very misleading. Many map projections have been proposed over the centuries, all balancing the same competing concerns—of minimizing various kinds of distortion while covering a large area in one contiguous piece—that are faced in texture mapping.</p>
<p>In some applications (some examples are in <a href="C16_chapter11.xhtml#sec11_2_1">Section 11.2.1</a>), there’s a clear reason to use a particular map. But in most cases, designing the texture coordinate map is a delicate task of balancing competing concerns, which skilled modelers put considerable effort into.</p>
<aside class="boxed-text" epub:type="sidebar" id="c011_box3">
<p class="noindent">“UV mapping” or “surface parameterization” are other names you may encounter for the texture coordinate function.</p>
</aside>
<p class="indent">You can define ϕ in just about any way you can dream up. But there are several competing goals to consider:</p>
<ul class="list-bullet">
<li>
<p class="list"><strong><span class="green"><a id="index_term1161"/>Bijectivity</span></strong>. In most cases, you’d like <em>ϕ</em> to be bijective (see <a href="C07_chapter2.xhtml#sec2_1_1">Section 2.1.1</a>), so that each point on the surface maps to a <em>different</em> point in texture space. If several points map to the same texture space point, the value at one point in the texture will affect several points on the surface. In cases where you want a texture to repeat over a surface (think of wallpaper or carpet with their repeating patterns), it makes sense to deliberately introduce a many-to-one mapping from surface points to texture points, but you don’t want this to happen by accident.</p>
</li>
<li>
<p class="list"><strong><span class="green"><a id="index_term1172"/>Size distortion</span></strong>. The scale of the texture should be approximately constant across the surface. That is, close-together points anywhere on the surface that are about the same distance apart should map to points about the same distance apart in the texture. In terms of the function <em>ϕ</em>, the magnitude of the derivatives of <em>ϕ</em> should not vary too much.</p>
</li>
<li>
<p class="list"><strong><span class="green"><a id="index_term1171"/>Shape distortion</span></strong>. The texture should not be very distorted. That is, a small circle drawn on the surface should map to a reasonably circular shape in texture space, rather than an extremely squashed or elongated shape. In terms of <em>ϕ</em>, the derivative of <em>ϕ</em> should not be too different in different directions.</p>
</li>
<li>
<p class="list"><strong><span class="green"><a id="index_term1162"/>Continuity.</span></strong> There should not be too many seams: neighboring points on the surface should map to neighboring points in the texture. That is, <em>ϕ</em> should be continuous or have as few discontinuities as possible. In most cases, some discontinuities are inevitable, and we’d like to put them in inconspicuous locations.</p>
</li>
</ul>
<p class="indent">Surfaces that are defined by parametric equations (<a href="C07_chapter2.xhtml#sec2_7_8">Section 2.7.8</a>) come with a built-in choice for the texture coordinate function: simply invert the function <a id="term-626"/><a id="term-858"/><a id="term-864"/><a id="term-865"/><span aria-label="260" epub:type="pagebreak" id="pg_260" role="doc-pagebreak"/>that defines the surface, and use the two parameters of the surface as texture coordinates. These texture coordinates may or may not have desirable properties, depending on the surface, but they do provide a mapping.</p>
<p>But for surfaces that are defined implicitly, or are just defined by a triangle mesh, we need some other way to define the texture coordinates, without relying on an existing parameterization. Broadly speaking, the two ways to define texture coordinates are to compute them geometrically, from the spatial coordinates of the surface point, or, for mesh surfaces, to store values of the texture coordinates at vertices and interpolate them across the surface. Let’s look at these options one at a time.</p>
<section>
<h3 id="sec11_2_1"><a id="index_term1167"/><span class="green">11.2.1 Geometrically Determined Coordinates</span></h3>
<p>Geometrically determined texture coordinates are used for simple shapes or special situations, as a quick solution, or as a starting point for designing a hand-tweaked texture coordinate map.</p>
<p>We will illustrate the various texture coordinate functions by mapping the test image in <a href="C16_chapter11.xhtml#f11_4">Figure 11.4</a> onto the surface. The numbers in the image let you read the approximate (<em>u,v</em>) coordinates out of the rendered image, and the grid lets you see how distorted the mapping is.</p>
<figure id="f11_4" tabindex="0">
<img alt="" src="../images/fig11_4.jpg"/>
<figcaption><p><span class="blue">Figure 11.4.</span> Test image.</p></figcaption>
</figure>
<section>
<h4 id="sec11_2_1_1"><a id="index_term1170"/><span class="blue">Planar Projection</span></h4>
<p>Probably, the simplest mapping from 3D to 2D is a parallel projection—the same mapping as used for orthographic viewing (<a href="C16_chapter11.xhtml#f11_5">Figure 11.5</a>). The machinery we developed already for viewing (<a href="C13_chapter8.xhtml#sec8_1">Section 8.1</a>) can be reused directly for defining texture coordinates: just as orthographic viewing boils down to multiplying by a matrix and discarding the z component, generating texture coordinates by planar projection can be done with a simple matrix multiply:</p>
<div class="disp-formula" id="uequ11_3">
<m:math alttext=""><m:mrow><m:mi>ϕ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mo stretchy="false">(</m:mo><m:mi>u</m:mi><m:mo>,</m:mo><m:mi>v</m:mi><m:mo stretchy="false">)</m:mo><m:mtext> </m:mtext><m:mtext>where</m:mtext><m:mtext> </m:mtext><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:mi>u</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>v</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mo>∗</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:mn>1</m:mn></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:mo>=</m:mo><m:msub><m:mrow><m:mi>M</m:mi></m:mrow><m:mrow><m:mi>t</m:mi></m:mrow></m:msub><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:mi>x</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>y</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>z</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mn>1</m:mn></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:mo>,</m:mo></m:mrow></m:math>
</div>
<figure id="f11_5" tabindex="0">
<img alt="" src="../images/fig11_5.jpg"/>
<figcaption><p><span class="blue">Figure 11.5.</span> Planar projection makes a useful parameterization for objects or parts of objects that are nearly flat to start with, if the projection direction is chosen roughly along the overall normal.</p></figcaption>
</figure>
<p>where the texturing matrix <em>M<sub>t</sub></em> represents an affine transformation, and the asterisk indicates that we don’t care what ends up in the third coordinate.</p>
<p>This works quite well for surfaces that are mostly flat, without too much variation in surface normal, and a good projection direction can be found by taking the average normal. For any kind of closed shape, though, a planar projection will <a id="term-627"/><a id="term-825"/><a id="term-856"/><a id="term-866"/><a id="term-867"/><span aria-label="261" epub:type="pagebreak" id="pg_261" role="doc-pagebreak"/>not be injective: points on the front and back will map to the same point in texture space (<a href="C16_chapter11.xhtml#f11_6">Figure 11.6</a>).</p>
<figure id="f11_6" tabindex="0">
<img alt="" src="../images/fig11_6.jpg"/>
<figcaption><p><span class="blue">Figure 11.6.</span> Using planar projection on a closed object will always result in a non-injective, one-to-many mapping, and extreme distortion near points where the projection direction is tangent to the surface.</p></figcaption>
</figure>
<p>By simply substituting perspective projection for orthographic, we get <em>projective</em> texture coordinates (<a href="C16_chapter11.xhtml#f11_7">Figure 11.7</a>):</p>
<div class="disp-formula" id="uequ11_4">
<m:math alttext=""><m:mrow><m:mi>ϕ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mo stretchy="false">(</m:mo><m:mover accent="true"><m:mrow><m:mi>u</m:mi></m:mrow><m:mrow><m:mo>˜</m:mo></m:mrow></m:mover><m:mo>/</m:mo><m:mi>w</m:mi><m:mo>,</m:mo><m:mover accent="true"><m:mrow><m:mi>v</m:mi></m:mrow><m:mrow><m:mo>˜</m:mo></m:mrow></m:mover><m:mo>/</m:mo><m:mi>w</m:mi><m:mo stretchy="false">)</m:mo><m:mtext> </m:mtext><m:mtext>where</m:mtext><m:mtext> </m:mtext><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:mover accent="true"><m:mrow><m:mi>u</m:mi></m:mrow><m:mrow><m:mo>˜</m:mo></m:mrow></m:mover></m:mtd></m:mtr><m:mtr><m:mtd><m:mover accent="true"><m:mrow><m:mi>v</m:mi></m:mrow><m:mrow><m:mo>˜</m:mo></m:mrow></m:mover></m:mtd></m:mtr><m:mtr><m:mtd><m:mo>∗</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>w</m:mi></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:mo>=</m:mo><m:msub><m:mrow><m:mi>P</m:mi></m:mrow><m:mrow><m:mi>t</m:mi></m:mrow></m:msub><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:mi>x</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>y</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mi>z</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:mn>1</m:mn></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:math>
</div>
<figure id="f11_7" tabindex="0">
<img alt="" src="../images/fig11_7.jpg"/>
<figcaption><p><span class="blue">Figure 11.7.</span> A projective texture transformation uses a viewing-like transformation that projects toward a point.</p></figcaption>
</figure>
<p class="noindent">Now the 4×4 matrix <em>P<sub>t</sub></em> represents a projective (not necessarily affine) transformation—that is, the last row may not be [0,0,0,1].</p>
<p>Projective texture coordinates are important in the technique of shadow mapping, discussed in <a href="C16_chapter11.xhtml#sec11_4_4">Section 11.4.4</a>.</p>
</section>
<section>
<h4 id="sec11_2_1_2"><span class="blue">Spherical Coordinates</span></h4>
<p>For spheres, the latitude/longitude parameterization is familiar and widely used. It has a lot of distortion near the poles, which can lead to difficulties, but it does cover the whole sphere with discontinuities only along one line of latitude.</p>
<p>Surfaces that are roughly spherical in shape can be parameterized using a texture coordinate function that maps a point on the surface to a point on a sphere using radial projection: take a line from the center of the sphere through the point on the surface, and find the intersection with the sphere. The spherical coordinates of this intersection point are the texture coordinates of the point you started with on the surface.</p>
<p>Another way to say this is that you express the surface point in spherical coordinates (<em>ρ</em>,<em>θ</em>,<em>ϕ</em>) and then discard the <em>ρ</em> coordinate and map <em>θ</em> and <em>ϕ</em> each to the range [0,1]. The formula depends on the spherical coordinates convention; using the convention of <a href="C07_chapter2.xhtml#sec2_7_8">Section 2.7.8</a>,</p>
<aside class="boxed-text" epub:type="sidebar" id="c011_box4">
<p class="noindent">This and other texture coordinate functions in this chapter for objects that are in the box [-1, 1]<sup>3</sup> and centered at the origin.</p>
</aside>
<div class="disp-formula" id="uequ11_5">
<m:math alttext=""><m:mrow><m:mi>ϕ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mrow><m:mo>(</m:mo><m:mrow><m:mo stretchy="false">[</m:mo><m:mi>π</m:mi><m:mo>+</m:mo><m:mtext>atan2</m:mtext><m:mo stretchy="false">(</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo stretchy="false">]</m:mo><m:mo>/</m:mo><m:mn>2</m:mn><m:mi>π</m:mi><m:mo>,</m:mo><m:mo stretchy="false">[</m:mo><m:mi>π</m:mi><m:mo>−</m:mo><m:mtext>acos</m:mtext><m:mo stretchy="false">(</m:mo><m:mi>z</m:mi><m:mo>/</m:mo><m:mo>‖</m:mo><m:mi>x</m:mi><m:mo>‖</m:mo><m:mo stretchy="false">)</m:mo><m:mo stretchy="false">]</m:mo><m:mo>/</m:mo><m:mi>π</m:mi></m:mrow><m:mo>)</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:math>
</div>
<p>A spherical coordinates map will be bijective everywhere except at the poles if the whole surface is visible from the center point. It inherits the same distortion near the poles as the latitude–longitude map on the sphere. <a href="C16_chapter11.xhtml#f11_8">Figure 11.8</a> shows an object for which spherical coordinates provide a suitable texture coordinate function.</p>
<figure id="f11_8" tabindex="0">
<img alt="" src="../images/fig11_8.jpg"/>
<figcaption><p><span class="blue">Figure 11.8.</span> For this vaguely sphere-like object, projecting each point onto a sphere centered at the center of the object provides an injective mapping, which here is used to place the same map texture as was used for the globe images. Note that areas become magnified (surface points are crowded together in texture space) where the surface is far from the center, and areas shrink where the surface is closer to the center.</p></figcaption>
</figure>
</section>
<section>
<h4 id="sec11_2_1_3"><a id="index_term1165"/><span class="blue">Cylindrical Coordinates</span></h4>
<p>For objects that are more columnar than spherical, projection outward from an axis onto a cylinder may work better than projection from a point onto a sphere <a id="term-852"/><span aria-label="262" epub:type="pagebreak" id="pg_262" role="doc-pagebreak"/>(<a href="C16_chapter11.xhtml#f11_9">Figure 11.9</a>). Analogously to spherical projection, this amounts to converting to cylindrical coordinates and discarding the radius:</p>
<figure id="f11_9" tabindex="0">
<img alt="" src="../images/fig11_9.jpg"/>
<figcaption><p><span class="blue">Figure 11.9.</span> A far-from-spherical vase for which spherical projection produces a lot of distortion (left) and cylindrical projection produces a very good result on the outer surface.</p></figcaption>
</figure>
<div class="disp-formula" id="uequ11_6">
<m:math alttext=""><m:mrow><m:mi>ϕ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mrow><m:mo>(</m:mo><m:mrow><m:mstyle scriptlevel="+1"><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mn>2</m:mn><m:mi>π</m:mi></m:mrow></m:mfrac></m:mstyle><m:mo stretchy="false">[</m:mo><m:mi>π</m:mi><m:mo>+</m:mo><m:mtext>atan2</m:mtext><m:mo stretchy="false">(</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo stretchy="false">]</m:mo><m:mo>/</m:mo><m:mn>2</m:mn><m:mi>π</m:mi><m:mo>,</m:mo><m:mstyle scriptlevel="+1"><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac></m:mstyle><m:mo stretchy="false">[</m:mo><m:mn>1</m:mn><m:mo>+</m:mo><m:mi>z</m:mi><m:mo stretchy="false">]</m:mo></m:mrow><m:mo>)</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:math>
</div>
</section>
<section>
<h4 id="sec11_2_1_4"><a id="index_term1163"/><span class="blue">Cubemaps</span></h4>
<p>Using spherical coordinates to parameterize a spherical or sphere-like shape leads to high distortion of shape and area near the poles, which often leads to visible artifacts that reveal that there are two special points where something is going wrong with the texture. A popular alternative is much more uniform at the cost of having more discontinuities. The idea is to project onto a cube, rather than a sphere, and then use six separate square textures for the six faces of the cube. The collection of six square textures is called a <em>cubemap</em>. This introduces discontinuities along all the cube edges, but it keeps distortion of shape and area low.</p>
<p>Computing cubemap texture coordinates is also cheaper than for spherical coordinates, because projecting onto a plane just requires a division—essentially the same as perspective projection for viewing. For instance, for a point that projects onto the + <em>z</em> face of the cube:</p>
<div class="disp-formula" id="uequ11_7">
<m:math alttext=""><m:mrow><m:mrow><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo stretchy="false">)</m:mo><m:mo>↦</m:mo><m:mrow><m:mo>(</m:mo><m:mfrac><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mi>z</m:mi></m:mrow></m:mfrac><m:mo>,</m:mo><m:mfrac><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mi>z</m:mi></m:mrow></m:mfrac><m:mo>)</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:mrow></m:math>
</div>
<p>A confusing aspect of cubemaps is establishing the convention for how the u and v directions are defined on the six faces. Any convention is fine, but the convention chosen affects the contents of textures, so standardization is important. Because <a id="term-227"/><a id="term-853"/><span aria-label="263" epub:type="pagebreak" id="pg_263" role="doc-pagebreak"/>cubemaps are very often used for textures that are viewed from the inside of the cube (see environment mapping in <a href="C16_chapter11.xhtml#sec11_4_5">Section 11.4.5</a>), the usual conventions have the u and v axes oriented so that u is clockwise from v as viewed from inside. The convention used by OpenGL is</p>
<div class="disp-formula" id="uequ11_8">
<m:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><m:mrow><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi>ϕ</m:mi></m:mrow><m:mrow><m:mo>−</m:mo><m:mi>x</m:mi></m:mrow></m:msub><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac><m:mrow><m:mo>[</m:mo><m:mn>1</m:mn><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:mo>+</m:mo><m:mi>z</m:mi><m:mo>,</m:mo><m:mo>−</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mo>/</m:mo><m:mo>|</m:mo><m:mi>x</m:mi><m:mo>|</m:mo></m:mrow><m:mo>]</m:mo><m:mo>,</m:mo></m:mrow></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>ϕ</m:mi></m:mrow><m:mrow><m:mo>+</m:mo><m:mi>x</m:mi></m:mrow></m:msub><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac><m:mrow><m:mo>[</m:mo><m:mn>1</m:mn><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:mo>−</m:mo><m:mi>z</m:mi><m:mo>,</m:mo><m:mo>−</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mo>/</m:mo><m:mo>|</m:mo><m:mi>x</m:mi><m:mo>|</m:mo></m:mrow><m:mo>]</m:mo><m:mo>,</m:mo></m:mrow></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>ϕ</m:mi></m:mrow><m:mrow><m:mo>−</m:mo><m:mi>y</m:mi></m:mrow></m:msub><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac><m:mrow><m:mo>[</m:mo><m:mn>1</m:mn><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:mo>+</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mo>−</m:mo><m:mi>z</m:mi><m:mo>)</m:mo><m:mo>/</m:mo><m:mo>|</m:mo><m:mi>y</m:mi><m:mo>|</m:mo></m:mrow><m:mo>]</m:mo><m:mo>,</m:mo></m:mrow></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>ϕ</m:mi></m:mrow><m:mrow><m:mo>+</m:mo><m:mi>y</m:mi></m:mrow></m:msub><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac><m:mrow><m:mo>[</m:mo><m:mn>1</m:mn><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:mo>+</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mo>+</m:mo><m:mi>z</m:mi><m:mo>)</m:mo><m:mo>/</m:mo><m:mo>|</m:mo><m:mi>y</m:mi><m:mo>|</m:mo></m:mrow><m:mo>]</m:mo><m:mo>,</m:mo></m:mrow></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>ϕ</m:mi></m:mrow><m:mrow><m:mo>−</m:mo><m:mi>z</m:mi></m:mrow></m:msub><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac><m:mrow><m:mo>[</m:mo><m:mn>1</m:mn><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:mo>−</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mo>−</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mo>/</m:mo><m:mo>|</m:mo><m:mi>z</m:mi><m:mo>|</m:mo></m:mrow><m:mo>]</m:mo><m:mo>,</m:mo></m:mrow></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>ϕ</m:mi></m:mrow><m:mrow><m:mo>+</m:mo><m:mi>z</m:mi></m:mrow></m:msub><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac><m:mrow><m:mo>[</m:mo><m:mn>1</m:mn><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:mo>+</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mo>−</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mo>/</m:mo><m:mo>|</m:mo><m:mi>z</m:mi><m:mo>|</m:mo></m:mrow><m:mo>]</m:mo><m:mo>,</m:mo></m:mrow></m:mrow></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>The subscripts indicate which face of the cube each projection corresponds to. For example, <em><em>ϕ</em><sub>−x</sub></em> is used for points that project to the face of the cube at <em>x</em> = +1. You can tell which face a point projects to by looking at the coordinate with the largest absolute value: for example, if |<em>x</em>| &gt; |<em>y</em>| and |<em>x</em>| &gt; |<em>z</em>|, the point projects to the +<em>x</em> or −<em>x</em> face, depending on the sign of <em>x</em>.</p>
<p>A texture to be used with a cube map has six square pieces. (See <a href="C16_chapter11.xhtml#f11_10">Figure 11.10</a>.) Often they are packed together in a single image for storage, arranged as if the cube was unwrapped.</p>
<figure id="f11_10" tabindex="0">
<img alt="" src="../images/fig11_10.jpg"/>
<figcaption><p><span class="blue">Figure 11.10.</span> A surface being projected into a cubemap. Points on the surface project outward from the center, each mapping to a point on one of the six faces.</p></figcaption>
</figure>
</section>
</section>
<section>
<h3 id="sec11_2_2"><a id="index_term1169"/><span class="green">11.2.2 Interpolated Texture Coordinates</span></h3>
<p>For more fine-grained control over the texture coordinate function on a triangle mesh surface, you can explicitly store the texture coordinates at each vertex, <a id="term-403"/><a id="term-860"/><a id="term-861"/><a id="term-862"/><span aria-label="264" epub:type="pagebreak" id="pg_264" role="doc-pagebreak"/>and interpolate them across the triangles using barycentric interpolation (<a href="C14_chapter9.xhtml#sec9_1_2">Section 9.1.2</a>). It works in exactly the same way as any other smoothly varying quantities you might define over a mesh: colors, normals, even the 3D position itself.</p>
<aside class="boxed-text" epub:type="sidebar" id="c011_box5">
<p class="noindent">The idea of interpolated texture coordinates is very simple—but it can be a bit confusing at first.</p>
</aside>
<p class="indent">Let’s look at an example with a single triangle. <a href="C16_chapter11.xhtml#f11_11">Figure 11.11</a> shows a triangle texture mapped with part of the by now familiar test pattern. By looking at the pattern that appears on the rendered triangle, you can deduce that the texture coordinates of the three vertices are (0.2, 0.2), (0.8, 0.2), and (0.2, 0.8), because those are the points in the texture that appear at the three corners of the triangle. Just as with the geometrically determined mappings in the previous section, we control where the texture goes on the surface by giving the mapping from the surface to the texture domain, in this case by specifying where each vertex should go in texture space. Once you position the vertices, linear (barycentric) interpolation across triangles takes care of the rest.</p>
<figure id="f11_11" tabindex="0">
<img alt="" src="../images/fig11_11.jpg"/>
<figcaption><p><span class="blue">Figure 11.11.</span> A single triangle using linearly interpolated texture coordinates. (a) The triangle drawn in texture space; (b) the triangle rendered in a 3D scene.</p></figcaption>
</figure>
<p>In <a href="C16_chapter11.xhtml#f11_12">Figure 11.12</a>, we show a common way to visualize texture coordinates on a whole mesh: simply draw triangles in texture space with the vertices positioned at <a id="term-859"/><a id="term-891"/><a id="term-1141"/><span aria-label="265" epub:type="pagebreak" id="pg_265" role="doc-pagebreak"/>their texture coordinates. This visualization shows you what parts of the texture are being used by which triangles, and it is a handy tool for evaluating texture coordinates and for debugging all sorts of texture-mapping code.</p>
<figure id="f11_12" tabindex="0">
<img alt="" src="../images/fig11_12.jpg"/>
<figcaption><p><span class="blue">Figure 11.12.</span> An <a id="index_term1168"/>icosahedron with its triangles laid out in texture space to provide zero distortion but with many seams.</p></figcaption>
</figure>
<p>The quality of a texture coordinate mapping that is defined by vertex texture coordinates depends on what coordinates are assigned to the vertices—that is, how the mesh is laid out in texture space. No matter what coordinates are assigned, as long as the triangles in the mesh share vertices (<a href="C17_chapter12.xhtml#sec12_1">Section 12.1</a>), the texture coordinate mapping is always continuous, because neighboring triangles will agree on the texture coordinate at points on their shared edge. But the other desirable qualities described above are not so automatic. Injectivity means the triangles don’t overlap in texture space—if they do, it means there’s some point in the texture that will show up at more than one place on the surface.</p>
<p>Size distortion is low when the areas of triangles in texture space are in proportion to their areas in 3D. For instance, if a character’s face is mapped with a continuous texture coordinate function, one often ends up with the nose squeezed into a relatively small area in texture space, as shown in <a href="C16_chapter11.xhtml#f11_13">Figure 11.13</a>. Although triangles on the nose are smaller than on the cheek, the ratio of sizes is more extreme in texture space. The result is that the texture is enlarged on the nose, because a small area of texture has to cover a large area of surface. Similarly, comparing the forehead to the temple, the triangles are similar in size in 3D, but the triangles around the temple are larger in texture space, causing the texture to appear smaller there.</p>
<figure id="f11_13" tabindex="0">
<img alt="" src="../images/fig11_13.jpg"/>
<figcaption><p><span class="blue">Figure 11.13.</span> A <a id="index_term1166"/>face model, with texture coordinates assigned so as to achieve reasonably low shape distortion, but still showing moderate area distortion.</p></figcaption>
</figure>
<p>Similarly, shape distortion is low when the shapes of triangles are similar in 3D and in texture space. The face example has fairly low shape distortion, but, for example, the sphere in <a href="C16_chapter11.xhtml#f11_15">Figure 11.15</a> has very large shape distortion near the poles.</p>
</section>
<section>
<h3 id="sec11_2_3"><a id="index_term1175"/><a id="index_term1176"/><a id="index_term1174"/><span class="green">11.2.3 Tiling, Wrapping Modes, and Texture Transformations</span></h3>
<p>It’s often useful to allow texture coordinates to go outside the bounds of the texture image. Sometimes, this is a detail: rounding error in a texture coordinate calculation might cause a vertex that lands exactly on the texture boundary to be slightly outside, and the texture mapping machinery should not fail in that case. But it can also be a modeling tool.</p>
<p>If a texture is only supposed to cover part of the surface, but texture coordinates are already set up to map the whole surface to the unit square, one option is to prepare a texture image that is mostly blank with the content in a small area. But that might require a very high resolution texture image to get enough detail in the relevant area. Another alternative is to scale up all the texture coordinates so <span aria-label="266" epub:type="pagebreak" id="pg_266" role="doc-pagebreak"/>that they cover a larger range—[−4.5,5.5] × [−4.5,5.5] for instance, to position the unit square at one-tenth size in the center of the surface.</p>
<p>For a case like this, texture lookups outside the unit-square area that’s covered by the texture image should return a constant background color. One way to do this is to set a background color to be returned by texture lookups outside the unit square. If the texture image already has a constant background color (for instance, a logo on a white background), another way to extend this background automatically over the plane is to arrange for lookups outside the unit square to return the color of the texture image at the closest point on the edge, achieved by <em>clamping</em> the u and v coordinates to the range from the first pixel to the last pixel in the image.</p>
<p>Sometimes, we want a repeating pattern, such as a checkerboard, a tile floor, or a brick wall. If the pattern repeats on a rectangular grid, it would be wasteful to create an image with many copies of the same data. Instead, we can handle texture lookups outside the texture image using wraparound indexing—when the lookup point exits the right edge of the texture image, it wraps around to the left edge. This is handled very simply using the integer remainder operation on the pixel coordinates.</p>
<pre class="pre1">Color texture_lookup_wrap(Texture t, float u, float v) {
        int i = round(u ⋆ t.width() - 0.5)
        int j = round(v ⋆ t.height() - 0.5)
        return t.get_pixel(i % t.width(), j % t.height())
}
Color texture_lookup_wrap(Texture t, float u, float v) {
        int i = round(u ⋆ t.width() - 0.5)
        int j = round(v ⋆ t.height() - 0.5)
        return t.get_pixel(max(0, min(i, t.width()-1)),
                          (max(0, min(j, t.height()-1))))
}</pre>
<p>The choice between these two ways of handling out-of-bounds lookups is specified by selecting a <em>wrapping mode</em> from a list that includes tiling, clamping, and often combinations or variants of the two. With wrapping modes, we can freely think of a texture as a function that returns a color for any point in the infinite 2D plane (<a href="C16_chapter11.xhtml#f11_14">Figure 11.14</a>). When we specify a texture using an image, these modes describe how the finite image data are supposed to be used to define this function. In <a href="C16_chapter11.xhtml#sec11_5">Section 11.5</a>, we’ll see that procedural textures can naturally extend across an infinite plane, since they are not limited by finite image data. Since both are logically infinite in extent, the two types of textures are interchangeable.</p>
<p>When adjusting the scale and placement of textures, it’s convenient to avoid actually changing the functions that generate texture coordinates, or the texture coordinate values stored at vertices of meshes, by instead applying a matrix transformation <a id="term-189"/><a id="term-738"/><a id="term-869"/><a id="term-1142"/><span aria-label="267" epub:type="pagebreak" id="pg_267" role="doc-pagebreak"/>to the texture coordinates before using them to sample the texture:</p>
<figure id="f11_14" tabindex="0">
<img alt="" src="../images/fig11_14.jpg"/>
<figcaption><p><span class="blue">Figure 11.14.</span> A wood floor texture tiled over texture space by wrapping texel coordinates.</p></figcaption>
</figure>
<div class="disp-formula" id="uequ11_9">
<m:math alttext=""><m:mrow><m:mi>ϕ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mtext> </m:mtext><m:msub><m:mrow><m:mi>M</m:mi></m:mrow><m:mrow><m:mi>T</m:mi></m:mrow></m:msub><m:msub><m:mrow><m:mi>ϕ</m:mi></m:mrow><m:mrow><m:mtext>model</m:mtext></m:mrow></m:msub><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo>,</m:mo></m:mrow></m:math>
</div>
<p>where <em>ϕ</em><sub>model</sub> is the texture coordinate function provided with the model, and <em>M</em><sub>T</sub> is a 3 by 3 matrix representing an affine or projective transformation of the 2D texture coordinates using <a id="index_term530"/>homogeneous coordinates. Such a transformation, sometimes limited just to scaling and/or translation, is supported by most renderers that use texture mapping.</p>
</section>
<section>
<h3 id="sec11_2_4"><a id="index_term1027"/><span class="green">11.2.4 Continuity and Seams</span></h3>
<p>Although low distortion and continuity are nice properties to have in a texture coordinate function, discontinuities are often unavoidable. For any closed 3D surface, it’s a basic result of topology that there is no continuous, bijective function that maps the whole surface into a texture image. Something has to give, and by introducing seams—curves on the surface where the texture coordinates change suddenly—we can have low distortion everywhere else. Many of the geometrically determined mappings discussed above already contain seams: in spherical and cylindrical coordinates, the seams are where the angle computed by atan2 wraps around from π to - π, and in the cubemap, the seams are along the cube edges, where the mapping switches between the six square textures.</p>
<p><a id="term-826"/><a id="term-863"/><a id="term-868"/><span aria-label="268" epub:type="pagebreak" id="pg_268" role="doc-pagebreak"/>With interpolated texture coordinates, seams require special consideration, because they don’t happen naturally. We observed earlier that interpolated texture coordinates are automatically continuous on shared-vertex meshes—the sharing of texture coordinates guarantees it. But this means that if a triangle spans a seam, with some vertices on one side and some on the other, the interpolation machinery will cheerfully provide a continuous mapping, but it will likely be highly distorted or fold over so that it’s not injective. <a href="C16_chapter11.xhtml#f11_15">Figure 11.15</a> illustrates this problem on a globe mapped with spherical coordinates. For example, there is a triangle near the bottom of the globe that has one vertex at the tip of New Zealand’s South Island, and another vertex in the Pacific about 400 km northeast of the North Island. A sensible pilot flying between these points would fly over New Zealand, but the path starts at longitude 167° E (+167) and ends at 179° W (i.e., longitude −179), so linear interpolation chooses a route that crosses South America on the way. This causes a backward copy of the entire map to be compressed into the strip of triangles that crosses the 180th meridian! The solution is to label the second vertex with the equivalent longitude of 181° E, but this just pushes the problem to the next triangle.</p>
<figure id="f11_15" tabindex="0">
<img alt="" src="../images/fig11_15.jpg"/>
<figcaption><p><span class="blue">Figure 11.15.</span> Polygonal globes: on the left, with all shared vertices, the texture coordinate function is continuous, but necessarily has problems with triangles that cross the 180th meridian, because texture coordinates are interpolated from longitudes near 180 to longitudes near −180. On the right, some vertices are duplicated, with identical 3D positions but texture coordinates differing by exactly 360° in longitude, so that texture coordinates are interpolated across the meridian rather than all the way across the map.</p></figcaption>
</figure>
<p>The only way to create a clean transition is to avoid sharing texture coordinates at the seam: the triangle crossing New Zealand needs to interpolate to longitude +181, and the next triangle in the Pacific needs to continue starting from to longitude −179. To do this, we duplicate the vertices at the seam: for each vertex, we add a second vertex with an equivalent longitude, differing by 360°, and the triangles on opposite sides of the seam use different vertices. This solution is shown in the right half of <a href="C16_chapter11.xhtml#f11_15">Figure 11.15</a>, in which the vertices at the far left and right of the texture space are duplicates, with the same 3D positions.</p>
</section>
<section>
<h3 id="sec11_2_5"><span aria-label="269" epub:type="pagebreak" id="pg_269" role="doc-pagebreak"/><span class="green">11.2.5 Texture coordinates in rendering systems</span></h3>
<p>Textures are used in all kinds of rendering systems, and although the fundamentals are the same, the details are different for ray tracing and rasterization systems.</p>
<p>Texture coordinates are part of the model being rendered, and the scene description needs to include enough information to define what they are. Mostly, this means storing texture coordinates as per-vertex attributes of all triangle meshes that will be used with textures. If the rendering system directly supports geometric primitives other than meshes, these primitives usually have pre-defined texture coordinates (e.g., latitude–longitude coordinates on spheres), possibly with a choice of mapping schemes for each primitive type.</p>
<p>In a ray tracing renderer, each type of surface that supports ray intersection must be able to compute not just the intersection point and surface normal, but also the texture coordinates of the intersection point. Like the other information about the intersection, texture coordinates can be stored in a hit record (see <a href="C09_chapter4.xhtml#sec4_4_3">Section 4.4.3</a>). In the common case of geometry represented by triangle meshes, the ray–triangle intersection code will compute texture coordinates by barycentric interpolation from the texture coordinates stored at the vertices, and for other types of geometry, the intersection code must compute the texture coordinates directly.</p>
<p>In a rasterization-based system, triangles will normally be the only supported type of geometry, so all surfaces must be converted to this form. Texture coordinates can be read in with the model (the common case), or for triangle meshes that are generated in code, they can be computed and stored at the time the mesh is created. Alternatively, for texture coordinates that can be computed from other vertex data (for instance, where texture coordinates are computed from the 3D position), texture coordinates can also be computed in a vertex shader and passed on to the rasterizer. Texture coordinates are then interpolated by the rasterizer, so that every invocation of the fragment shader has the appropriate texture coordinates for its fragment.</p>
</section>
</section>
<section>
<h2 id="sec11_3"><a id="index_term54"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec11_3" role="doc-backlink"><span class="green">11.3 Antialiasing Texture Lookups</span></a></h2>
<p>The second fundamental problem of texture mapping is antialiasing. Rendering a texture mapped image is a sampling process: mapping the texture onto the surface and then projecting the surface into the image produce a 2D function across the image plane, and we are sampling it at pixels. As we saw in <a href="C15_chapter10.xhtml#c10">Chapter 10</a>, doing this using point samples will produce aliasing artifacts when the image contains detail or sharp edges—and since the whole point of textures is to introduce detail, <span aria-label="270" epub:type="pagebreak" id="pg_270" role="doc-pagebreak"/>they become a prime source of aliasing problems like the ones we saw in <a href="C16_chapter11.xhtml#f11_3">Figure 11.3</a>.</p>
<aside class="boxed-text" epub:type="sidebar" id="c011_box6">
<p class="noindent">It’s a good idea to review the first half of <a href="C15_chapter10.xhtml#c10">Chapter 10</a> now.</p>
</aside>
<p class="indent">Just as with antialiased rasterization of lines or triangles (<a href="C14_chapter9.xhtml#sec9_3">Section 9.3</a>), antialiased ray tracing, or downsampling images (<a href="C15_chapter10.xhtml#sec10_4">Section 10.4</a>), the solution is to make each pixel not a point sample but an area average of the image, over an area similar in size to the pixel. Using the same supersampling approach used for antialiased rasterization and ray tracing, with enough samples, excellent results can be obtained with no changes to the texture mapping machinery: many samples within a pixel’s area will land at different places in the texture map, and averaging the shading results computed using the different texture lookups is an accurate way to approximate the average color of the image over the pixel. However, with detailed textures it takes very many samples to get good results, which is slow. Computing this area average <em>efficiently</em> in the presence of textures on the surface is the first key topic in texture antialiasing.</p>
<p>Texture images are usually defined by raster images, so there is also a reconstruction problem to be considered, just as with upsampling images (<a href="C15_chapter10.xhtml#sec10_4">Section 10.4</a>). The solution is the same for textures: use a reconstruction filter to interpolate between texels.</p>
<p>We expand on each of these topics in the following sections.</p>
<section>
<h3 id="sec11_3_1"><a id="index_term869"/><span class="green">11.3.1 The Footprint of a Pixel</span></h3>
<p>What makes antialiasing textures more complex than other kinds of antialiasing is that the relationship between the rendered image and the texture is constantly changing. Every pixel value should be computed as an average color over the area belonging to the pixel in the image, and in the common case that the pixel is looking at a single surface, this corresponds to averaging over an area on the surface. If the surface color comes from a texture, this in turn amounts to averaging over a corresponding part of the texture, known as the <em>texture space footprint</em> of the pixel. <a href="C16_chapter11.xhtml#f11_16">Figure 11.16</a> illustrates how the footprints of square areas (which could be pixel areas in a lower-resolution image) map to very different sized and shaped areas in the floor’s texture space.</p>
<figure id="f11_16" tabindex="0">
<img alt="" src="../images/fig11_16.jpg"/>
<figcaption><p><span class="blue">Figure 11.16.</span> The footprints in texture space of identically sized square areas in the image vary in size and shape across the image.</p></figcaption>
</figure>
<p>Recall the three spaces involved in rendering with textures: the projection π that maps 3D points into the image and the texture coordinate function <em>ϕ</em> that maps 3D points into texture space. To work with pixel footprints, we need to understand the composition of these two mappings: first follow π backwards to get from the image to the surface and then follow <em>ϕ</em> forwards. This composition <em>ψ</em> = <em>ϕ</em> ∘ π<sup>-1</sup> is what determines pixel footprints: the footprint of a pixel is the image of that pixel’s square area of the image under the mapping <em>ψ</em>.</p>
<p>The core problem in texture antialiasing is computing an average value of <a id="term-617"/><a id="term-873"/><span aria-label="271" epub:type="pagebreak" id="pg_271" role="doc-pagebreak"/>the texture over the footprint of a pixel. To do this exactly in general could be a pretty complicated job: for a faraway object with a complicated surface shape, the footprint could be a complicated shape covering a large area, or possibly several disconnected areas, in texture space. But in the typical case, a pixel lands in a smooth area of surface that is mapped to a single area in the texture.</p>
<p>Because <em><em>ψ</em></em> contains both the mapping from image to surface and the mapping from surface to texture, the size and shape of the footprint depend on both the viewing situation and the texture coordinate function. When a surface is closer to the camera, pixel footprints will be smaller; when the same surface moves farther away, the footprint gets bigger. When surfaces are viewed at an oblique angle, the footprint of a pixel on the surface is elongated, which usually means it will be elongated in texture space also. Even with a fixed view, the texture coordinate function can cause variations in the footprint: if it distorts area, the size of footprints will vary, and if it distorts shape, they can be elongated even for head-on views of the surface.</p>
<p>However, to find an efficient algorithm for computing antialiased lookups, some substantial approximations will be needed. When a function is smooth, a linear approximation is often useful. In the case of texture antialiasing, this means approximating the mapping <em>ψ</em> from image space to texture space as a linear mapping from 2D to 2D:</p>
<aside class="boxed-text" epub:type="sidebar" id="c011_box7">
<p class="noindent">In mathematicians’ terms, we have made a one-term Taylor series approximation to the function <em>ψ</em>.</p>
</aside>
<div class="disp-formula" id="uequ11_10">
<m:math xmlns:mml="http://www.w3.org/1998/Math/MathML" alttext=""><m:mrow><m:mi>ψ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mi>ψ</m:mi><m:mo stretchy="false">(</m:mo><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo stretchy="false">)</m:mo><m:mo>+</m:mo><m:mtext> </m:mtext><m:mi mathvariant="bold">J</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>−</m:mo><m:mtext> </m:mtext><m:msub><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>0</m:mn></m:mrow></m:msub><m:mo stretchy="false">)</m:mo><m:mo>,</m:mo></m:mrow></m:math>
</div>
<p>where the two-by-two matrix <strong>J</strong> is some approximation to the derivative of <em><em>ψ</em></em>. It has four entries, and if we denote the image-space position as <strong>x</strong> = (<em>x,y</em>) and the <a id="term-618"/><a id="term-874"/><span aria-label="272" epub:type="pagebreak" id="pg_272" role="doc-pagebreak"/>texture-space position as <strong>u</strong> = (<em>u,v</em>), then</p>
<div class="disp-formula" id="uequ11_11">
<m:math alttext=""><m:mrow><m:mrow><m:mi mathvariant="bold">J</m:mi><m:mo>=</m:mo><m:mrow><m:mo>[</m:mo><m:mtable><m:mtr><m:mtd><m:mfrac><m:mrow><m:mi>d</m:mi><m:mi>u</m:mi></m:mrow><m:mrow><m:mi>d</m:mi><m:mi>x</m:mi></m:mrow></m:mfrac></m:mtd><m:mtd><m:mfrac><m:mrow><m:mi>d</m:mi><m:mi>u</m:mi></m:mrow><m:mrow><m:mi>d</m:mi><m:mi>y</m:mi></m:mrow></m:mfrac></m:mtd></m:mtr><m:mtr><m:mtd><m:mfrac><m:mrow><m:mi>d</m:mi><m:mi>v</m:mi></m:mrow><m:mrow><m:mi>d</m:mi><m:mi>x</m:mi></m:mrow></m:mfrac></m:mtd><m:mtd><m:mfrac><m:mrow><m:mi>d</m:mi><m:mi>v</m:mi></m:mrow><m:mrow><m:mi>d</m:mi><m:mi>y</m:mi></m:mrow></m:mfrac></m:mtd></m:mtr></m:mtable><m:mo>]</m:mo></m:mrow><m:mo>,</m:mo></m:mrow></m:mrow></m:math>
</div>
<p>where the four derivatives describe how the texture point (<em>u,v</em>) that is seen at a point (<em>x,y</em>) in the image changes when we change <em>x</em> and <em>y</em>.</p>
<p>A geometric interpretation of this approximation (<a href="C16_chapter11.xhtml#f11_17">Figure 11.17</a>) is that it says a unit-sized square pixel area centered at <strong>x</strong> in the image will map approximately to a parallelogram in texture space, centered at <em><em>ψ</em></em>(<strong>x</strong>) and with its edges parallel to the vectors <strong>u</strong><sub>x</sub> = (<em>du/dx,dv/dx</em>) and <strong>u</strong><sub>y</sub> = (<em>du/dy,dv/dy</em>).</p>
<figure id="f11_17" tabindex="0">
<img alt="" src="../images/fig11_17.jpg"/>
<figcaption><p><span class="blue">Figure 11.17.</span> An approximation of the texture-space footprint of a pixel can be made using the derivative of the mapping from (<em>x,y</em>) to (<em>u,v</em>). The partial derivatives with respect to <em>x</em> and <em>y</em> are parallel to the images of the <em>x</em> and <em>y</em> isolines (blued) and span a parallelogram (shaded in orange) that approximates the curved shape of the exact footprint (outlined in black).</p></figcaption>
</figure>
<p>The derivative matrix <strong>J</strong> is useful because it tells the whole story of variation in the (approximated) texture-space footprint across the image. Derivatives that are larger in magnitude indicate larger texture-space footprints, and the relationship between the derivative vectors <strong>u</strong><sub>x</sub> and <strong>u</strong><sub>y</sub> indicates the shape. When they are orthogonal and the same length, the footprint is square, and as they become skewed and/or very different in length, the footprint becomes elongated.</p>
<aside class="boxed-text" epub:type="sidebar" id="c011_box8">
<p class="noindent">The approach here uses a box filter to sample the image. Some systems instead use a Gaussian pixel filter, which becomes an elliptical Gaussian in texture space; this is elliptical weighted averaging (EWA).</p>
</aside>
<p class="indent">We’ve now reached the form of the problem that’s usually thought of as the “right answer”: a <em>filtered texture sample</em> at a particular image-space position should be the average value of the texture map over the parallelogram-shaped footprint defined by the texture coordinate derivatives at that point. This already has some assumptions baked into it—namely, that the mapping from image to <a id="term-619"/><a id="term-875"/><span aria-label="273" epub:type="pagebreak" id="pg_273" role="doc-pagebreak"/>texture is smooth—but it is sufficiently accurate for excellent image quality. However, this parallelogram area average is already too expensive to compute exactly, so various approximations are used. Approaches to texture antialiasing differ in the speed/quality tradeoffs they make in approximating this lookup. We discuss these in the following sections.</p>
</section>
<section>
<h3 id="sec11_3_2"><a id="index_term873"/><span class="green">11.3.2 Reconstruction</span></h3>
<p>When the footprint is smaller than a texel, we are magnifying the texture as it is mapped into the image. This case is analogous to upsampling an image, and the main consideration is interpolating between texels to produce a smooth image in which the texel grid is not obvious. Just as in image upsampling, this smoothing process is defined by a reconstruction filter that is used to compute texture samples at arbitrary locations in texture space. (See <a href="C16_chapter11.xhtml#f11_18">Figure 11.18</a>.)</p>
<figure id="f11_18" tabindex="0">
<img alt="" src="../images/fig11_18.jpg"/>
<figcaption><p><span class="blue">Figure 11.18.</span> The dominant issues in texture filtering change with the footprint size. For small footprints (left) interpolating between <a id="index_term1183"/>pixels is needed to avoid blocky artifacts; for large footprints, the challenge is to efficiently find the average of many pixels.</p></figcaption>
</figure>
<p>The considerations are pretty much the same as in image resampling, with one important difference. In image resampling, the task is to compute output samples on a regular grid and that regularity enabled an important optimization in the case of a separable reconstruction filter. In texture filtering, the pattern of lookups is not regular, and the samples have to be computed separately. This means large, high-quality reconstruction filters are very expensive to use, and <span aria-label="274" epub:type="pagebreak" id="pg_274" role="doc-pagebreak"/>for this reason the highest-quality filter normally used for textures is bilinear interpolation.</p>
<p>The calculation of a bilinearly interpolated texture sample is the same as computing one pixel in an image being upsampled with bilinear interpolation. First, we express the texture-space sample point in terms of (real-valued) texel coordinates, then we read the values of the four neighboring texels and average them. Textures are usually parameterized over the unit square, and the texels are located in the same way as pixels in any image, spaced a distance 1/<em>n<sub>u</sub></em> apart in the u direction and 1/<em>n<sub>v</sub></em> in <em>v</em>, with texel (0,0) positioned half a texel in from the edge for symmetry. (See <a href="C15_chapter10.xhtml#c10">Chapter 10</a> for the full explanation.)</p>
<pre class="pre1">Color tex_sample_bilinear(Texture t, float u, float v) {
    u_p = u ⋆ t.width - 0.5
    v_p = v ⋆ t.height - 0.5
    iu0 = floor(u_p); iu1 = iu0 + 1
    iv0 = floor(v_p); iv1 = iv0 + 1
    a_u = (iu1 - u_p); b_u = 1 - a_u
    a_v = (iv1 - v_p); b_v = 1 - a_v
    return a_u ⋆ a_v ⋆ t[iu0][iv0] + a_u ⋆ b_v ⋆ t[iu0][iv1] +
           b_u ⋆ a_v ⋆ t[iu1][iv0] + b_u ⋆ b_v ⋆ t[iu1][iv1]
}</pre>
<p>In many systems, this operation becomes an important performance bottleneck, mainly because of the memory latency involved in fetching the four texel values from the texture data. The pattern of sample points for textures is irregular, because the mapping from image to texture space is arbitrary, but often coherent, since nearby image points tend to map to nearby texture points that may read the same texels. For this reason, high-performance systems have special hardware devoted to texture sampling that handles interpolation and manages caches of recently used texture data to minimize the number of slow data fetches from the memory where the texture data are stored.</p>
<p>After reading <a href="C15_chapter10.xhtml#c10">Chapter 10</a>, you may complain that linear interpolation may not be a smooth enough reconstruction for some demanding applications. However, it can always be made good enough by resampling the texture to a somewhat higher resolution using a better filter, so that the texture is smooth enough that bilinear interpolation works well.</p>
</section>
<section>
<h3 id="sec11_3_3"><span class="green">11.3.3 Mipmapping</span></h3>
<p>Doing a good job of interpolation only suffices in situations where the texture is being magnified: where the pixel footprint is small compared to the spacing of texels. When a pixel footprint covers many texels, good antialiasing requires <span aria-label="275" epub:type="pagebreak" id="pg_275" role="doc-pagebreak"/>computing the average of many texels to smooth out the signal so that it can be sampled safely.</p>
<p>One very accurate way to compute the average texture value over the footprint would be to find all the texels within the footprint and add them up. However, this is potentially very expensive when the footprint is large—it could require reading many thousands of texel just for a single lookup. A better approach is to precompute and store the averages of the texture over various areas of different size and position.</p>
<aside class="boxed-text" epub:type="sidebar" id="c011_box9">
<p class="noindent">The name “mip” stands for the Latin phrase <em>multim in parvo</em> meaning “much in a small space.”</p>
</aside>
<p class="indent">A very popular version of this idea is known as “MIP mapping” or just <a id="index_term741"/><a id="index_term1182"/>mipmapping. A mipmap is a sequence of textures that all contain the same image but at lower and lower resolution. The original, full-resolution texture image is called the <em><a id="index_term73"/>base level</em>, or level 0, of the mipmap, and level 1 is generated by taking that image and downsampling it by a factor of 2 in each dimension, resulting in an image with one-fourth as many texels. The texels in this image are, roughly speaking, averages of square areas 2 by 2 texels in size in the level-0 image.</p>
<p>This process can be continued to define as many mipmap levels as desired: the image at level k is computed by downsampling the image at level <em>k</em> − 1 by two. A texel at level k corresponds to a square area measuring 2<sup><em>k</em></sup> by 2<sup><em>k</em></sup> texels in the original texture. For instance, starting with a 1024 × 1024 texture image, we could generate a mipmap with 11 levels: level 0 is 1024 × 1024; level 1 is 512 × 512, and so on until level 10, which has just a single texel. This kind of structure, with images that represent the same content at a series of lower and lower sampling rates, is called an <em>image pyramid</em>, based on the visual metaphor of stacking all the smaller images on top of the original.</p>
</section>
<section>
<h3 id="sec11_3_4"><span class="green">11.3.4 Basic Texture Filtering with Mipmaps</span></h3>
<p>With the mipmap, or <a id="index_term552"/>image pyramid, in hand, texture filtering can be done much more efficiently than by accessing many texels individually. When we need a texture value averaged over a large area, we simply use values from higher levels of the mipmap, which are already averages over large areas of the image. The simplest and fastest way to do this is to look up a single value from the mipmap, choosing the level so that the size covered by the texels at that level is roughly the same as the overall size of the pixel footprint. Of course, the pixel footprint might be quite different in shape from the (always square) area represented by the texel, and we can expect that to produce some artifacts.</p>
<p>Setting aside for a moment the question of what to do when the pixel footprint has an elongated shape, suppose the footprint is a square of width <em>D</em>, <a id="term-23"/><span aria-label="276" epub:type="pagebreak" id="pg_276" role="doc-pagebreak"/>measured in terms of texels in the full-resolution texture. What level of the mipmap is it appropriate to sample? Since the texels at level <em>k</em> cover squares of width 2<sup><em>k</em></sup>, it seems appropriate to choose k so that</p>
<div class="disp-formula" id="uequ11_12">
<m:math alttext=""><m:mrow><m:msup><m:mrow><m:mn>2</m:mn></m:mrow><m:mrow><m:mi>k</m:mi></m:mrow></m:msup><m:mo>≈</m:mo><m:mi>D</m:mi></m:mrow></m:math>
</div>
<p>so we let <em>k = log</em><sub>2</sub><em>D</em>. Of course, this will give non-integer values of k most of the time, and we only have stored mipmap images for integer levels. Two possible solutions are to look up a value only for the integer nearest to k (efficient but produces seams at the abrupt transitions between levels) or to look up values for the <em>two</em> nearest integers to k and linearly interpolate the values (twice the work, but smoother).</p>
<p>Before we can actually write down the algorithm for sampling a mipmap, we have to decide how we will choose the “width” D when footprints are not square. Some possibilities might be to use the square root of the area or to find the longest axis of the footprint and call that the width. A practical compromise that is easy to compute is to use the length of the longest edge:</p>
<div class="disp-formula" id="uequ11_13">
<m:math alttext=""><m:mrow><m:mrow><m:mi>D</m:mi><m:mo>=</m:mo><m:mi>max</m:mi><m:mrow><m:mo>{</m:mo><m:mrow><m:mo>‖</m:mo><m:msub><m:mrow><m:mi mathvariant="bold">u</m:mi></m:mrow><m:mrow><m:mi>x</m:mi></m:mrow></m:msub><m:mo>‖</m:mo><m:mo>,</m:mo><m:mo>‖</m:mo><m:msub><m:mrow><m:mi mathvariant="bold">u</m:mi></m:mrow><m:mrow><m:mi>y</m:mi></m:mrow></m:msub><m:mo>‖</m:mo></m:mrow><m:mo>}</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:mrow></m:math>
</div>
<pre class="pre1">Color mipmap_sample_trilinear(Texture mip[], float u, float v,
        matrix J) {
        D = max_column_norm(J)
        k = log2(D)
        k0 = floor(k); k1 = k0 + 1
        a = k1 - k; b = 1 - a
        c0 = tex_sample_bilinear(mip[k0], u, v)
        c1 = tex_sample_bilinear(mip[k1], u, v)
        return a ⋆ c0 + b ⋆ c1
}</pre>
<p>Basic mipmapping does a good job of removing aliasing, but because it’s unable to handle elongated, or <em>anisotropic</em> pixel footprints, it doesn’t perform well when surfaces are viewed at grazing angles. This is most commonly seen on large planes that represent a surface the viewer is standing on. Points on the floor that are far away are viewed at very steep angles, resulting in very anisotropic footprints that mipmapping approximates with much larger square areas. The resulting image will appear blurred in the horizontal direction.</p>
</section>
<section>
<h3 id="sec11_3_5"><a id="index_term49"/><span class="green">11.3.5 Anisotropic Filtering</span></h3>
<p>A mipmap can be used with multiple lookups to approximate an elongated <a id="index_term870"/>footprint better. The idea is to select the mipmap level based on the <em>shortest</em> axis of the <a id="term-22"/><span aria-label="277" epub:type="pagebreak" id="pg_277" role="doc-pagebreak"/>footprint rather than the largest and then average together several lookups spaced along the long axis. (See <a href="C16_chapter11.xhtml#f11_19">Figure 11.19</a>.)</p>
<figure id="f11_19" tabindex="0">
<img alt="" src="../images/fig11_19.jpg"/>
<figcaption><p><span class="blue">Figure 11.19.</span> The results of antialiasing a challenging test scene (reference images showing detailed structure, at left) using three different strategies: simply taking a single point sample with nearest-neighbor interpolation; using a mipmap pyramid to average a square area in the texture for each pixel; using several samples from a mipmap to average an anisotropic region in the texture.</p></figcaption>
</figure>
</section>
</section>
<section>
<h2 id="sec11_4"><a id="index_term1178"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec11_4" role="doc-backlink"><span class="green">11.4 Applications of Texture Mapping</span></a></h2>
<p>Once you understand the idea of defining texture coordinates for a surface and the machinery of looking up texture values, this machinery has many uses. In this section, we survey a few of the most important techniques in texture mapping, but textures are a very general tool with applications limited only by what the programmer can think up.</p>
<section>
<h3 id="sec11_4_1"><a id="index_term1038"/><a id="index_term1066"/><span class="green">11.4.1 Controlling Shading Parameters</span></h3>
<p>The most basic use of texture mapping is to introduce variation in color by making the diffuse color that is used in shading computations—whether in a ray tracer or in a fragment shader—dependent on a value looked up from a texture. A textured diffuse component can be used to paste decals, paint decorations, or print text on a surface, and it can also simulate the variation in material color, for example, for wood or stone.</p>
<p>Nothing limits us to varying only the diffuse color, though. Any other parameters, such as the specular reflectance or specular roughness, can also be textured. For instance, a cardboard box with transparent packing tape stuck to it may have the same diffuse color everywhere but be shinier, with higher specular reflectance and lower roughness, where the tape is than elsewhere. In many cases, the maps for different parameters are correlated: for instance, a glossy white ceramic cup with a logo printed on it may be both rougher and darker where it is printed <a id="term-749"/><span aria-label="278" epub:type="pagebreak" id="pg_278" role="doc-pagebreak"/>(<a href="C16_chapter11.xhtml#f11_20">Figure 11.20</a>), and a book with its title printed in metallic ink might change in diffuse color, specular color, and roughness, all at once.</p>
<figure id="f11_20" tabindex="0">
<img alt="" src="../images/fig11_20.jpg"/>
<figcaption><p><span class="blue">Figure 11.20.</span> A ceramic mug with specular roughness controlled by an inverted copy of the diffuse color texture.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec11_4_2"><a id="index_term783"/><a id="index_term115"/><span class="green">11.4.2 Normal Maps and Bump Maps</span></h3>
<p>Another quantity that is important for shading is the surface normal. With interpolated normals (<a href="C14_chapter9.xhtml#sec9_2">Section 9.2</a>), we know that the shading normal does not have to be the same as the geometric normal of the underlying surface. <em>Normal mapping</em> takes advantage of this fact by making the shading normal depend on values read from a texture map. The simplest way to do this is just to store the normals in a texture, with three numbers stored at every texel that are interpreted, instead of as the three components of a color, as the 3D coordinates of the normal vector.</p>
<p>Before a normal map can be used, though, we need to know what coordinate system the normals read from the map are represented in. Storing normals directly in object space, in the same coordinate system used for representing the surface geometry itself, is simplest: the normal read from the map can be used in exactly the same way as the normal reported by the surface itself: in most cases, it will need to be transformed into world space for lighting calculations, just like a normal that came with the geometry.</p>
<p>However, normal maps that are stored in object space are inherently tied to the surface geometry—even for the normal map to have no effect, to reproduce the result with the geometric normals, the contents of the normal map have to track the orientation of the surface. Furthermore, if the surface is going to deform, so that the geometric normal changes, the object-space normal map can no longer be used, since it would keep providing the same shading normals.</p>
<p>The solution is to define a coordinate system for the normals that is attached to the surface. Such a coordinate system can be defined based on the tangent space of the surface (see <a href="C07_chapter2.xhtml#sec2_7">Section 2.7</a>): select a pair of tangent vectors and use them to define an orthonormal basis (<a href="C07_chapter2.xhtml#sec2_4_5">Section 2.4.5</a>). The texture coordinate function itself provides a useful way to select a pair of tangent vectors: use the directions tangent to lines of constant u and v. These tangents are not generally orthogonal, but we can use the procedure from <a href="C07_chapter2.xhtml#sec2_4_7">Section 2.4.7</a> to “square up” the orthonormal basis, or it can be defined using the surface normal and just one tangent vector.</p>
<p>When normals are expressed in this basis they vary a lot less; since they are mostly pointing near the direction of the normal to the smooth surface, they will be near the vector (0,0,1)<sup><em>T</em></sup> in the normal map.</p>
<p><a id="term-75"/><a id="term-555"/><a id="term-750"/><span aria-label="279" epub:type="pagebreak" id="pg_279" role="doc-pagebreak"/>Where do normal maps come from? Often they are computed from a more detailed model to which the smooth surface is an approximation; other times, they can be measured directly from real surfaces. They can also be authored as part of the modeling process; in this case, it’s often nice to use a <em>bump map</em> to specify the normals indirectly. The idea is that a bump map is a height field: a function that give the local height of the detailed surface above the smooth surface. Where the values are high (where the map looks bright, if you display it as an image), the surface is protruding outside the smooth surface; where the values are low (where the map looks dark), the surface is receding below it. For instance, a narrow dark line in the bump map is a scratch, or a small white dot is a bump.</p>
<p>Deriving a normal map from a bump map is simple: the normal map (expressed in the tangent frame) is the derivative of the bump map.</p>
<p><a href="C16_chapter11.xhtml#f11_21">Figure 11.21</a> shows texture maps being used to create woodgrain color and to simulate increased surface roughness due to finish soaking into the more porous parts of the wood, together with a bump map to create an imperfect finish and gaps between boards, to make a realistic wood floor.</p>
<figure id="f11_21" tabindex="0">
<img alt="" src="../images/fig11_21.jpg"/>
<figcaption><p><span class="blue">Figure 11.21.</span> A wood floor rendered using texture maps to control the shading. (a) Only the diffuse color is modulated by a texture map. (b) The specular roughness is also modulated by a second texture map. (c) The surface normal is modified by a bump map.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec11_4_3"><span aria-label="280" epub:type="pagebreak" id="pg_280" role="doc-pagebreak"/><span class="green">11.4.3 Displacement Maps</span></h3>
<p>A problem with normal maps is that they don’t actually change the surface at all; they are just a shading trick. This becomes obvious when the geometry implied by the normal map should cause noticeable effects in 3D. In still images, the first problem to be noticed is usually that the silhouettes of objects remain smooth despite the appearance of bumps in the interior. In animations, the lack of parallax gives away that the bumps, however convincing, are really just “painted” on the surface.</p>
<p>Textures can be used for more than just shading, though: they can be used to alter geometry. A displacement map is one of the simplest versions of this idea. The concept is the same as a bump map: a scalar (one-channel) map that gives the height above the “average terrain.” But the effect is different. Rather than deriving a shading normal from the height map while using the smooth geometry, a displacement map actually changes the surface, moving each point along the normal of the smooth surface to a new location. The normals are roughly the same in each case, but the surface is different.</p>
<p>The most common way to implement displacement maps is to tessellate the smooth surface with a large number of small triangles and then displace the vertices of the resulting mesh using the <a id="index_term337"/>displacement map. In the graphics pipeline, this can be done using a texture lookup at the vertex stage and is particularly handy for terrain.</p>
</section>
<section>
<h3 id="sec11_4_4"><a id="index_term1068"/><span class="green">11.4.4 Shadow Maps</span></h3>
<p>Shadows are an important cue to object relationships in a scene, and as we have seen, they are simple to include in ray-traced images. However, it’s not obvious how to get shadows in rasterized renderings, because surfaces are considered one at a time, in isolation. Shadow maps are a technique for using the machinery of texture mapping to get shadows from point light sources.</p>
<p>The idea of a shadow map is to represent the volume of space that is illuminated by a point light source. Think of a source like a spotlight or video projector, which emits light from a point into a limited range of directions. The volume that is illuminated—the set of points where you would see light on your hand if you held it there—is the union of line segments joining the light source to the closest surface point along every ray leaving that point.</p>
<p>Interestingly, this volume is the same as the volume that is visible to a perspective camera located at the same point as the light source: a point is illuminated by a source if and only if it is visible from the light source location. In both <a id="term-782"/><span aria-label="281" epub:type="pagebreak" id="pg_281" role="doc-pagebreak"/>cases, there’s a need to evaluate visibility for points in the scene: for visibility, we needed to know whether a fragment was visible to the camera, to know whether to draw it in the image; and for shadowing, we need to know whether a fragment is visible to the light source, to know whether it’s illuminated by that source or not. (See <a href="C16_chapter11.xhtml#f11_22">Figure 11.22</a>.)</p>
<figure id="f11_22" tabindex="0">
<img alt="" src="../images/fig11_22.jpg"/>
<figcaption><p><span class="blue">Figure 11.22.</span> (a) The region of space illuminated by a point light. (b) That region as approximated by a 10-pixel-wide shadow map.</p></figcaption>
</figure>
<p>In both cases, the solution is the same: a depth map that tells the distance to the closest surface along a bunch of rays. In the visibility case, this is the z-buffer (<a href="C14_chapter9.xhtml#sec9_2_3">Section 9.2.3</a>), and for the shadowing case, it is called a <em>shadow map</em>. In both cases, visibility is evaluated by comparing the depth of a new fragment to the depth stored in the map, and the surface is hidden from the projection point (occluded or shadowed) if its depth is greater than the depth of the closest visible surface. A difference is that the z buffer is used to keep track of the closest surface seen <em>so far</em> and is updated during rendering, whereas a shadow map tells the distance to the closest surface in the whole scene.</p>
<p>A shadow map is calculated in a separate rendering pass ahead of time: simply rasterize the whole scene as usual, and retain the resulting depth map (there is no need to bother with calculating pixel values). Then, with the shadow map in hand, you perform an ordinary rendering pass, and when you need to know whether a fragment is visible to the source, you project its location in the shadow map (using the same perspective projection that was used to render the shadow map in the first place) and compare the looked-up value <em>d</em><sub>map</sub> with the actual distance <em>d</em> to <a id="term-595"/><span aria-label="282" epub:type="pagebreak" id="pg_282" role="doc-pagebreak"/>the source. If the distances are the same, the fragment’s point is illuminated; if the d &gt; d<sub>map</sub>, that implies there is a different surface closer to the source, so it is shadowed.</p>
<p>The phrase “if the distances are the same” should raise some red flags in your mind: since all the quantities involved are approximations with limited precision, we can’t expect them to be exactly the same. For visible points, the d ≈ d<sub>map</sub> but sometimes d will be a bit larger and sometimes a bit smaller. For this reason, a tolerance is required: a point is considered illuminated if d - d<sub>map</sub> &lt; ϵ. This tolerance ϵ is known as <em>shadow bias</em>.</p>
<p>When looking up in shadow maps it doesn’t make a lot of sense to interpolate between the depth values recorded in the map. This might lead to more accurate depths (requiring less <a id="index_term1067"/>shadow bias) in smooth areas, but will cause bigger problems near shadow boundaries, where the depth value changes suddenly. Therefore, texture lookups in shadow maps are done using nearest-neighbor reconstruction. To reduce aliasing, multiple samples can be used, with the 1-or-0 shadow results (rather than the depths) averaged; this is known as <em>percentage closer</em> <em>filtering</em>.</p>
</section>
<section>
<h3 id="sec11_4_5"><a id="index_term348"/><span class="green">11.4.5 Environment Maps</span></h3>
<p>Just as a texture is handy for introducing detail into the shading on a surface without having to add more detail to the model, a texture can also be used to introduce detail into the illumination without having to model complicated light source geometry. When light comes from far away compared to the size of objects in view, the illumination changes very little from point to point in the scene. It is handy to make the assumption that the illumination depends only on the direction you look and is the same for all points in the scene, and then to express this dependence of illumination on direction using an <em>environment</em> <em>map</em>.</p>
<p>The idea of an environment map is that a function defined over directions in 3D is a function on the unit sphere, so it can be represented using a texture map in exactly the same way as we might represent color variation on a spherical object. Instead of computing texture coordinates from the 3D coordinates of a surface point, we use exactly the same formulas to compute texture coordinates from the 3D coordinates of the unit vector that represents the direction from which we want to know the illumination.</p>
<p>The simplest application of an environment map is to give colors to rays in a ray tracer that don’t hit any objects:</p>
<pre class="pre1">trace_ray(ray, scene) {
        if (surface = scene.intersect(ray)) {<a id="term-718"/><a id="term-854"/><span aria-label="283" epub:type="pagebreak" id="pg_283" role="doc-pagebreak"/>
           return surface.shade(ray)
    } else {
           u, v = spheremap_coords(r.direction)
           return texture_lookup(scene.env_map, u, v)
    }
}</pre>
<p>With this change to the ray tracer, shiny objects that reflect other scene objects will now also reflect the background environment.</p>
<p>A similar effect can be achieved in the rasterization context by adding a mirror reflection to the shading computation, which is computed in the same way as in a ray tracer, but simply looks up directly in the environment map with no regard for other objects in the scene:</p>
<pre class="pre1">shade_fragment(view_dir, normal) {
        out_color = diffuse_shading(k_d, normal)
        out_color += specular_shading(k_s, view_dir, normal)
        u, v = spheremap_coords(reflect(view_dir, normal))
        out_color += k_m ⋆ texture_lookup(environment_map, u, v)
}</pre>
<p class="noindent">This technique is known as <em>reflection mapping</em>.</p>
<p>A more advanced used of environment maps computes all the illumination from the environment map, not just the mirror reflection. This is <em><a id="index_term347"/>environment lighting</em> and can be computed in a ray tracer using Monte Carlo integration or in rasterization by approximating the environment with a collection of point sources and computing many shadow maps.</p>
<p>Environment maps can be stored in any coordinates that could be used for mapping a sphere. Spherical (longitude–latitude) coordinates are one popular option, though the compression of textures at the poles wastes texture resolution and can create artifacts at the poles. <a id="index_term1164"/>Cubemaps are a more efficient choice, widely used in interactive applications (<a href="C16_chapter11.xhtml#f11_23">Figure 11.23</a>).</p>
<figure id="f11_23" tabindex="0">
<img alt="" src="../images/fig11_23.jpg"/>
<figcaption><p><span class="blue">Figure 11.23.</span> A cube map of St. Peter’s Basilica, with the six faces stored in on image in the unwrapped “horizontal cross” arrangement. (texture: Emil Persson)</p></figcaption>
</figure>
</section>
</section>
<section>
<h2 id="sec11_5"><a id="index_term1204"/><a id="index_term1188"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec11_5" role="doc-backlink"><span class="green">11.5 Procedural 3D Textures</span></a></h2>
<p>In previous chapters, we used <em>c<sub>r</sub></em> as the diffuse reflectance at a point on an object. For an object that does not have a solid color, we can replace this with a function <em>c<sub>r</sub></em>(<strong>p</strong>) which maps 3D points to RGB colors (Peachey, 1985; Perlin, 1985). This function might just return the reflectance of the object that contains <strong>p</strong>. But for objects with <em>texture</em>, we should expect <em>c<sub>r</sub></em>(<strong>p</strong>) to vary as <strong>p</strong> moves across a surface.</p>
<p>An alternative to defining texture <a id="index_term970"/>mapping functions that map from a 3D surface to a 2D texture domain is to create a 3D texture that defines an RGB value at <a id="term-228"/><a id="term-855"/><span aria-label="284" epub:type="pagebreak" id="pg_284" role="doc-pagebreak"/>every point in 3D space. We will only call it for points <strong>p</strong> on the surface, but it is usually easier to define it for all 3D points than a potentially strange 2D subset of points that are on an arbitrary surface. The good thing about 3D texture mapping is that it is easy to define the mapping function, because the surface is already embedded in 3D space, and there is no distortion in the mapping from 3D to texture space. Such a strategy is clearly suitable for surfaces that are “carved” from a solid medium, such as a marble sculpture.</p>
<p>The downside to 3D textures is that storing them as 3D raster images or <em>volumes</em> consumes a great deal of memory. For this reason, 3D texture coordinates are most commonly used with <em><a id="index_term904"/>procedural textures</em> in which the texture values are computed using a mathematical procedure rather than by looking them up from a texture image. In this section, we look at a couple of the fundamental tools used to define procedural textures. These could also be used to define 2D procedural textures, though in 2D it is more common to use raster texture images.</p>
<section>
<h3 id="sec11_5_1"><a id="index_term1203"/><a id="index_term1189"/><span class="green">11.5.1 3D Stripe Textures</span></h3>
<p>There are a surprising number of ways to make a striped texture. Let’s <a id="index_term998"/>assume we have two colors c<sub>0</sub> and c<sub>1</sub> that we want to use to make the stripe color. We need some oscillating function to switch between the two colors. An easy one is a sine:</p>
<p class="indent1"><span aria-label="285" epub:type="pagebreak" id="pg_285" role="doc-pagebreak"/>RGB stripe( point <strong>p</strong>)</p>
<p class="indent1"><strong>if</strong> (sin(<em>x<sub>p</sub></em>) &gt; 0) <strong>then</strong></p>
<p class="indent1">   <strong>return </strong><em>c</em><sub>0</sub></p>
<p class="indent1"><strong>else</strong></p>
<p class="indent1">   <strong>return </strong><em>c</em><sub>1</sub></p>
<p class="noindent1">We can also make the stripe’s width <em>w</em> controllable:</p>
<p class="indent1">RGB stripe( point <strong>p</strong>, real <em>w</em>)</p>
<p class="indent1"><strong>if</strong> (sin(<em>πx<sub>p</sub>∕w</em>) &gt; 0) <strong>then</strong></p>
<p class="indent1">   <strong>return </strong><em>c</em><sub>0</sub></p>
<p class="indent1"><strong>else</strong></p>
<p class="indent1">   <strong>return </strong><em>c</em><sub>1</sub></p>
<p class="noindent1">If we want to interpolate smoothly between the stripe colors, we can use a parameter <em>t</em> to vary the color linearly:</p>
<p class="indent1">RGB stripe( point <strong>p</strong>, real <em>w</em></p>
<p class="indent1"><em>t</em> = (1 + sin(<em>πp<sub>x</sub></em>/<em>w</em>))/2</p>
<p class="indent1"><strong>return </strong>(1 - <em>t</em>)<em>c</em><sub>0</sub> + <em>tc</em><sub>1</sub></p>
<p class="noindent1">These three possibilities are shown in <a href="C16_chapter11.xhtml#f11_24">Figure 11.24</a>.</p>
<figure id="f11_24" tabindex="0">
<img alt="" src="../images/fig11_24.jpg"/>
<figcaption><p><span class="blue">Figure 11.24.</span> Various stripe textures result from drawing a regular array of <em>xy</em> points while keeping <em>z</em> constant.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec11_5_2"><a id="index_term1100"/><span class="green">11.5.2 Solid Noise</span></h3>
<p>Although regular textures such as stripes are often useful, we would like to be able to make “mottled” textures such as we see on birds’ eggs. This is usually done by using a sort of “solid noise,” usually called <em><a id="index_term836"/>Perlin noise</em> after its inventor, who received a technical Academy Award for its impact in the film industry (Perlin, 1985).</p>
<p>Getting a noisy appearance by calling a random number for every point would not be appropriate, because it would just be like “white noise” in TV static. We would like to make it smoother without losing the random quality. One possibility is to blur white noise, but there is no practical implementation of this. Another possibility is to make a large <a id="index_term661"/>lattice with a random number at every lattice point and then interpolate these random points for new points between lattice nodes; this is just a 3D texture array as described in the last section with random numbers in the array. This technique makes the lattice too obvious. <a id="index_term834"/>Perlin used a variety of tricks to improve this basic lattice technique so the lattice was not so obvious. This results in a rather baroque-looking set of steps, but essentially there are just three changes from linearly interpolating a 3D array of random values. The first change is to use Hermite interpolation to avoid mach bands, just as can be done <span aria-label="286" epub:type="pagebreak" id="pg_286" role="doc-pagebreak"/>with regular textures. The second change is the use of random vectors rather than values, with a dot product to derive a random number; this makes the underlying grid structure less visually obvious by moving the local minima and maxima off the grid vertices. The third change is to use a 1D array and hashing to create a virtual 3D array of random vectors. This adds computation to lower memory use. Here is his basic method:</p>
<div class="disp-formula" id="uequ11_14">
<m:math xmlns:mml="http://www.w3.org/1998/Math/MathML" alttext=""><m:mrow><m:mi>n</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mstyle displaystyle="true"><m:mrow><m:munderover><m:mrow><m:mo>∑</m:mo></m:mrow><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mo>⌊</m:mo><m:mi>x</m:mi><m:mo>⌋</m:mo></m:mrow><m:mrow><m:mo>⌊</m:mo><m:mi>x</m:mi><m:mo>⌋</m:mo><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:munderover><m:mrow><m:mstyle displaystyle="true"><m:mrow><m:munderover><m:mrow><m:mo>∑</m:mo></m:mrow><m:mrow><m:mi>j</m:mi><m:mo>=</m:mo><m:mo>⌊</m:mo><m:mi>y</m:mi><m:mo>⌋</m:mo></m:mrow><m:mrow><m:mo>⌊</m:mo><m:mi>y</m:mi><m:mo>⌋</m:mo><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:munderover><m:mrow><m:mstyle displaystyle="true"><m:mrow><m:munderover><m:mrow><m:mo>∑</m:mo></m:mrow><m:mrow><m:mi>k</m:mi><m:mo>=</m:mo><m:mo>⌊</m:mo><m:mi>z</m:mi><m:mo>⌋</m:mo></m:mrow><m:mrow><m:mo>⌊</m:mo><m:mi>z</m:mi><m:mo>⌋</m:mo><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:munderover><m:mrow><m:msub><m:mrow><m:mi>Ω</m:mi></m:mrow><m:mrow><m:mi>i</m:mi><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:mrow></m:mrow></m:mstyle></m:mrow></m:mrow></m:mstyle></m:mrow></m:mrow></m:mstyle><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>−</m:mo><m:mi>i</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>−</m:mo><m:mi>j</m:mi><m:mo>,</m:mo><m:mi>z</m:mi><m:mo>−</m:mo><m:mi>k</m:mi><m:mo stretchy="false">)</m:mo><m:mo>,</m:mo></m:mrow></m:math>
</div>
<p>where (<em>x,y,z</em>) are the Cartesian coordinates of <strong>x</strong>, and</p>
<div class="disp-formula" id="uequ11_15">
<m:math alttext=""><m:mrow><m:mrow><m:msub><m:mrow><m:mi>Ω</m:mi></m:mrow><m:mrow><m:mi>i</m:mi><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub><m:mo stretchy="false">(</m:mo><m:mi>u</m:mi><m:mo>,</m:mo><m:mi>v</m:mi><m:mo>,</m:mo><m:mi>w</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mi>ω</m:mi><m:mo stretchy="false">(</m:mo><m:mi>u</m:mi><m:mo stretchy="false">)</m:mo><m:mi>ω</m:mi><m:mo stretchy="false">(</m:mo><m:mi>v</m:mi><m:mo stretchy="false">)</m:mo><m:mi>ω</m:mi><m:mo stretchy="false">(</m:mo><m:mi>w</m:mi><m:mo stretchy="false">)</m:mo><m:mrow><m:mo>(</m:mo><m:mrow><m:msub><m:mrow><m:mi>Γ</m:mi></m:mrow><m:mrow><m:mi>i</m:mi><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub><m:mo>⋅</m:mo><m:mo stretchy="false">(</m:mo><m:mi>u</m:mi><m:mo>,</m:mo><m:mi>v</m:mi><m:mo>,</m:mo><m:mi>w</m:mi><m:mo stretchy="false">)</m:mo></m:mrow><m:mo>)</m:mo></m:mrow><m:mo>,</m:mo></m:mrow></m:mrow></m:math>
</div>
<p>and <em>ω(t)</em> is the cubic weighting function:</p>
<div class="disp-formula" id="uequ11_16">
<m:math alttext=""><m:mrow><m:mrow><m:mi>ω</m:mi><m:mo stretchy="false">(</m:mo><m:mi>t</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mrow><m:mo>{</m:mo><m:mtable columnalign="left"><m:mtr columnalign="left"><m:mtd columnalign="left"><m:mn>2</m:mn><m:mo>|</m:mo><m:mi>t</m:mi><m:msup><m:mrow><m:mo>|</m:mo></m:mrow><m:mrow><m:mn>3</m:mn></m:mrow></m:msup><m:mo>−</m:mo><m:mn>3</m:mn><m:mo>|</m:mo><m:mi>t</m:mi><m:msup><m:mrow><m:mo>|</m:mo></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msup><m:mo>+</m:mo><m:mn>1</m:mn></m:mtd><m:mtd columnalign="left"><m:mtext>if|t|</m:mtext><m:mo>&lt;</m:mo><m:mtext>1</m:mtext><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr columnalign="left"><m:mtd columnalign="left"><m:mn>0</m:mn></m:mtd><m:mtd columnalign="left"><m:mtext>otherwise</m:mtext><m:mo>.</m:mo></m:mtd></m:mtr></m:mtable></m:mrow></m:mrow></m:mrow></m:math>
</div>
<p>The final piece is that Γ<sub><em>ijk</em></sub> is a random unit vector for the lattice point (<em>x,y,z</em>) = (<em>i,j,k</em>). Since we want any potential <em>ijk</em>, we use a pseudorandom table:</p>
<div class="disp-formula" id="uequ11_17">
<m:math xmlns:mml="http://www.w3.org/1998/Math/MathML" alttext=""><m:mrow><m:msub><m:mrow><m:mi>Γ</m:mi></m:mrow><m:mrow><m:mi>i</m:mi><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:mtext> </m:mtext><m:mi mathvariant="normal">G</m:mi><m:mrow><m:mo>(</m:mo><m:mrow><m:mi>ϕ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>i</m:mi><m:mo>+</m:mo><m:mi>ϕ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>j</m:mi><m:mo>+</m:mo><m:mi>ϕ</m:mi><m:mo stretchy="false">(</m:mo><m:mi>k</m:mi><m:mo stretchy="false">)</m:mo><m:mo stretchy="false">)</m:mo><m:mo stretchy="false">)</m:mo></m:mrow><m:mo>)</m:mo></m:mrow><m:mo>,</m:mo></m:mrow></m:math>
</div>
<p>where <strong>G</strong> is a precomputed array of <em>n</em> random unit vectors, and <em>ϕ</em>(<em>i</em>) = <em>P</em>[<em>i</em> mod <em>n</em>] where <em>P</em> is an array of length <em>n</em> containing a permutation of the integers 0 through <em>n</em> - 1. In practice, Perlin reports <em>n</em> = 256 works well. To choose a random unit vector (<em>v<sub>x</sub>,v<sub>y</sub>,v<sub>z</sub></em>), first set</p>
<div class="disp-formula" id="uequ11_18">
<m:math xmlns:mml="http://www.w3.org/1998/Math/MathML" alttext=""><m:mrow><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi>υ</m:mi></m:mrow><m:mrow><m:mi>x</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:mn>2</m:mn><m:mi>ξ</m:mi><m:mo>−</m:mo><m:mn>1</m:mn><m:mo>,</m:mo><m:mspace linebreak="newline"/><m:mspace linebreak="newline"/></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>υ</m:mi></m:mrow><m:mrow><m:mi>y</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:mn>2</m:mn><m:msup><m:mrow><m:mi>ξ</m:mi></m:mrow><m:mrow><m:mo>′</m:mo></m:mrow></m:msup><m:mo>−</m:mo><m:mn>1</m:mn><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>υ</m:mi></m:mrow><m:mrow><m:mi>z</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:mn>2</m:mn><m:msup><m:mrow><m:mi>ξ</m:mi></m:mrow><m:mrow><m:mo>″</m:mo></m:mrow></m:msup><m:mo>−</m:mo><m:mn>1</m:mn><m:mo>,</m:mo></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>where ξ,ξ′,ξ<sup>″</sup> are canonical random numbers (uniform in the interval [0,1)). Then, if <span class="inline-formula"><m:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><m:mrow><m:mo stretchy="false">(</m:mo><m:msubsup><m:mrow><m:mi>υ</m:mi></m:mrow><m:mrow><m:mi>x</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msubsup><m:mo>+</m:mo><m:msubsup><m:mrow><m:mi>υ</m:mi></m:mrow><m:mrow><m:mi>y</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msubsup><m:mo>+</m:mo><m:msubsup><m:mrow><m:mi>υ</m:mi></m:mrow><m:mrow><m:mi>z</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:msubsup><m:mo stretchy="false">)</m:mo><m:mo>&lt;</m:mo><m:mn>1</m:mn></m:mrow></m:math></span>, make the vector a unit vector. Otherwise, keep setting it randomly until its length is less than one, and then make it a unit vector. This is an example of a <em>rejection method</em>, which will be discussed more in <a href="C18_chapter13.xhtml#c13">Chapter 13</a>. Essentially, the “less than” test gets a random point in the unit sphere, and the vector for the origin to that point is uniformly random. That would not be true of random points in the cube, so we “get rid” of the corners with the test.</p>
<p><a id="term-552"/><a id="term-806"/><span aria-label="287" epub:type="pagebreak" id="pg_287" role="doc-pagebreak"/>Because solid noise can be positive or negative, it must be transformed before being converted to a color. The absolute value of noise over a 10 × 10 square is shown in <a href="C16_chapter11.xhtml#f11_25">Figure 11.25</a>, along with stretched versions. These versions are stretched by scaling the points input to the <a id="index_term778"/>noise function.</p>
<figure id="f11_25" tabindex="0">
<img alt="" src="../images/fig11_25.jpg"/>
<figcaption><p><span class="blue">Figure 11.25.</span> Absolute value of solid noise, and noise for scaled <em>x</em> and <em>y</em> values.</p></figcaption>
</figure>
<p>The dark curves are where the original noise function changed from positive to negative. Since noise varies from - 1 to 1, a smoother image can be achieved by using (noise + 1)∕2 for color. However, since noise values close to 1 or - 1 are rare, this will be a fairly smooth image. Larger scaling can increase the contrast (<a href="C16_chapter11.xhtml#f11_26">Figure 11.26</a>).</p>
<figure id="f11_26" tabindex="0">
<img alt="" src="../images/fig11_26.jpg"/>
<figcaption><p><span class="blue">Figure 11.26.</span> Using 0.5(noise+1) (a) and 0.8(noise+1) (b) for intensity.</p></figcaption>
</figure>
</section>
<section>
<h3 id="sec11_5_3"><span class="green">11.5.3 Turbulence</span></h3>
<p>Many natural textures contain a variety of feature sizes in the same texture. <a id="index_term835"/>Perlin uses a pseudofractal “turbulence” function:</p>
<div class="disp-formula" id="uequ11_19">
<m:math alttext=""><m:mrow><m:mrow><m:msub><m:mrow><m:mi>n</m:mi></m:mrow><m:mrow><m:mi>t</m:mi></m:mrow></m:msub><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mstyle displaystyle="true"><m:mrow><m:munder><m:mrow><m:mo>∑</m:mo></m:mrow><m:mrow><m:mi>i</m:mi></m:mrow></m:munder><m:mrow><m:mfrac><m:mrow><m:mo>|</m:mo><m:mi>n</m:mi><m:mo stretchy="false">(</m:mo><m:msup><m:mrow><m:mn>2</m:mn></m:mrow><m:mrow><m:mi>i</m:mi></m:mrow></m:msup><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo>|</m:mo></m:mrow><m:mrow><m:msup><m:mrow><m:mn>2</m:mn></m:mrow><m:mrow><m:mi>i</m:mi></m:mrow></m:msup></m:mrow></m:mfrac></m:mrow></m:mrow></m:mstyle></m:mrow></m:mrow></m:math>
</div>
<p>This effectively repeatedly adds scaled copies of the noise function on top of itself as shown in <a href="C16_chapter11.xhtml#f11_27">Figure 11.27</a>.</p>
<figure id="f11_27" tabindex="0">
<img alt="" src="../images/fig11_27.jpg"/>
<figcaption><p><span class="blue">Figure 11.27.</span> Turbulence function with (from top left to bottom right) one through eight terms in the summation.</p></figcaption>
</figure>
<p>The turbulence can be used to distort the stripe function:</p>
<ul class="none">
<li><p class="list">RGB turbstripe( point <strong>p</strong>, double w)</p></li>
<li><p class="list">double <em>t</em> = (1 + sin(<em>k</em><sub>1</sub><em>z<sub>p</sub></em> + turbulence(<em>k</em><sub>2</sub><strong>p</strong>))∕<em>w</em>)∕2</p></li>
<li><p class="list"><strong>return</strong> t * <em>s</em>0 + (1 − <em>t</em>) * <em>s</em>1</p></li>
</ul>
<p>Various values for <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub> were used to generate <a href="C16_chapter11.xhtml#f11_28">Figure 11.28</a>.</p>
<figure id="f11_28" tabindex="0">
<img alt="" src="../images/fig11_28.jpg"/>
<figcaption><p><span class="blue">Figure 11.28.</span> Various turbulent stripe textures with different<em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>. The top row has only the first term of the <a id="index_term1287"/>turbulence series.</p></figcaption>
</figure>
</section>
</section>
<section>
<h2 id="sec11_6"><span class="green">Frequently Asked Questions</span></h2>
<ul class="list-bullet">
<li>
<p class="list"><span class="green">How do I implement displacement mapping in ray tracing?</span></p></li>
</ul>
<p class="noindent1b">There is no ideal way to do it. Generating all the triangles and caching the geometry when necessary will prevent memory overload (Pharr &amp; Hanrahan, 1996; Pharr, Kolb, Gershbein, &amp; Hanrahan, 1997). Trying to intersect the displaced surface directly is possible when the displacement function is restricted (Patterson, Hoggar, &amp; Logie, 1991; Heidrich &amp; Seidel, 1998; Smits, Shirley, &amp; Stark, 2000).</p>
<ul class="list-bullet">
<li>
<p class="list"><a id="term-980"/><span aria-label="288" epub:type="pagebreak" id="pg_288" role="doc-pagebreak"/><span class="green">Why don’t my images with textures look realistic?</span></p>
</li>
</ul>
<p class="noindent1b">Humans are good at seeing small imperfections in surfaces. Geometric imperfections are typically absent in computer-generated images that use texture maps for details, so they look “too smooth.”</p>
</section>
<section>
<h2 id="sec11_7"><span aria-label="289" epub:type="pagebreak" id="pg_289" role="doc-pagebreak"/><span class="green">Notes</span></h2>
<p>The discussion of perspective-correct textures is based on <em>Fast Shadows and</em> <em>Lighting Effects Using Texture Mapping</em> (Segal, Korobkin, van Widenfelt, Foran, &amp; Haeberli, 1992) and on <em>3D Game Engine Design</em> (Eberly, 2000).</p>
</section>
<section>
<h2 id="sec11_8"><span class="green">Exercises</span></h2>
<p class="qpara"><span class="green">1.</span> Find several ways to implement an infinite 2D checkerboard using surface and solid techniques. Which is best?</p>
<p class="qpara"><span class="green">2.</span> Verify that Equation (9.4) is a valid equality using brute-force algebra.</p>
<p class="qpara"><span class="green">3.</span> How could you implement solid texturing by using the z-buffer depth and a matrix transform?</p>
<p class="qpara"><span class="green">4.</span> Expand the function mipmap_sample_trilinear into a single function.</p>
</section>
</section>
</body>
</html>