<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:svg="http://www.w3.org/2000/svg" dir="ltr" lang="en" xml:lang="en">
<head>
<meta charset="UTF-8"/>
<title>20 Tone Reproduction</title>
<link href="../styles/9781000426359.css" rel="stylesheet" type="text/css"/>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX","input/MathML","output/SVG"],
extensions: ["tex2jax.js","mml2jax.js","MathEvents.js"],
TeX: {
extensions: ["noErrors.js","noUndefined.js","autoload-all.js"]
},
MathMenu: {
showRenderer: false
},
menuSettings: {
zoom: "Click"
},
messageStyle: "none"
});
</script>
<script src="../mathjax/MathJax.js" type="text/javascript"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006665500" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter">
<section epub:type="chapter" role="doc-chapter">
<header>
<p class="chap-auz"><span class="green">Erik Reinhard</span></p>
<h1 class="chapz1" id="c20"><span aria-label="569" epub:type="pagebreak" id="pg_569" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rc20" role="doc-backlink"><span class="green"><span class="big1">20</span><br/>Tone Reproduction</span></a></h1>
</header>
<p class="noindent1">As discussed in <a href="C24_chapter19.xhtml#c19">Chapter 19</a>, the human visual system adapts to a wide range of viewing conditions. Under normal viewing, we may discern a range of around 4 to 5 log units of illumination, i.e., the ratio between brightest and darkest areas where we can see detail may be as large as 100<em>,</em>000 : 1. Through adaptation processes, we may adapt to an even larger range of illumination. We call images that are matched to the capabilities of the human visual system <em>high dynamic range</em>.</p>
<p>Visual simulations routinely produce images with a high dynamic range (Ward Larson &amp; Shakespeare, 1998). Recent developments in image-capturing techniques allow multiple exposures to be aligned and recombined into a single high dynamic range image (Debevec &amp; Malik, 1997). Multiple exposure techniques are also available for video. In addition, we expect future hardware to be able to photograph or film high dynamic range scenes directly. In general, we may think of each pixel as a triplet of three floating point numbers.</p>
<p>As it is becoming easier to create high dynamic range imagery, the need to display such data is rapidly increasing. Unfortunately, most current display devices, monitors and printers, are only capable of displaying around 2 log units of dynamic range. We consider such devices to be of low dynamic range. Most images in existence today are represented with a byte-per-pixel-per-color channel, which is matched to current display devices, rather than to the scenes they represent.</p>
<p>Typically, low dynamic range images are not able to represent scenes without loss of information. A common example is an indoor room with an out <a id="term-896"/><span aria-label="570" epub:type="pagebreak" id="pg_570" role="doc-pagebreak"/>door area visible through the window. Humans are easily able to see details of both the indoor part and the outside part. A conventional photograph typically does not capture this full range of information—the photographer has to choose whether the indoor or the outdoor part of the scene is properly exposed (see <a href="C25_chapter20.xhtml#f20_1">Figure 20.1</a>). These decisions may be avoided by using high dynamic range imaging and preparing these images for display using techniques described in this chapter (see <a href="C25_chapter20.xhtml#f20_2">Figure 20.2</a>).</p>
<figure id="f20_1" tabindex="0">
<img alt="" src="../images/fig20_1.jpg"/>
<figcaption><p><span class="blue">Figure 20.1.</span> With conventional photography, some parts of the scene may be under- or over-exposed. To visualize the snooker table, the view through the window is burned out in the left image. On the other hand, the snooker table will be too dark if the outdoor part of this scene is properly exposed. Compare with <a href="C25_chapter20.xhtml#f20_2">Figure 20.2</a>, which shows a high dynamic range image prepared for display using a tone reproduction algorithm.</p></figcaption>
</figure>
<figure id="f20_2" tabindex="0">
<img alt="" src="../images/fig20_2.jpg"/>
<figcaption><p><span class="blue">Figure 20.2.</span> A high dynamic range image tonemapped for display using a recent <a id="index_term1210"/>tone reproduction operator (Reinhard &amp; Devlin, 2005). In this image, both the indoor part and the view through the window are properly exposed.</p></figcaption>
</figure>
<p>There are two strategies available to display high dynamic range images. First, we may develop display devices which can directly accommodate a high dynamic range (Seetzen, Whitehead, &amp; Ward, 2003; Seetzen et al., 2004). Second, we may prepare high dynamic range images for display on low dynamic range display devices (Upstill, 1985). This is currently the more common approach and the topic of this chapter. Although we foresee that high dynamic range display devices will become widely used in the (near) future, the need to compress the dynamic range of an image may diminish, but will not disappear. In particular, printed media such as this book are, by their very nature, low dynamic range.</p>
<p>Compressing the range of values of an image for the purpose of display on a low dynamic range display device is called tonemapping or tone reproduction. A simple compression function would be to normalize an image (see <a href="C25_chapter20.xhtml#f20_3">Figure 20.3</a> (left)). This constitutes a linear scaling which tends to be sufficient only if the dynamic range of the image is only marginally higher than the dynamic range of the display device. For images with a higher dynamic range, small intensity differences will be quantized to the same display value such that visible details are lost. In <a href="C25_chapter20.xhtml#f20_3">Figure 20.3</a> (middle) all pixel values larger than a user-specified maximum are set to this maximum (i.e., they are clamped). This makes the normalization less dependent on noisy outliers, but here we lose information in the bright areas of the image. For comparison, <a href="C25_chapter20.xhtml#f20_3">Figure 20.3</a> (right) is a tonemapped version showing detail in both the dark and the bright regions.</p>
<figure id="f20_3" tabindex="0">
<img alt="" src="../images/fig20_3.jpg"/>
<figcaption><p><span class="blue">Figure 20.3.</span> <a id="term-897"/><span aria-label="571" epub:type="pagebreak" id="pg_571" role="doc-pagebreak"/>Linear scaling of high dynamic range images to fit a given display device may cause significant detail to be lost (left and middle). The left image is linearly scaled. In the middle image high values are clamped. For comparison, the right image is tonemapped, allowing details in both bright and dark regions to be visible.</p></figcaption>
</figure>
<p>In general, linear scaling will not be appropriate for tone reproduction. The key issue in tone reproduction is then to compress an image while at the same time preserving one or more attributes of the image. Different tone reproduction algorithms focus on different attributes such as contrast, visible detail, brightness, or appearance.</p>
<p>Ideally, displaying a tonemapped image on a low dynamic range display device would create the same visual response in the observer as the original scene. Given the limitations of display devices, this will not be achievable, although we could aim for approximating this goal as closely as possible.</p>
<p>As an example, we created the high dynamic range image shown in <a href="C25_chapter20.xhtml#f20_4">Figure 20.4</a>. This image was then tonemapped and displayed on a display device. The display device itself was then placed in the scene such that it displays its own background (<a href="C25_chapter20.xhtml#f20_5">Figure 20.5</a>). In the ideal case, the display should appear transparent. Dependent on the quality of the tone reproduction operator, as well as the nature of the scene being depicted, this goal may be more or less achievable.</p>
<figure id="f20_4" tabindex="0">
<img alt="" src="../images/fig20_4.jpg"/>
<figcaption><p><span class="blue">Figure 20.4.</span> Image used for demonstrating the goal of tone reproduction in <a href="C25_chapter20.xhtml#f20_5">Figure 20.5</a>.</p></figcaption>
</figure>
<figure id="f20_5" tabindex="0">
<img alt="" src="../images/fig20_5.jpg"/>
<figcaption><p><span class="blue">Figure 20.5.</span> <a id="term-893"/><a id="term-898"/><span aria-label="572" epub:type="pagebreak" id="pg_572" role="doc-pagebreak"/>After tonemapping the image in <a href="C25_chapter20.xhtml#f20_4">Figure 20.4</a> and displaying it on a monitor, the monitor is placed in the scene approximately at the location where the image was taken. Dependent on the quality of the tone reproduction operator, the result should appear as if the monitor is transparent.</p></figcaption>
</figure>
<section>
<h2 id="sec20_1"><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_1" role="doc-backlink"><span class="green">20.1 Classification</span></a></h2>
<p>Although it would be possible to classify tone reproduction operators by which attribute they aim to preserve, or for which task they were developed, we classify algorithms according to their general technique. This will enable us to show the differences and similarities between a significant number of different operators, and so, hopefully, contribute to the meaningful selection of specific operators for given tone reproduction tasks.</p>
<p>The main classification scheme we follow hinges upon the realization that tone reproduction operators are based on insights gained from various disciplines. In particular, several operators are based on knowledge of human visual perception.</p>
<p>The human visual system detects light using photoreceptors located in the retina. Light is converted to an electrical signal which is partially processed in the retina and then transmitted to the brain. Except for the first few layers of cells in the retina, the signal derived from detected light is transmitted using impulse trains. The information-carrying quantity is the frequency with which these electrical pulses occur.</p>
<p>The range of light that the human visual system can detect is much larger than the range of frequencies employed by the human brain to transmit information. Thus, the human visual system effortlessly solves the tone reproduction problem—a large range of luminances is transformed into a small range of frequencies of <a id="index_term615"/>impulse trains. Emulating relevant aspects of the human visual system is therefore a worthwhile approach to tone reproduction; this approach is explained in more detail in <a href="C25_chapter20.xhtml#sec20_7">Section 20.7</a>.</p>
<p><span aria-label="573" epub:type="pagebreak" id="pg_573" role="doc-pagebreak"/>A second class of operators is grounded in physics. Light interacts with surfaces and volumes before being absorbed by the photoreceptors. In computer graphics, light interaction is generally modeled by the rendering equation. For purely diffuse surfaces, this equation may be simplified to the product between light incident upon a surface (illuminance), and this surface’s ability to reflect light (reflectance) (Oppenheim, Schafer, &amp; Stockham, 1968).</p>
<p>Since reflectance is a passive property of surfaces, for diffuse surfaces it is, by definition, low <a id="index_term1215"/>dynamic range—typically between 0<em>.</em>005 and 1 (Stockham, 1972). The reflectance of a surface cannot be larger than 1, since then it would reflect more light than was incident upon the surface. Illuminance, on the other hand, can produce arbitrarily large values and is limited only by the intensity and proximity of the light sources.</p>
<p>The dynamic range of an image is thus predominantly governed by the illuminance component. In the face of diffuse scenes, a viable approach to tone reproduction may therefore be to separate reflectance from illuminance, compress the illuminance component, and then recombine the image.</p>
<p>However, the assumption that all surfaces in a scene are diffuse is generally incorrect. Many high dynamic range images depict highlights and/or directly visible light sources (<a href="C25_chapter20.xhtml#f20_3">Figure 20.3</a>). The luminance reflected by a specular surface may be almost as high as the light source it reflects.</p>
<p>Various tone reproduction operators currently used split the image into a high dynamic range base layer and a low dynamic range detail layer. These layers would represent illuminance and reflectance if the depicted scene were entirely diffuse. For scenes containing directly visible light sources or specular highlights, separation into base and detail layers still allows the design of effective tone reproduction operators, although no direct meaning can be attached to the separate layers. Such operators are discussed in <a href="C25_chapter20.xhtml#sec20_5">Section 20.5</a>.</p>
</section>
<section>
<h2 id="sec20_2"><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_2" role="doc-backlink"><span class="green">20.2 Dynamic Range</span></a></h2>
<p>Conventional images are stored with one byte per pixel for each of the red, green and blue components. The dynamic range afforded by such an encoding depends on the ratio between smallest and largest representable value, as well as the step size between successive values. Thus, for low dynamic range images, there are only 256 different values per color channel.</p>
<p>High dynamic range images encode a significantly larger set of possible values; the maximum representable value may be much larger and the step size between successive values may be much smaller. The file size of high dynamic <a id="term-899"/><span aria-label="574" epub:type="pagebreak" id="pg_574" role="doc-pagebreak"/>range images is therefore generally larger as well, although at least one standard (the OpenEXR high dynamic range file format (Kainz, Bogart, &amp; Hess, 2003)) includes a very capable compression scheme.</p>
<figure id="f20_6" tabindex="0">
<img alt="" src="../images/fig20_6.jpg"/>
<figcaption><p><span class="blue">Figure 20.6.</span> Dynamic range of 2.65 log<sub>2</sub> units.</p></figcaption>
</figure>
<p>A different approach to limit file sizes is to apply a tone reproduction operator to the high dynamic data. The result may then be encoded in JPEG format. In addition, the input image may be divided pixel-wise by the tonemapped image.</p>
<figure id="f20_7" tabindex="0">
<img alt="" src="../images/fig20_7.jpg"/>
<figcaption><p><span class="blue">Figure 20.7.</span> Dynamic range of 3.96 log<sub>2</sub> units.</p></figcaption>
</figure>
<p>The result of this division can then be subsampled and stored as a small amount of data in the header of the same JPEG image (G. Ward &amp; Simmons, 2004). The file size of such sub-band encoded images is of the same order as conventional JPEG encoded images. Display programs can display the JPEG image directly or may reconstruct the high dynamic range image by multiplying the tonemapped image with the data stored in the header.</p>
<figure id="f20_8" tabindex="0">
<img alt="" src="../images/fig20_8.jpg"/>
<figcaption><p><span class="blue">Figure 20.8.</span> Dynamic range of 4.22 log<sub>2</sub> units.</p></figcaption>
</figure>
<p>In general, the combination of smallest step size and ratio of the smallest and largest representable values determines the dynamic range that an image encoding scheme affords. For computer-generated imagery, an image is typically stored as a triplet of floating point values before it is written to file or displayed on screen, although more efficient encoding schemes are possible (Reinhard, Ward, Debevec, &amp; Pattanaik, 2005). Since most display devices are still fitted with eight-bit D/A converters, we may think of tone reproduction as the mapping of floating point numbers to bytes such that the result is displayable on a low dynamic range display device.</p>
<figure id="f20_9" tabindex="0">
<img alt="" src="../images/fig20_9.jpg"/>
<figcaption><p><span class="blue">Figure 20.9.</span> Dynamic range of 5.01 log<sub>2</sub> units.</p></figcaption>
</figure>
<p>The dynamic range of individual images is generally smaller, and is determined by the smallest and largest luminances found in the scene. A simplistic approach to measure the dynamic range of an image may therefore compute the ratio between the largest and smallest pixel value of an image. Sensitivity to outliers may be reduced by ignoring a small percentage of the darkest and brightest pixels.</p>
<figure id="f20_10" tabindex="0">
<img alt="" src="../images/fig20_10.jpg"/>
<figcaption><p><span class="blue">Figure 20.10.</span> Dynamic range of 6.56 log<sub>2</sub> units.</p></figcaption>
</figure>
<p>Alternatively, the same ratio may be expressed as a difference in the logarithmic domain. This measure is less sensitive to outliers. The images shown in the margin on this page are examples of images with different dynamic ranges. Note that the night scene in this case does not have a smaller dynamic range than the day scene. While all the values in the night scene are smaller, the ratio between largest and smallest values is not.</p>
<p>However, the recording device or rendering algorithm may introduce noise which will lower the useful dynamic range. Thus, a measurement of the dynamic range of an image should factor in noise. A better measure of dynamic range would therefore be a signal-to-noise ratio, expressed in decibels, as used in signal processing.</p>
</section>
<section>
<h2 id="sec20_3"><a id="index_term1212"/><span aria-label="575" epub:type="pagebreak" id="pg_575" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_3" role="doc-backlink"><span class="green">20.3 Color</span></a></h2>
<p><a id="index_term179"/>Tone reproduction operators normally compress luminance values, rather than work directly on the red, green, and blue components of a color image. After these luminance values have been compressed into display values <em>L<sub>d</sub></em>(<em>x, y</em>), a color image may be reconstructed by keeping the ratios between color channels the same as they were before compression (using <em>s</em> = 1) (Schlick, 1994b):</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">r</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="normal">=</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="normal">(</m:mi><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">r</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:msup><m:mrow><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">s</m:mi></m:mrow></m:msup><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>,</m:mo></m:mrow></m:math>
</div>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">g</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="normal">=</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="normal">(</m:mi><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">g</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:msup><m:mrow><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">s</m:mi></m:mrow></m:msup><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>,</m:mo></m:mrow></m:math>
</div>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">b</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="normal">=</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="normal">(</m:mi><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">b</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:msup><m:mrow><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">s</m:mi></m:mrow></m:msup><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>The results frequently appear over-saturated, because human color <a id="index_term167"/>perception is nonlinear with respect to overall luminance level. This means that if we view an image of a bright outdoor scene on a monitor in a dim environment, our eyes are adapted to the dim environment rather than the outdoor lighting. By keeping color ratios constant, we do not take this effect into account.</p>
<p>Alternatively, the saturation constant <em>s</em> may be chosen smaller than one. Such per-channel gamma correction may desaturate the results to an appropriate level, as shown in <a href="C25_chapter20.xhtml#f20_11">Figure 20.11</a> (Fattal, Lischinski, &amp; Werman, 2002). A more comprehensive solution is to incorporate ideas from the field of color appearance modeling into tone reproduction operators (Pattanaik, Ferwerda, Fairchild, &amp; Greenberg, 1998; Fairchild &amp; Johnson, 2004; Reinhard &amp; Devlin, 2005).</p>
<figure id="f20_11" tabindex="0">
<img alt="" src="../images/fig20_11.jpg"/>
<figcaption><p><span class="blue">Figure 20.11.</span> Per-channel gamma correction may desaturate the image. The left image was desaturated with a value of <em>s</em> = 0.5. The right image was not desaturated ( <em>s</em> =1).</p></figcaption>
</figure>
<p><a id="term-127"/><a id="term-894"/><span aria-label="576" epub:type="pagebreak" id="pg_576" role="doc-pagebreak"/>Finally, if an example image with a representative color scheme is already available, this color scheme may be applied to a new image. Such a mapping of colors between images may be used for subtle color correction, such as saturation adjustment or for more creative color mappings. The mapping proceeds by converting both source and target images to a decorrelated color space. In such a color space, the pixel values in each color channel may be treated independently without introducing too many artifacts (Reinhard, Ashikhmin, Gooch, &amp; Shirley, 2001).</p>
<p>Mapping colors from one image to another in a decorrelated color space is then straightforward: compute the mean and standard deviation of all pixels in the source and target images for the three color channels separately. Then, shift and scale the target image so that in each color channel the mean and standard deviation of the target image is the same as the source image. The resulting image is then obtained by converting from the decorrelated color space to RGB and clamping negative pixels to zero. The dynamic range of the image may have changed as a result of applying this algorithm. It is therefore recommended to apply this algorithm on high dynamic range images and apply a conventional tone reproduction algorithm afterward. A suitable decorrelated color space is the opponent space from <a href="C23_chapter18.xhtml#sec18_2_4">Section 18.2.4</a>.</p>
<p>The result of applying such a color transform to the image in <a href="C25_chapter20.xhtml#f20_12">Figure 20.12</a> is shown in <a href="C25_chapter20.xhtml#f20_13">Figure 20.13</a>.</p>
<figure id="f20_12" tabindex="0">
<img alt="" src="../images/fig20_12.jpg"/>
<figcaption><p><span class="blue">Figure 20.12.</span> Image used for demonstrating the color transfer technique. Results are shown in <a href="C26_chapter21.xhtml#f21_13">Figures 21.13</a> and <a href="C26_chapter21.xhtml#f21_31">21.31</a>.</p></figcaption>
</figure>
<figure id="f20_13" tabindex="0">
<img alt="" src="../images/fig20_13.jpg"/>
<figcaption><p><span class="blue">Figure 20.13.</span> The image on the left is used to adjust the colors of the image shown in <a href="C25_chapter20.xhtml#f20_12">Figure 20.12</a>. The result is shown on the right.</p></figcaption>
</figure>
</section>
<section>
<h2 id="sec20_4"><a id="term-908"/><a id="term-912"/><span aria-label="577" epub:type="pagebreak" id="pg_577" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_4" role="doc-backlink"><span class="green">20.4 Image Formation</span></a></h2>
<p>For now, we assume that an image is formed as the result of light being diffusely reflected off of surfaces. In <a href="C25_chapter20.xhtml#sec20_5">Sections 20.5</a> and <a href="C25_chapter20.xhtml#sec20_6">20.6</a>, we relax this constraint to scenes directly depicting light sources and highlights. The luminance <em>L<sub>v</sub></em> of each pixel is then approximated by the following product:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mtext> </m:mtext><m:mi mathvariant="normal">=</m:mi><m:mtext> </m:mtext><m:mi mathvariant="italic">r</m:mi><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:msub><m:mrow><m:mi mathvariant="italic">E</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>Here, <em>r</em> denotes the reflectance of a surface, and <em>E<sub>v</sub></em> denotes the illuminance. The subscript <em>v</em> indicates that we are using photometrically weighted quantities. Alternatively, we may write this expression in the logarithmic domain (Oppenheim et al., 1968):</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtable><m:mtr><m:mtd><m:mi>D</m:mi><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mtd><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd columnalign="left"><m:mtext>log</m:mtext><m:mo>(</m:mo><m:msub><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mo>)</m:mo></m:mtd></m:mtr><m:mtr><m:mtd/><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd columnalign="left"><m:mtext>log</m:mtext><m:mo>(</m:mo><m:mi>r</m:mi><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mtext> </m:mtext><m:msub><m:mrow><m:mi>E</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mo>)</m:mo></m:mtd></m:mtr><m:mtr><m:mtd/><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd columnalign="left"><m:mtext>log</m:mtext><m:mo>(</m:mo><m:mi>r</m:mi><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mo>)</m:mo><m:mo>+</m:mo><m:mtext>log</m:mtext><m:mo>(</m:mo><m:msub><m:mrow><m:mi>E</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mo>)</m:mo><m:mn>.</m:mn></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>Photographic transparencies record images by varying the density of the material. In traditional photography, this variation has a logarithmic relation with luminance. Thus, in analogy with common practice in photography, we will use the term <em><a id="index_term1213"/>density representation</em> ( <em>D</em>) for log luminance. When represented in the log domain, reflectance and illuminance become additive. This facilitates separation of these two components, despite the fact that isolating either reflectance or illuminance is an under-constrained problem. In practice, separation is possible only to a certain degree and depends on the composition of the image. Nonetheless, tone reproduction could be based on disentangling these two components of <a id="index_term1224"/>image formation, as shown in the following two sections.</p>
</section>
<section>
<h2 id="sec20_5"><a id="index_term1220"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_5" role="doc-backlink"><span class="green">20.5 Frequency-Based Operators</span></a></h2>
<p>For typical diffuse scenes, the reflectance component tends to exhibit high spatial frequencies due to textured surfaces as well as the presence of surface edges. On the other hand, illuminance tends to be a slowly varying function over space.</p>
<p>Since reflectance is low dynamic range and illuminance is high dynamic range, we may try to separate the two components. The frequency-dependence of both reflectance and illuminance provides a solution. We may, for instance, compute the Fourier transform of an image and attenuate only the low frequencies. This compresses the illuminance component while leaving the reflectance component <a id="term-902"/><span aria-label="578" epub:type="pagebreak" id="pg_578" role="doc-pagebreak"/>largely unaffected—the very first digital tone reproduction operator known to us takes this approach (Oppenheim et al., 1968).</p>
<p>More recently, other operators have also followed this line of reasoning. In particular, bilateral and trilateral filters were used to separate an image into base and detail layers (Durand &amp; Dorsey, 2002; Choudhury &amp; Tumblin, 2003). Both filters are edge-preserving smoothing operators which may be used in a variety of different ways. Applying an edge-preserving smoothing operator to a density image results in a blurred image in which sharp edges remain present (<a href="C25_chapter20.xhtml#f20_14">Figure 20.14</a> (left)). We may view such an image as a base layer. If we then pixel-wise divide the high dynamic range image by the base layer, we obtain a detail layer which contains all the high-frequency detail (<a href="C25_chapter20.xhtml#f20_14">Figure 20.14</a> (right)).</p>
<figure id="f20_14" tabindex="0">
<img alt="" src="../images/fig20_14.jpg"/>
<figcaption><p><span class="blue">Figure 20.14.</span> Bilateral filtering removes small details but preserves sharp gradients (left). The associated detail layer is shown on the right.</p></figcaption>
</figure>
<p>For diffuse scenes, base and detail layers are similar to representations of illuminance and reflectance. For images depicting highlights and light sources, this parallel does not hold. However, separation of an image into base and detail layers is possible regardless of the image’s content. By compressing the base layer before recombining into a compressed density image, a low dynamic range density image may be created (<a href="C25_chapter20.xhtml#f20_15">Figure 20.15</a>). After exponentiation, a displayable image is obtained.</p>
<figure id="f20_15" tabindex="0">
<img alt="" src="../images/fig20_15.jpg"/>
<figcaption><p><span class="blue">Figure 20.15.</span> An image tonemapped using bilateral filtering. The base and detail layers shown in <a href="C25_chapter20.xhtml#f20_14">Figure 20.14</a> are recombined after compressing the base layer.</p></figcaption>
</figure>
<p>Edge-preserving smoothing opera-torsmayalsobeusedtocomputealocal adaptation level for each pixel, which may be used in a spatially varying or local tone reproduction operator. We describe this use of bilateral and trilateral <a id="index_term1217"/>filters in <a href="C25_chapter20.xhtml#sec20_7">Section 20.7</a>.</p>
</section>
<section>
<h2 id="sec20_6"><a id="index_term1221"/><a id="term-909"/><a id="term-911"/><span aria-label="579" epub:type="pagebreak" id="pg_579" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_6" role="doc-backlink"><span class="green">20.6 Gradient-Domain Operators</span></a></h2>
<p>The arguments made for the frequency-based operators in the preceding section also hold for the gradient field. Assuming that no light sources are directly visible, the reflectance component will be a constant function with sharp spikes in the gradient field. Similarly, the illuminance component will cause small gradients everywhere.</p>
<p>Humans are generally able to separate illuminance from reflectance in typical scenes. The perception of surface reflectance after discounting the illuminant is called <em>lightness</em>. To assess the lightness of an image depicting only diffuse surfaces, B. K. P. Horn was the first to separate reflectance and illuminance using a gradient field (Horn, 1974). He used simple thresholding to remove all small gradients and then integrated the image, which involves solving a Poisson equation using the Full Multigrid Method (Press, Teukolsky, Vetterling, &amp; Flannery, 1992).</p>
<p>The result is similar to an edge-preserving smoothing filter. This is according to expectation since Oppenheim’s frequency-based operator works under the same assumptions of scene reflectivity and image formation. In particular, Horn’s work was directly aimed at “mini-worlds of Mondrians,” which are simplified versions of diffuse scenes which resemble the abstract paintings by the famous Dutch painter Piet Mondrian.</p>
<p><a id="index_term533"/>Horn’s work cannot be employed directly as a tone reproduction operator, since most high dynamic range images depict light sources. However, a relatively small variation will turn this work into a suitable tone reproduction operator. If light sources or specular surfaces are depicted in the image, then large gradients will be associated with the edges of light sources and <a id="index_term1222"/>highlights. These cause the image to have a high dynamic range. An example is shown in <a href="C25_chapter20.xhtml#f20_16">Figure 20.16</a>, where the highlights on the snooker balls cause sharp gradients.</p>
<figure id="f20_16" tabindex="0">
<img alt="" src="../images/fig20_16.jpg"/>
<figcaption><p><span class="blue">Figure 20.16.</span> The image on the left (tonemapped using gradient-domain compression) shows a scene with highlights. These highlights show up as large gradients on the right, where the magnitude of the gradients is mapped to a grayscale (black is a gradient of 0, white is the maximum gradient in the image).</p></figcaption>
</figure>
<p><a id="term-347"/><a id="term-910"/><a id="term-922"/><span aria-label="580" epub:type="pagebreak" id="pg_580" role="doc-pagebreak"/>We could therefore compress a high dynamic range image by attenuating large gradients, rather than thresholding the gradient field. This approach was taken by Fattal et al. who showed that high dynamic range imagery may be successfully compressed by integrating a compressed gradient field (<a href="C25_chapter20.xhtml#f20_17">Figure 20.17</a>) (Fattal et al., 2002). Fattal’s gradient-domain compression is not limited to diffuse scenes.</p>
<figure id="f20_17" tabindex="0">
<img alt="" src="../images/fig20_17.jpg"/>
<figcaption><p><span class="blue">Figure 20.17.</span> An image tonemapped using gradient-domain compression.</p></figcaption>
</figure>
</section>
<section>
<h2 id="sec20_7"><a id="index_term1231"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_7" role="doc-backlink"><span class="green">20.7 Spatial Operators</span></a></h2>
<p>In the following sections, we discuss tone reproduction operators which apply compression directly on pixels without transformation to other domains. Often global and local operators are distinguished. Tone reproduction operators in the former class change each pixel’s luminance values according to a compressive function which is the same for each pixel. The term global stems from the fact that many such functions need to be anchored to some values determined by analyzing the full image. In practice, most operators use the geometric average <span class="inline-formula"><m:math alttext=""><m:mrow><m:msub><m:mrow><m:mover><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub></m:mrow></m:math></span> to steer the compression:</p>

<div class="disp-formula" id="equ20_1">
<m:math xmlns:mml="http://www.w3.org/1998/Math/MathML" alttext=""><m:mrow><m:msub><m:mrow><m:mover><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>v</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:mtext>exp</m:mtext><m:mrow><m:mo>(</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn></m:mrow><m:mrow><m:mi>N</m:mi></m:mrow></m:mfrac><m:munder><m:mrow><m:mi mathvariant="normal">Σ</m:mi></m:mrow><m:mrow><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi></m:mrow></m:munder><m:mtext>log</m:mtext><m:mrow><m:mo>(</m:mo><m:mo>∂</m:mo><m:mo>+</m:mo><m:msub><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mi>v</m:mi></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow><m:mo>)</m:mo></m:mrow><m:mo>)</m:mo></m:mrow><m:mn>.</m:mn></m:mrow><m:mspace width="3em"/><m:mo>(20.1)</m:mo></m:math>
</div>
<p>In Equation (20.1), a small constant <em>δ</em> is introduced to prevent the average to become zero in the presence of black pixels. The geometric average is normally mapped to a predefined display value. The effect of mapping the geometric average to different display values is shown in <a href="C25_chapter20.xhtml#f20_18">Figure 20.18</a>. Alternatively, sometimes the minimum or maximum image luminance is used. The main challenge faced in the design of a global operator lies in the choice of the compressive function.</p>
<figure id="f20_18" tabindex="0">
<img alt="" src="../images/fig20_18.jpg"/>
<figcaption><p><span class="blue">Figure 20.18.</span> Spatial tonemapping operator applied after mapping the geometric average to different display values (left: 0.12, right: 0.38).</p></figcaption>
</figure>
<p>On the other hand, local operators compress each pixel according to a specific compression function which is modulated by information derived from a selection of neighboring pixels, rather than the full image. The rationale is that a bright pixel in a bright neighborhood may be perceived differently than a bright pixel in a dim neighborhood. Design challenges in the development of a local operator involves choosing the compressive function, the size of the local neighborhood for each pixel, and the manner in which local pixel values are used. In general, local operators achieve better compression than global operators (<a href="C25_chapter20.xhtml#f20_19">Figure 20.19</a>), albeit at a higher computational cost.</p>
<figure id="f20_19" tabindex="0">
<img alt="" src="../images/fig20_19.jpg"/>
<figcaption><p><span class="blue">Figure 20.19.</span> A global tone reproduction operator (left) and a local tone reproduction operator (right) (Reinhard, Stark, Shirley, &amp; Ferwerda, 2002) of each image. The local operator shows more detail; for example, the metal badge on the right shows better contrast and the highlights are crisper.</p></figcaption>
</figure>
<p><a id="term-813"/><a id="term-923"/><span aria-label="581" epub:type="pagebreak" id="pg_581" role="doc-pagebreak"/>Both global and local operators are often inspired by the human visual system. Most operators employ one of two distinct compressive functions, which is orthogonal to the distinction between local and global operators. Display values <em>L<sub>d</sub></em>(<em>x, y</em>) are most commonly derived from image luminances <em>L<sub>v</sub></em>(<em>x, y</em>) by the <a id="term-895"/><span aria-label="582" epub:type="pagebreak" id="pg_582" role="doc-pagebreak"/>following two functional forms:</p>
<div class="disp-formula" id="equ20_2">
<m:math alttext=""><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="normal">=</m:mi><m:mi mathvariant="normal"/><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">f</m:mi><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:mo>,</m:mo></m:mrow><m:mspace width="3em"/><m:mo>(20.2)</m:mo></m:math>
</div>
<div class="disp-formula" id="equ20_3">
<m:math alttext=""><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="normal">=</m:mi><m:mi mathvariant="normal"/><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>+</m:mo><m:msup><m:mrow><m:mi mathvariant="italic">f</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">n</m:mi></m:mrow></m:msup><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:mn>.</m:mn></m:mrow><m:mspace width="3em"/><m:mo>(20.3)</m:mo></m:math>
</div>
<p>In these equations, <em>f</em> (<em>x, y</em>) may either be a constant or a function which varies per pixel. In the former case, we have a global operator, whereas a spatially varying function <em>f</em> (<em>x, y</em>) results in a local operator. The exponent <em>n</em> is usually a constant which is fixed for a particular operator.</p>
<p>Equation (20.2) divides each pixel’s luminance by a value derived from either the full image or a local neighborhood. Equation (20.3) has an S-shaped curve on a log-linear plot and is called a sigmoid for that reason. This functional form fits data obtained from measuring the electrical response of photoreceptors to flashes of light in various species. In the following sections, we discuss both functional forms.</p>
</section>
<section>
<h2 id="sec20_8"><a id="index_term1214"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_8" role="doc-backlink"><span class="green">20.8 Division</span></a></h2>
<p>Each pixel may be divided by a constant to bring the high dynamic range image within a displayable range. Such a division essentially constitutes linear scaling, as shown in <a href="C25_chapter20.xhtml#f20_3">Figure 20.3</a>. While <a href="C25_chapter20.xhtml#f20_3">Figure 20.3</a> shows ad-hoc linear scaling, this approach may be refined by employing psychophysical data to derive the scaling constant <em>f</em> (<em>x, y</em>) =<em>k</em> in Equation (20.2) (G. J. Ward, 1994; Ferwerda, Pattanaik, Shirley, &amp; Greenberg, 1996).</p>
<p>Alternatively, several approaches exist that compute a spatially varying divisor. In each of these cases, <em>f</em> (<em>x, y</em>) is a blurred version of the image, i.e., <span class="inline-formula"><m:math alttext=""><m:mrow><m:mi mathvariant="italic">f</m:mi><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>=</m:mo><m:msubsup><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow><m:mrow><m:mi mathvariant="normal">b</m:mi><m:mi mathvariant="normal">l</m:mi><m:mi mathvariant="normal">u</m:mi><m:mi mathvariant="normal">r</m:mi></m:mrow></m:msubsup><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:math></span>. The blur is achieved by convolving the image with a Gaussian <a id="index_term1218"/>filter (Chiu et al., 1993; Rahman, Jobson, &amp; Woodell, 1996). In addition, the computation of <em>f</em> (<em>x, y</em>) by blurring the image may be combined with a shift in white point for the purpose of color appearance modeling (Fairchild &amp; Johnson, 2002; G. M. Johnson &amp; Fairchild, 2003; Fairchild &amp; Johnson, 2004).</p>
<p>The size and the weight of the <a id="index_term430"/>Gaussian filter has a profound impact on the resulting displayable image. The Gaussian filter has the effect of selecting a weighted local average. Tone reproduction is then a matter of dividing each pixel by its associated weighted local average. If the size of the filter kernel is chosen too small, then haloing artifacts will occur (<a href="C25_chapter20.xhtml#f20_20">Figure 20.20</a> (left)). Haloing is a common problem with local operators and is particularly evident when tone mapping relies on division.</p>
<figure id="f20_20" tabindex="0">
<img alt="" src="../images/fig20_20.jpg"/>
<figcaption><p><span class="blue">Figure 20.20.</span> <a id="term-333"/><a id="term-789"/><a id="term-903"/><a id="term-919"/><span aria-label="583" epub:type="pagebreak" id="pg_583" role="doc-pagebreak"/>Images tonemapped by dividing by Gaussian-blurred versions. The size of the filter kernel is 64 pixels for the left image and 512 pixels for the right image. For division-based algorithms, halo artifacts are minimized by choosing large filter kernels.</p></figcaption>
</figure>
<p>In general, haloing artifacts may be minimized in this approach by making the filter kernel large (<a href="C25_chapter20.xhtml#f20_20">Figure 20.20</a> (right)). Reasonable results may be obtained by choosing a filter size of at least one quarter of the image. Sometimes even larger filter kernels are desirable to minimize artifacts. Note, that in the limit, the filter size becomes as large as the image itself. In that case, the local operator becomes global, and the extra compression normally afforded by a local approach is lost.</p>
<p>The functional form whereby each pixel is divided by a Gaussian-blurred pixel at the same spatial position thus requires an undesirable tradeoff between amount of compression and severity of artifacts.</p>
</section>
<section>
<h2 id="sec20_9"><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_9" role="doc-backlink"><span class="green">20.9 Sigmoids</span></a></h2>
<p>Equation (20.3) follows a different functional form from simple division, and, therefore, affords a different tradeoff between amount of compression, presence of artifacts, and speed of computation.</p>
<p>Sigmoids have several desirable properties. For very small luminance values, the mapping is approximately linear, so that contrast is preserved in dark areas of the image. The function has an asymptote at one, which means that the output mapping is always bounded between 0 and 1.</p>
<p>In Equation (20.3), the function <em>f</em> (<em>x, y</em>) may be computed as a global constant or as a spatially varying function. Following common practice in electro-physiology, we call <em>f</em> (<em>x, y</em>) the <em>semi-saturation</em> constant. Its value determines which values in the input image are optimally visible after tonemapping. In particular, if we assume that the exponent <em>n</em> equals 1, then luminance values equal to the <a id="index_term1229"/>semi-saturation constant will be mapped to 0<em>.</em>5. The effect of choosing different semi-saturation constants is shown in <a href="C25_chapter20.xhtml#f20_21">Figure 20.21</a>.</p>
<figure id="f20_21" tabindex="0">
<img alt="" src="../images/fig20_21.jpg"/>
<figcaption><p><span class="blue">Figure 20.21.</span> <a id="term-790"/><a id="term-918"/><a id="term-920"/><span aria-label="584" epub:type="pagebreak" id="pg_584" role="doc-pagebreak"/>The choice of semi-saturation constant determines how input values are mapped to display values.</p></figcaption>
</figure>
<p>The function <em>f</em> (<em>x, y</em>) may be computed in several different ways (Reinhard et al., 2005). In its simplest form, <em>f</em> (<em>x, y</em>) is set to <span class="inline-formula"><m:math alttext=""><m:mrow><m:msub><m:mrow><m:mover><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mo>/</m:mo><m:mi>k</m:mi></m:mrow></m:math></span>, so that the geometric average is mapped to user parameter <em>k</em> (<a href="C25_chapter20.xhtml#f20_22">Figure 20.22</a>) (Reinhard et al., 2002). In this case, a good initial value for <em>k</em> is 0<em>.</em>18, although for particularly bright or dark scenes this value may be raised or lowered. Its value may be estimated from the image itself (Reinhard, 2003). The exponent <em>n</em> in Equation (20.3) may be set to 1.</p>
<figure id="f20_22" tabindex="0">
<img alt="" src="../images/fig20_22.jpg"/>
<figcaption><p><span class="blue">Figure 20.22.</span> A linearly scaled image (left) and an image tonemapped using sigmoidal compression (right).</p></figcaption>
</figure>
<p>In this approach, the semi-saturation constant is a function of the geometric average, and the operator is therefore global. A variation of this global operator computes the semi-saturation constant by linearly interpolating between the geometric average and each pixel’s luminance:</p>
<p>
<a id="term-791"/><a id="term-921"/><span aria-label="585" epub:type="pagebreak" id="pg_585" role="doc-pagebreak"/>
</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi mathvariant="italic">f</m:mi><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>=</m:mo><m:mi mathvariant="italic">a</m:mi><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>+</m:mo><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">1</m:mi><m:mo>−</m:mo><m:mi mathvariant="italic">a</m:mi><m:mi mathvariant="normal">)</m:mi><m:msub><m:mrow><m:mover><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">.</m:mi></m:mrow></m:math>
</div>
<p>The interpolation is governed by user parameter <em>a</em> which has the effect of varying the amount of contrast in the displayable image (<a href="C25_chapter20.xhtml#f20_23">Figure 20.23</a>) (Reinhard &amp; Devlin, 2005). More contrast means less visible detail in the light and dark areas and vice versa. This interpolation may be viewed as a halfway house between a fully global and a fully local operator by interpolating between the two extremes without resorting to expensive blurring operations.</p>
<figure id="f20_23" tabindex="0">
<img alt="" src="../images/fig20_23.jpg"/>
<figcaption><p><span class="blue">Figure 20.23.</span> Linear interpolation varies contrast in the tonemapped image. The parameter <em>a</em> is set to 0.0 in the left image, and to 1.0 in the right image.</p></figcaption>
</figure>
<p>Although operators typically compress luminance values, this particular operator may be extended to include a simple form of chromatic adaptation. It thus presents an opportunity to adjust the level of saturation normally associated with tonemapping, as discussed at the beginning of this chapter.</p>
<p>Rather than compress the luminance channel only, sigmoidal compression is applied to each of the three color channels:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">r</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mtd><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">r</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">r</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>+</m:mo><m:msup><m:mrow><m:mi mathvariant="italic">f</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">n</m:mi></m:mrow></m:msup><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:mi mathvariant="normal">,</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">g</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mtd><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">g</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">g</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>+</m:mo><m:msup><m:mrow><m:mi mathvariant="italic">f</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">n</m:mi></m:mrow></m:msup><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:mi mathvariant="normal">,</m:mi></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">b</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mtd><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">b</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">b</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>+</m:mo><m:msup><m:mrow><m:mi mathvariant="italic">f</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">n</m:mi></m:mrow></m:msup><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:mi mathvariant="normal">.</m:mi></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>The computation of <em>f</em> (<em>x, y</em>) is also modified to bilinearly interpolate between the geometric average luminance and pixel luminance and between each independent color channel and the pixel’s luminance value. We therefore compute the geometric average luminance value <span class="inline-formula"><m:math alttext=""><m:mrow><m:msub><m:mrow><m:mover><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub></m:mrow></m:math></span>, as well as the geometric average of the red, green, and blue channels (<span class="inline-formula"><m:math alttext=""><m:mrow><m:msub><m:mrow><m:mover><m:mrow><m:mi>I</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>,</m:mo><m:msub><m:mrow><m:mover><m:mrow><m:mi>I</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>g</m:mi></m:mrow></m:msub></m:mrow></m:math></span>, and <span class="inline-formula"><m:math alttext=""><m:mrow><m:msub><m:mrow><m:mover><m:mrow><m:mi>I</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>b</m:mi></m:mrow></m:msub></m:mrow></m:math></span> ). From these values, we compute <em>f</em> (<em>x, y</em>) for each pixel and for each color channel independently. We show the equation for the red channel ( <em>f<sub>r</sub></em>(<em>x, y</em>)):</p>
<p>
<a id="term-904"/><span aria-label="586" epub:type="pagebreak" id="pg_586" role="doc-pagebreak"/>
</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mtable><m:mtr><m:mtd><m:msub><m:mrow><m:mi mathvariant="italic">G</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">r</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mtd><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd columnalign="left"><m:mi mathvariant="italic">c</m:mi><m:msub><m:mrow><m:mi mathvariant="italic">I</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">r</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>+</m:mo><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">1</m:mi><m:mo>−</m:mo><m:mi mathvariant="italic">c</m:mi><m:mi mathvariant="normal">)</m:mi><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>,</m:mo></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mover><m:mrow><m:mi>G</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi mathvariant="italic">r</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mtd><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd columnalign="left"><m:mi>c</m:mi><m:msub><m:mrow><m:mover><m:mrow><m:mi>I</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>+</m:mo><m:mrow><m:mrow><m:mo>(</m:mo><m:mn>1</m:mn><m:mo>−</m:mo><m:mi>c</m:mi><m:mo>)</m:mo></m:mrow><m:msub><m:mrow><m:mover><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mo>,</m:mo></m:mrow></m:mtd></m:mtr><m:mtr><m:mtd><m:msub><m:mrow><m:mi>f</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mtd><m:mtd><m:mo>=</m:mo></m:mtd><m:mtd columnalign="left"><m:mi>a</m:mi><m:msub><m:mrow><m:mi>G</m:mi></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:mn>1</m:mn><m:mo>−</m:mo><m:mi>a</m:mi><m:mo>)</m:mo></m:mrow><m:msub><m:mrow><m:mover><m:mrow><m:mi>G</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>r</m:mi></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mn>.</m:mn></m:mrow></m:mtd></m:mtr></m:mtable></m:mrow></m:math>
</div>
<p>The interpolation parameter <em>a</em> steers the amount of contrast as before, and the new interpolation parameter <em>c</em> allows a simple form of color correction (<a href="C25_chapter20.xhtml#f20_24">Figure 20.24</a>).</p>
<figure id="f20_24" tabindex="0">
<img alt="" src="../images/fig20_24.jpg"/>
<figcaption><p><span class="blue">Figure 20.24.</span> Linear interpolation for color correction. The parameter <em>c</em> is set to 0.0 in the left image, and to 1.0 in the right image.</p></figcaption>
</figure>
<p>So far we have not discussed the value of the exponent <em>n</em> in Equation (20.3). Studies in electrophysiology report values between <em>n</em> = 0<em>.</em>2 and <em>n</em> = 0<em>.</em>9 (Hood, Finkelstein, &amp; Buckingham, 1979). While the exponent may be user-specified, for a wide variety of images we may estimate a reasonable value from the geometric average luminance <span class="inline-formula"><m:math alttext=""><m:mrow><m:msub><m:mrow><m:mover><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub></m:mrow></m:math></span> and the minimum and maximum luminance in the image (<em>L</em><sub>min</sub> and <em>L</em><sub>max</sub>) with the following empirical equation:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi mathvariant="italic">n</m:mi><m:mo>=</m:mo><m:mi mathvariant="normal">0</m:mi><m:mi mathvariant="normal">.</m:mi><m:mi mathvariant="normal">3</m:mi><m:mo>+</m:mo><m:mi mathvariant="normal">0</m:mi><m:mi mathvariant="normal">.</m:mi><m:mi mathvariant="normal">7</m:mi><m:mi mathvariant="normal">(</m:mi><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow>
<m:mtext>max</m:mtext></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mover><m:mrow><m:mi>L</m:mi></m:mrow><m:mrow><m:mo>¯</m:mo></m:mrow></m:mover></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub></m:mrow><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow>
<m:mtext>max</m:mtext></m:mrow></m:msub><m:mo>−</m:mo><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow>
<m:mtext>min</m:mtext></m:mrow></m:msub></m:mrow></m:mfrac><m:msup><m:mrow><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:mi mathvariant="normal">1</m:mi><m:mn>.</m:mn><m:mi mathvariant="normal">4</m:mi></m:mrow></m:msup><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>The several variants of sigmoidal compression shown so far are all global in nature. This has the advantage that they are fast to compute, and they are very suitable for medium to high dynamic range images. For very high dynamic range images, it may be necessary to resort to a local operator, since this may give some extra compression. A straightforward method to extend sigmoidal compression replaces the global semi-saturation constant by a spatially varying function, which may be computed in several different ways.</p>
<p>In other words, the function <em>f</em> (<em>x, y</em>) is so far assumed to be constant, but may also be computed as a spatially localized average. Perhaps the simplest way to accomplish this is to once more use a Gaussian-blurred image. Each pixel in <a id="term-334"/><a id="term-905"/><span aria-label="587" epub:type="pagebreak" id="pg_587" role="doc-pagebreak"/>a blurred image represents a locally averaged value which may be viewed as a suitable choice for the semi-saturation constant<a epub:type="noteref" href="C25_chapter20.xhtml#fn20_1" id="rfn20_1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>As with division-based operators discussed in the previous section, we have to consider haloing artifacts. However, when an image is divided by a Gaussian-blurred version of itself, the size of the Gaussian <a id="index_term1219"/>filter kernel needs to be large in order to minimize halos. If sigmoids are used with a spatially variant semi-saturation constant, the <a id="index_term431"/>Gaussian filter kernel needs to be made small in order to minimize artifacts. This is a significant improvement, since small amounts of Gaussian blur may be efficiently computed directly in the spatial domain. In other words, there is no need to resort to expensive Fourier transforms. In practice, filter kernels of only a few pixels width are sufficient to suppress significant artifacts while at the same time producing more local contrast in the tonemapped images.</p>
<p>One potential issue with Gaussian blur is that the filter blurs across sharp contrast edges in the same way that it blurs small details. In practice, if there is a large contrast gradient in the neighborhood of the pixel under consideration, this causes the Gaussian-blurred pixel to be significantly different from the pixel itself. This is the direct cause for halos. By using a very large filter kernel in a division-based approach, such large contrasts are averaged out.</p>
<p>In sigmoidal compression schemes, a small Gaussian filter minimizes the chances of overlapping with a sharp contrast gradient. In that case, halos still occur, but their size is such that they usually go unnoticed and instead are perceived as enhancing contrast.</p>
<p>Another way to blur an image, while minimizing the negative effects of nearby large contrast steps, is to avoid blurring over such edges. A simple, but computationally expensive way, is to compute a stack of Gaussian-blurred images with different kernel sizes. For each pixel, we may choose the largest Gaussian that does not overlap with a significant gradient.</p>
<p>In a relatively uniform neighborhood, the value of a Gaussian-blurred pixel should be the same regardless of the filter kernel size. Thus, the difference between a pixel filtered with two different Gaussians should be approximately zero. This difference will only change significantly if the wider filter kernel overlaps with a neighborhood containing a sharp contrast step, whereas the smaller filter kernel does not.</p>
<aside class="footnote" epub:type="footnote" role="doc-footnote"><p><a href="#rfn20_1" id="fn20_1"><sup>1</sup></a> Although <em>f</em> (<em>x, y</em>) is now no longer a constant, we continue to refer to it as the semi-saturation constant.</p></aside>
<p class="indent"><a id="term-906"/><span aria-label="588" epub:type="pagebreak" id="pg_588" role="doc-pagebreak"/>It is possible, therefore, to find the largest neighborhood around a pixel that does not contain sharp edges by examining differences of Gaussians at different kernel sizes. For the image shown in <a href="C25_chapter20.xhtml#f20_25">Figure 20.25</a>, the scale selected for each pixel is shown in <a href="C25_chapter20.xhtml#f20_26">Figure 20.26</a> (left). Such a scale selection mechanism is employed by the photographic tone reproduction operator (Reinhard et al., 2002) as well as in Ashikhmin’s operator (Ashikhmin, 2002).</p>
<figure id="f20_25" tabindex="0">
<img alt="" src="../images/fig20_25.jpg"/>
<figcaption><p><span class="blue">Figure 20.25.</span> Example image used to demonstrate the scale selection mechanism shown in <a href="C25_chapter20.xhtml#f20_26">Figure 20.26</a>.</p></figcaption>
</figure>
<figure id="f20_26" tabindex="0">
<img alt="" src="../images/fig20_26.jpg"/>
<figcaption><p><span class="blue">Figure 20.26.</span> Scale selection mechanism: the left image shows the scale selected for each pixel of the image shown in <a href="C25_chapter20.xhtml#f20_25">Figure 20.25</a>; the darker the pixel, the smaller the scale. A total of eight different scales were used to compute this image. The right image shows the local average computed for each pixel on the basis of the neighborhood selection mechanism.</p></figcaption>
</figure>
<p>Once the appropriate neighborhood for each pixel is known, the Gaussian-blurred average <em>L</em><sub>blur</sub> for this neighborhood (shown on the right of <a href="C25_chapter20.xhtml#f20_26">Figure 20.26</a>) may be used to steer the semi-saturation constant, such as for instance employed by the photographic tone reproduction operator:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="normal">d</m:mi></m:mrow></m:msub><m:mo>=</m:mo><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="normal">w</m:mi></m:mrow></m:msub></m:mrow><m:mrow><m:mi mathvariant="normal">1</m:mi><m:mo>+</m:mo><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="normal">b</m:mi><m:mi mathvariant="normal">l</m:mi><m:mi mathvariant="normal">u</m:mi><m:mi mathvariant="normal">r</m:mi></m:mrow></m:msub></m:mrow></m:mfrac><m:mi mathvariant="normal">.</m:mi></m:mrow></m:math>
</div>
<p>An alternative, and arguably better, approach is to employ edge-preserving smoothing operators, which are designed specifically for removing small details while keeping sharp contrasts in tact. Several such filters, such as the bilateral filter (<a href="C25_chapter20.xhtml#f20_27">Figure 20.27</a>), trilateral filter, Susan filter, the LCIS algorithm and the mean shift algorithm are suitable, although some of them are expensive to compute (Durand &amp; Dorsey, 2002; Choudhury &amp; Tumblin, 2003; Pattanaik &amp; Yee, 2002; Tumblin &amp; Turk, 1999; Comaniciu &amp; Meer, 2002).</p>
<figure id="f20_27" tabindex="0">
<img alt="" src="../images/fig20_27.jpg"/>
<figcaption><p><span class="blue">Figure 20.27.</span> Sigmoidal compression (left) and sigmoidal compression using bilateral filtering to compute the semi-saturation constant (right). Note the improved contrast in the sky in the right image.</p></figcaption>
</figure>
</section>
<section>
<h2 id="sec20_10"><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_10" role="doc-backlink"><span class="green">20.10 Other Approaches</span></a></h2>
<p>Although the previous sections together discuss most tone reproduction operators to date, there are one or two operators that do not directly fit into the above categories. The simplest of these are variations of logarithmic compression, and the other is a histogram-based approach.</p>
<p><a id="term-900"/><a id="term-907"/><span aria-label="589" epub:type="pagebreak" id="pg_589" role="doc-pagebreak"/><a id="index_term1216"/>Dynamic range reduction may be accomplished by taking the logarithm, provided that this number is greater than 1. Any positive number may then be nonlinearly scaled between 0 and 1 using the following equation:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>=</m:mo><m:mfrac><m:mrow><m:msub><m:mrow>
<m:mtext>log</m:mtext></m:mrow><m:mrow><m:mi mathvariant="italic">b</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">1</m:mi><m:mo>+</m:mo><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow>
<m:mtext>log</m:mtext></m:mrow><m:mrow><m:mi mathvariant="italic">b</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">1</m:mi><m:mo>+</m:mo><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow>
<m:mtext>max</m:mtext></m:mrow></m:msub><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:mi mathvariant="normal">.</m:mi></m:mrow></m:math>
</div>
<p>While the base <em>b</em> of the logarithm above is not specified, any choice of base will do. This freedom to choose the base of the logarithm may be used to vary the base with input luminance, and thus achieve an operator that is better matched to the image being compressed (Drago, Myszkowski, Annen, &amp; Chiba, 2003). This method uses Perlin and Hoffert’s bias function which takes user parameter <em>p</em> (Perlin &amp; Hoffert, 1989):</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi mathvariant="normal">b</m:mi><m:mi mathvariant="normal">i</m:mi><m:mi mathvariant="normal">a</m:mi><m:msub><m:mrow><m:mi mathvariant="normal">s</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">p</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>=</m:mo><m:msup><m:mrow><m:mi mathvariant="italic">x</m:mi></m:mrow><m:mrow><m:msub><m:mrow>
<m:mtext>log</m:mtext></m:mrow><m:mrow><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">0</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">p</m:mi><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal">/</m:mi><m:msub><m:mrow>
<m:mtext>log</m:mtext></m:mrow><m:mrow><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">0</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">/</m:mi><m:mi mathvariant="normal">2</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:msup><m:mi mathvariant="normal">.</m:mi></m:mrow></m:math>
</div>
<p><a id="term-901"/><span aria-label="590" epub:type="pagebreak" id="pg_590" role="doc-pagebreak"/>Making the base <em>b</em> dependent on luminance and smoothly interpolating bases between 2 and 10, the logarithmic mapping above may be refined:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">d</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="normal"/><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mo>=</m:mo><m:mfrac><m:mrow><m:msub><m:mrow>
<m:mtext>log</m:mtext></m:mrow><m:mrow><m:mi mathvariant="normal">l</m:mi><m:mi mathvariant="normal">0</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">1</m:mi><m:mo>+</m:mo><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow>
<m:mtext>log</m:mtext></m:mrow><m:mrow><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">0</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">1</m:mi><m:mo>+</m:mo><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow>
<m:mtext>max</m:mtext></m:mrow></m:msub><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:mo>⋅</m:mo><m:mfrac><m:mrow><m:mi mathvariant="normal">1</m:mi></m:mrow><m:mrow><m:msub><m:mrow>
<m:mtext>log</m:mtext></m:mrow><m:mrow><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">0</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">2</m:mi><m:mo>+</m:mo><m:mi mathvariant="normal">8</m:mi><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">(</m:mi><m:mfrac><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow><m:mi>υ</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">x</m:mi><m:mi mathvariant="normal">,</m:mi><m:mi mathvariant="italic">y</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow><m:mi mathvariant="italic">L</m:mi></m:mrow><m:mrow>
<m:mtext>max</m:mtext></m:mrow></m:msub></m:mrow></m:mfrac><m:msup><m:mrow><m:mi mathvariant="normal">)</m:mi></m:mrow><m:mrow><m:msub><m:mrow>
<m:mtext>log</m:mtext></m:mrow><m:mrow><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">0</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="italic">p</m:mi><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal">/</m:mi><m:msub><m:mrow>
<m:mtext>log</m:mtext></m:mrow><m:mrow><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">0</m:mi></m:mrow></m:msub><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">/</m:mi><m:mi mathvariant="normal">2</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:msup><m:mi mathvariant="normal">)</m:mi><m:mi mathvariant="normal">)</m:mi></m:mrow></m:mfrac><m:mi mathvariant="normal">.</m:mi></m:mrow></m:math>
</div>
<p>For user parameter <em>p</em>, an initial value of around 0<em>.</em>85 tends to yield plausible results (<a href="C25_chapter20.xhtml#f20_28">Figure 20.28</a> (right)).</p>
<figure id="f20_28" tabindex="0">
<img alt="" src="../images/fig20_28.jpg"/>
<figcaption><p><span class="blue">Figure 20.28.</span> <a id="index_term1225"/>Logarithmic compression using base 10 logarithms (left) and logarithmic compression with varying base (right).</p></figcaption>
</figure>
<p>Alternatively, tone reproduction may be based on histogram equalization. Traditional histogram equalization aims to give each luminance value equal probability of occurrence in the output image. Greg Ward refines this method in a manner that preserves contrast (Ward Larson, Rushmeier, &amp; Piatko, 1997).</p>
<p>First, a histogram is computed from the luminances in the high dynamic range image. From this histogram, a cumulative histogram is computed such that each bin contains the number of pixels that have a luminance value less than or equal to the luminance value that the bin represents. The cumulative histogram is a monotonically increasing function. Plotting the values in each bin against the luminance values represented by each bin therefore yields a function which may be viewed as a luminance mapping function. Scaling this function, such that the vertical axis spans the range of the display device, yields a tone reproduction operator. This technique is called <a id="index_term1223"/>histogram equalization.</p>
<p>Ward further refined this method by ensuring that the gradient of this function never exceeds 1. This means, that if the difference between neighboring values in the cumulative histogram is too large, this difference is clamped to 1. This avoids the problem that small changes in luminance in the input may yield large differences in the output image. In other words, by limiting the gradient of the cumulative histogram to 1, contrast is never exaggerated. The resulting algorithm is called histogram adjustment (see <a href="C25_chapter20.xhtml#f20_29">Figure 20.29</a>).</p>
<figure id="f20_29" tabindex="0">
<img alt="" src="../images/fig20_29.jpg"/>
<figcaption><p><span class="blue">Figure 20.29.</span> A linearly scaled image (left) and a histogram adjusted image (right). <em>Image created with the kind permission of the Albin Polasek museum, Winter Park, Florida.</em></p></figcaption>
</figure>
</section>
<section>
<h2 id="sec20_11"><a id="index_term1226"/><a id="term-550"/><a id="term-606"/><a id="term-736"/><a id="term-913"/><a id="term-914"/><a id="term-916"/><a id="term-917"/><span aria-label="591" epub:type="pagebreak" id="pg_591" role="doc-pagebreak"/><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_11" role="doc-backlink"><span class="green">20.11 Night Tonemapping</span></a></h2>
<p>The tone reproduction operators discussed so far nearly all assume that the image represents a scene under <em>photopic</em> viewing conditions, i.e., as seen at normal light levels. For <em>scotopic</em> scenes, i.e., very dark scenes, the human visual system exhibits distinctly different behavior. In particular, perceived contrast is lower, visual acuity (i.e., the smallest detail that we can distinguish) is lower, and everything has a slightly blue appearance.</p>
<p>To allow such images to be viewed correctly on monitors placed in photopic lighting conditions, we may preprocess the image such that it appears as if we were adapted to a very dark viewing environment. Such preprocessing frequently takes the form of a reduction in brightness and contrast, desaturation of the image, blue shift, and a reduction in visual acuity (Thompson, Shirley, &amp; Ferwerda, 2002).</p>
<p>A typical approach starts by converting the image from RGB to XYZ. Then, scotopic luminance <em>V</em> may be computed for each pixel:</p>
<div class="disp-formula">
<m:math alttext=""><m:mrow><m:mi mathvariant="italic">V</m:mi><m:mo>=</m:mo><m:mi mathvariant="italic">Y</m:mi><m:mi mathvariant="normal">[</m:mi><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">.</m:mi><m:mi mathvariant="normal">3</m:mi><m:mi mathvariant="normal">3</m:mi><m:mtext> </m:mtext><m:mi mathvariant="normal">(</m:mi><m:mi mathvariant="normal">1</m:mi><m:mo>+</m:mo><m:mfrac><m:mrow><m:mi mathvariant="italic">Y</m:mi><m:mo>+</m:mo><m:mi mathvariant="italic">Z</m:mi></m:mrow><m:mrow><m:mi mathvariant="italic">X</m:mi></m:mrow></m:mfrac><m:mi mathvariant="normal">)</m:mi><m:mo>−</m:mo><m:mi mathvariant="normal">1</m:mi><m:mi mathvariant="normal">.</m:mi><m:mi mathvariant="normal">6</m:mi><m:mi mathvariant="normal">8</m:mi><m:mi mathvariant="normal">]</m:mi><m:mn>.</m:mn></m:mrow></m:math>
</div>
<p>This single channel image may then be scaled and multiplied by an empirically chosen bluish gray. An example is shown in <a href="C25_chapter20.xhtml#f20_30">Figure 20.30</a>. If some pixels are in the photopic range, then the night image may be created by linearly blending the bluish-gray image with the input image. The fraction to use for each pixel depends on <em>V</em>.</p>
<figure id="f20_30" tabindex="0">
<img alt="" src="../images/fig20_30.jpg"/>
<figcaption><p><span class="blue">Figure 20.30.</span> Simulated night scene using the image shown in <a href="C25_chapter20.xhtml#f20_12">Figure 20.12</a>.</p></figcaption>
</figure>
<p>Loss of visual acuity may be modeled by low-pass filtering the night image, although this would give an incorrect sense of blurriness. A better approach is to apply a bilateral filter to retain sharp edges while blurring smaller details (Tomasi &amp; Manduchi, 1998).</p>
<p>Finally, the color transfer technique outlined in <a href="C25_chapter20.xhtml#sec20_3">Section 20.3</a> may also be used to transform a day-lit image into a night scene. The effectiveness of this approach depends on the availability of a suitable night image from which to transfer colors. As an example, the image in <a href="C25_chapter20.xhtml#f20_12">Figure 20.12</a> is transformed into a night image in <a href="C25_chapter20.xhtml#f20_31">Figure 20.31</a>.</p>
<figure id="f20_31" tabindex="0">
<img alt="" src="../images/fig20_31.jpg"/>
<figcaption><p><span class="blue">Figure 20.31.</span> <a id="term-551"/><a id="term-915"/><span aria-label="592" epub:type="pagebreak" id="pg_592" role="doc-pagebreak"/>The image on the left is used to transform the image of <a href="C25_chapter20.xhtml#f20_12">Figure 20.12</a> into a night scene, shown here on the right.</p></figcaption>
</figure>
</section>
<section>
<h2 id="sec20_12"><a epub:type="backlink" href="C02a_toc.xhtml#rsec20_12" role="doc-backlink"><span class="green">20.12 Discussion</span></a></h2>
<p>Since global illumination algorithms naturally produce high dynamic range images, direct display of the resulting images is not possible. Rather than resort to linear scaling or clamping, a tone reproduction operator should be used. Any tone reproduction operator is better than using no tone reproduction. Dependent on the requirements of the application, one of several operators may be suitable.</p>
<p>For instance, <a id="index_term442"/>real-time rendering applications should probably resort to a simple sigmoidal compression, since these are fast enough to also run in real time. In addition, their visual quality is often good enough. The histogram adjustment technique (Ward Larson et al., 1997) may also be fast enough for real-time operation.</p>
<p>For scenes containing a very high dynamic range, better compression may be achieved with a local operator. However, the computational cost is frequently substantially higher, leaving these operators suitable only for noninteractive applications. Among the fastest of the local operators is the bilateral filter due to the optimizations afforded by this technique (Durand &amp; Dorsey, 2002).</p>
<p>This filter is interesting as a tone reproduction operator by itself, or it may be used to compute a local adaptation level for use in a sigmoidal compression function. In either case, the filter respects sharp contrast changes and smoothes over smaller contrasts. This is an important feature that helps minimize halo artifacts, which are a common problem with local operators.</p>
<p>An alternative approach to minimize halo artifacts is the scale selection mechanism used in the photographic tone reproduction operator (Reinhard et al., 2002), although this technique is slower to compute.</p>
<p>In summary, while a large number of tone reproduction operators is currently available, only a small number of fundamentally different approaches exist. Fourier-domain and gradient-domain operators are both rooted in knowledge of <span aria-label="593" epub:type="pagebreak" id="pg_593" role="doc-pagebreak"/>image formation. Spatial-domain operators are either spatially variant (local) or global in nature. These operators are usually based on insights gained from studying the human visual system (and the visual system of many other species).</p>
</section>
</section>
</body>
</html>